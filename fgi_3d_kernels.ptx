//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31442593
// Cuda compilation tools, release 11.7, V11.7.99
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_35
.address_size 64

	// .globl	_Z18computeImpulsivityPKdPdjjjjj
// _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage has been demoted
// _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage has been demoted
// _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage has been demoted
// _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage has been demoted
// _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage has been demoted
// _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage has been demoted
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust6system3cpp3parE[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust8cuda_cub3parE[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust3seqE[1];
.global .align 1 .b8 _ZN45_INTERNAL_5b9a3292_14_fgi_kernels_cu_5e178a4c6thrust6deviceE[1];

.visible .entry _Z18computeImpulsivityPKdPdjjjjj(
	.param .u64 _Z18computeImpulsivityPKdPdjjjjj_param_0,
	.param .u64 _Z18computeImpulsivityPKdPdjjjjj_param_1,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_2,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_3,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_4,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_5,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_6
)
{
	.local .align 4 .b8 	__local_depot0[120];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<267>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<315>;
	.reg .f64 	%fd<46>;
	.reg .b64 	%rd<133>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd34, [_Z18computeImpulsivityPKdPdjjjjj_param_0];
	ld.param.u32 	%r174, [_Z18computeImpulsivityPKdPdjjjjj_param_2];
	ld.param.u32 	%r175, [_Z18computeImpulsivityPKdPdjjjjj_param_3];
	ld.param.u32 	%r176, [_Z18computeImpulsivityPKdPdjjjjj_param_4];
	ld.param.u32 	%r178, [_Z18computeImpulsivityPKdPdjjjjj_param_6];
	cvta.to.global.u64 	%rd1, %rd34;
	add.u64 	%rd130, %SPL, 0;
	add.u64 	%rd3, %SPL, 60;
	mov.u32 	%r179, %ntid.x;
	mov.u32 	%r180, %ctaid.x;
	mov.u32 	%r181, %tid.x;
	mad.lo.s32 	%r1, %r180, %r179, %r181;
	mov.u32 	%r182, %ntid.y;
	mov.u32 	%r183, %ctaid.y;
	mov.u32 	%r184, %tid.y;
	mad.lo.s32 	%r2, %r183, %r182, %r184;
	mul.lo.s32 	%r185, %r2, %r174;
	mul.lo.s32 	%r3, %r185, %r175;
	add.s32 	%r4, %r3, %r1;
	mul.lo.s32 	%r5, %r175, %r174;
	setp.ge.u32 	%p1, %r1, %r5;
	setp.ge.u32 	%p2, %r2, %r176;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_132;

	shl.b32 	%r6, %r178, 1;
	or.b32  	%r187, %r6, 1;
	mul.lo.s32 	%r188, %r187, %r187;
	mad.lo.s32 	%r7, %r188, %r187, 244;
	div.u32 	%r189, %r1, %r175;
	cvt.u64.u32 	%rd37, %r189;
	cvt.u64.u32 	%rd38, %r178;
	sub.s64 	%rd39, %rd37, %rd38;
	neg.s64 	%rd40, %rd39;
	shr.s64 	%rd41, %rd39, 63;
	cvt.u32.u64 	%r190, %rd40;
	cvt.u32.u64 	%r191, %rd41;
	and.b32  	%r192, %r191, %r190;
	add.s64 	%rd42, %rd37, %rd38;
	cvt.u64.u32 	%rd43, %r174;
	setp.lt.u64 	%p4, %rd42, %rd43;
	mov.u64 	%rd44, 1;
	sub.s64 	%rd45, %rd44, %rd43;
	add.s64 	%rd46, %rd45, %rd42;
	selp.b64 	%rd47, 0, %rd46, %p4;
	cvt.u32.u64 	%r193, %rd47;
	mul.lo.s32 	%r194, %r189, %r175;
	sub.s32 	%r195, %r1, %r194;
	cvt.u64.u32 	%rd48, %r195;
	sub.s64 	%rd49, %rd48, %rd38;
	neg.s64 	%rd50, %rd49;
	shr.s64 	%rd51, %rd49, 63;
	cvt.u32.u64 	%r196, %rd50;
	cvt.u32.u64 	%r197, %rd51;
	and.b32  	%r198, %r197, %r196;
	add.s64 	%rd52, %rd48, %rd38;
	cvt.u64.u32 	%rd53, %r175;
	setp.lt.u64 	%p5, %rd52, %rd53;
	sub.s64 	%rd54, %rd44, %rd53;
	add.s64 	%rd55, %rd54, %rd52;
	selp.b64 	%rd56, 0, %rd55, %p5;
	cvt.u32.u64 	%r199, %rd56;
	sub.s32 	%r200, %r187, %r192;
	sub.s32 	%r8, %r200, %r193;
	sub.s32 	%r201, %r187, %r198;
	sub.s32 	%r9, %r201, %r199;
	sub.s32 	%r202, %r189, %r178;
	add.s32 	%r293, %r202, %r192;
	sub.s32 	%r203, %r195, %r178;
	add.s32 	%r11, %r203, %r198;
	add.s32 	%r12, %r293, %r187;
	setp.ge.u32 	%p6, %r293, %r12;
	mov.u32 	%r288, 0;
	@%p6 bra 	$L__BB0_30;

	add.s32 	%r205, %r6, 1;
	add.s32 	%r13, %r11, %r205;
	add.s32 	%r14, %r8, %r293;
	add.s32 	%r15, %r9, %r11;
	sub.s32 	%r206, %r4, %r175;
	add.s32 	%r16, %r206, -1;
	add.s32 	%r17, %r206, 1;
	add.s32 	%r207, %r4, %r175;
	add.s32 	%r18, %r207, -1;
	add.s32 	%r19, %r207, 1;
	and.b32  	%r20, %r205, 3;
	add.s32 	%r21, %r11, 1;
	add.s32 	%r22, %r11, 2;
	add.s32 	%r23, %r11, 3;
	setp.ge.u32 	%p7, %r11, %r13;
	setp.ge.u32 	%p8, %r11, %r15;
	setp.eq.s32 	%p18, %r20, 1;
	setp.lt.u32 	%p39, %r6, 3;
	mov.u32 	%r265, %r293;

$L__BB0_3:
	@%p7 bra 	$L__BB0_29;

	mad.lo.s32 	%r26, %r265, %r175, %r3;
	setp.ge.u32 	%p9, %r265, %r14;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB0_7;

	add.s32 	%r27, %r26, %r11;
	setp.eq.s32 	%p11, %r27, %r16;
	setp.eq.s32 	%p12, %r27, %r17;
	or.pred  	%p13, %p12, %p11;
	setp.eq.s32 	%p14, %r27, %r18;
	or.pred  	%p15, %p14, %p13;
	setp.eq.s32 	%p16, %r27, %r19;
	or.pred  	%p17, %p16, %p15;
	@%p17 bra 	$L__BB0_7;

	mul.wide.s32 	%rd57, %r288, 4;
	add.s64 	%rd58, %rd3, %rd57;
	st.local.u32 	[%rd58], %r27;
	add.s32 	%r288, %r288, 1;

$L__BB0_7:
	mov.u32 	%r269, %r21;
	@%p18 bra 	$L__BB0_14;

	setp.ge.u32 	%p20, %r21, %r15;
	or.pred  	%p21, %p9, %p20;
	@%p21 bra 	$L__BB0_11;

	add.s32 	%r30, %r26, %r21;
	setp.eq.s32 	%p22, %r30, %r16;
	setp.eq.s32 	%p23, %r30, %r17;
	or.pred  	%p24, %p23, %p22;
	setp.eq.s32 	%p25, %r30, %r18;
	or.pred  	%p26, %p25, %p24;
	setp.eq.s32 	%p27, %r30, %r19;
	or.pred  	%p28, %p27, %p26;
	@%p28 bra 	$L__BB0_11;

	mul.wide.s32 	%rd59, %r288, 4;
	add.s64 	%rd60, %rd3, %rd59;
	st.local.u32 	[%rd60], %r30;
	add.s32 	%r288, %r288, 1;

$L__BB0_11:
	setp.ge.u32 	%p29, %r22, %r15;
	or.pred  	%p31, %p9, %p29;
	mov.u32 	%r269, %r23;
	@%p31 bra 	$L__BB0_14;

	add.s32 	%r33, %r26, %r22;
	setp.eq.s32 	%p32, %r33, %r16;
	setp.eq.s32 	%p33, %r33, %r17;
	or.pred  	%p34, %p33, %p32;
	setp.eq.s32 	%p35, %r33, %r18;
	or.pred  	%p36, %p35, %p34;
	setp.eq.s32 	%p37, %r33, %r19;
	or.pred  	%p38, %p37, %p36;
	mov.u32 	%r269, %r23;
	@%p38 bra 	$L__BB0_14;

	mul.wide.s32 	%rd61, %r288, 4;
	add.s64 	%rd62, %rd3, %rd61;
	st.local.u32 	[%rd62], %r33;
	add.s32 	%r288, %r288, 1;
	mov.u32 	%r269, %r23;

$L__BB0_14:
	@%p39 bra 	$L__BB0_29;

$L__BB0_16:
	setp.ge.u32 	%p40, %r269, %r15;
	or.pred  	%p42, %p9, %p40;
	@%p42 bra 	$L__BB0_19;

	add.s32 	%r39, %r26, %r269;
	setp.eq.s32 	%p43, %r39, %r16;
	setp.eq.s32 	%p44, %r39, %r17;
	or.pred  	%p45, %p44, %p43;
	setp.eq.s32 	%p46, %r39, %r18;
	or.pred  	%p47, %p46, %p45;
	setp.eq.s32 	%p48, %r39, %r19;
	or.pred  	%p49, %p48, %p47;
	@%p49 bra 	$L__BB0_19;

	mul.wide.s32 	%rd63, %r288, 4;
	add.s64 	%rd64, %rd3, %rd63;
	st.local.u32 	[%rd64], %r39;
	add.s32 	%r288, %r288, 1;

$L__BB0_19:
	add.s32 	%r42, %r269, 1;
	setp.ge.u32 	%p50, %r42, %r15;
	or.pred  	%p52, %p9, %p50;
	@%p52 bra 	$L__BB0_22;

	add.s32 	%r43, %r26, %r42;
	setp.eq.s32 	%p53, %r43, %r16;
	setp.eq.s32 	%p54, %r43, %r17;
	or.pred  	%p55, %p54, %p53;
	setp.eq.s32 	%p56, %r43, %r18;
	or.pred  	%p57, %p56, %p55;
	setp.eq.s32 	%p58, %r43, %r19;
	or.pred  	%p59, %p58, %p57;
	@%p59 bra 	$L__BB0_22;

	mul.wide.s32 	%rd65, %r288, 4;
	add.s64 	%rd66, %rd3, %rd65;
	st.local.u32 	[%rd66], %r43;
	add.s32 	%r288, %r288, 1;

$L__BB0_22:
	add.s32 	%r46, %r269, 2;
	setp.ge.u32 	%p60, %r46, %r15;
	or.pred  	%p62, %p9, %p60;
	@%p62 bra 	$L__BB0_25;

	add.s32 	%r47, %r26, %r46;
	setp.eq.s32 	%p63, %r47, %r16;
	setp.eq.s32 	%p64, %r47, %r17;
	or.pred  	%p65, %p64, %p63;
	setp.eq.s32 	%p66, %r47, %r18;
	or.pred  	%p67, %p66, %p65;
	setp.eq.s32 	%p68, %r47, %r19;
	or.pred  	%p69, %p68, %p67;
	@%p69 bra 	$L__BB0_25;

	mul.wide.s32 	%rd67, %r288, 4;
	add.s64 	%rd68, %rd3, %rd67;
	st.local.u32 	[%rd68], %r47;
	add.s32 	%r288, %r288, 1;

$L__BB0_25:
	add.s32 	%r50, %r269, 3;
	setp.ge.u32 	%p70, %r50, %r15;
	or.pred  	%p72, %p9, %p70;
	@%p72 bra 	$L__BB0_28;

	add.s32 	%r51, %r26, %r50;
	setp.eq.s32 	%p73, %r51, %r16;
	setp.eq.s32 	%p74, %r51, %r17;
	or.pred  	%p75, %p74, %p73;
	setp.eq.s32 	%p76, %r51, %r18;
	or.pred  	%p77, %p76, %p75;
	setp.eq.s32 	%p78, %r51, %r19;
	or.pred  	%p79, %p78, %p77;
	@%p79 bra 	$L__BB0_28;

	mul.wide.s32 	%rd69, %r288, 4;
	add.s64 	%rd70, %rd3, %rd69;
	st.local.u32 	[%rd70], %r51;
	add.s32 	%r288, %r288, 1;

$L__BB0_28:
	add.s32 	%r269, %r269, 4;
	setp.lt.u32 	%p80, %r269, %r13;
	@%p80 bra 	$L__BB0_16;

$L__BB0_29:
	add.s32 	%r265, %r265, 1;
	setp.lt.u32 	%p81, %r265, %r12;
	@%p81 bra 	$L__BB0_3;

$L__BB0_30:
	setp.ge.u32 	%p265, %r293, %r12;
	setp.eq.s32 	%p82, %r2, 0;
	or.pred  	%p84, %p82, %p265;
	@%p84 bra 	$L__BB0_59;

	ld.param.u32 	%r264, [_Z18computeImpulsivityPKdPdjjjjj_param_2];
	mul.lo.s32 	%r263, %r175, %r264;
	ld.param.u32 	%r262, [_Z18computeImpulsivityPKdPdjjjjj_param_6];
	shl.b32 	%r261, %r262, 1;
	add.s32 	%r208, %r261, 1;
	mov.u32 	%r209, 1;
	add.s32 	%r58, %r11, %r208;
	add.s32 	%r59, %r8, %r293;
	add.s32 	%r60, %r9, %r11;
	sub.s32 	%r61, %r3, %r263;
	add.s32 	%r210, %r263, %r175;
	sub.s32 	%r62, %r4, %r210;
	sub.s32 	%r211, %r175, %r263;
	add.s32 	%r63, %r211, %r4;
	not.b32 	%r212, %r263;
	add.s32 	%r64, %r4, %r212;
	sub.s32 	%r213, %r209, %r263;
	add.s32 	%r65, %r213, %r4;
	and.b32  	%r66, %r208, 3;
	add.s32 	%r67, %r11, 1;
	add.s32 	%r68, %r11, 2;
	add.s32 	%r69, %r11, 3;
	setp.ge.u32 	%p85, %r11, %r58;
	setp.ge.u32 	%p86, %r11, %r60;
	setp.eq.s32 	%p96, %r66, 1;
	setp.lt.u32 	%p117, %r261, 3;
	mov.u32 	%r279, %r293;

$L__BB0_32:
	@%p85 bra 	$L__BB0_58;

	mad.lo.s32 	%r72, %r279, %r175, %r61;
	setp.ge.u32 	%p87, %r279, %r59;
	or.pred  	%p88, %p87, %p86;
	@%p88 bra 	$L__BB0_36;

	add.s32 	%r73, %r72, %r11;
	setp.eq.s32 	%p89, %r73, %r62;
	setp.eq.s32 	%p90, %r73, %r63;
	or.pred  	%p91, %p90, %p89;
	setp.eq.s32 	%p92, %r73, %r64;
	or.pred  	%p93, %p92, %p91;
	setp.eq.s32 	%p94, %r73, %r65;
	or.pred  	%p95, %p94, %p93;
	@%p95 bra 	$L__BB0_36;

	mul.wide.s32 	%rd71, %r288, 4;
	add.s64 	%rd72, %rd3, %rd71;
	st.local.u32 	[%rd72], %r73;
	add.s32 	%r288, %r288, 1;

$L__BB0_36:
	mov.u32 	%r283, %r67;
	@%p96 bra 	$L__BB0_43;

	setp.ge.u32 	%p98, %r67, %r60;
	or.pred  	%p99, %p87, %p98;
	@%p99 bra 	$L__BB0_40;

	add.s32 	%r76, %r72, %r67;
	setp.eq.s32 	%p100, %r76, %r62;
	setp.eq.s32 	%p101, %r76, %r63;
	or.pred  	%p102, %p101, %p100;
	setp.eq.s32 	%p103, %r76, %r64;
	or.pred  	%p104, %p103, %p102;
	setp.eq.s32 	%p105, %r76, %r65;
	or.pred  	%p106, %p105, %p104;
	@%p106 bra 	$L__BB0_40;

	mul.wide.s32 	%rd73, %r288, 4;
	add.s64 	%rd74, %rd3, %rd73;
	st.local.u32 	[%rd74], %r76;
	add.s32 	%r288, %r288, 1;

$L__BB0_40:
	setp.ge.u32 	%p107, %r68, %r60;
	or.pred  	%p109, %p87, %p107;
	mov.u32 	%r283, %r69;
	@%p109 bra 	$L__BB0_43;

	add.s32 	%r79, %r72, %r68;
	setp.eq.s32 	%p110, %r79, %r62;
	setp.eq.s32 	%p111, %r79, %r63;
	or.pred  	%p112, %p111, %p110;
	setp.eq.s32 	%p113, %r79, %r64;
	or.pred  	%p114, %p113, %p112;
	setp.eq.s32 	%p115, %r79, %r65;
	or.pred  	%p116, %p115, %p114;
	mov.u32 	%r283, %r69;
	@%p116 bra 	$L__BB0_43;

	mul.wide.s32 	%rd75, %r288, 4;
	add.s64 	%rd76, %rd3, %rd75;
	st.local.u32 	[%rd76], %r79;
	add.s32 	%r288, %r288, 1;
	mov.u32 	%r283, %r69;

$L__BB0_43:
	@%p117 bra 	$L__BB0_58;

$L__BB0_45:
	setp.ge.u32 	%p118, %r283, %r60;
	or.pred  	%p120, %p87, %p118;
	@%p120 bra 	$L__BB0_48;

	add.s32 	%r85, %r72, %r283;
	setp.eq.s32 	%p121, %r85, %r62;
	setp.eq.s32 	%p122, %r85, %r63;
	or.pred  	%p123, %p122, %p121;
	setp.eq.s32 	%p124, %r85, %r64;
	or.pred  	%p125, %p124, %p123;
	setp.eq.s32 	%p126, %r85, %r65;
	or.pred  	%p127, %p126, %p125;
	@%p127 bra 	$L__BB0_48;

	mul.wide.s32 	%rd77, %r288, 4;
	add.s64 	%rd78, %rd3, %rd77;
	st.local.u32 	[%rd78], %r85;
	add.s32 	%r288, %r288, 1;

$L__BB0_48:
	add.s32 	%r88, %r283, 1;
	setp.ge.u32 	%p128, %r88, %r60;
	or.pred  	%p130, %p87, %p128;
	@%p130 bra 	$L__BB0_51;

	add.s32 	%r89, %r72, %r88;
	setp.eq.s32 	%p131, %r89, %r62;
	setp.eq.s32 	%p132, %r89, %r63;
	or.pred  	%p133, %p132, %p131;
	setp.eq.s32 	%p134, %r89, %r64;
	or.pred  	%p135, %p134, %p133;
	setp.eq.s32 	%p136, %r89, %r65;
	or.pred  	%p137, %p136, %p135;
	@%p137 bra 	$L__BB0_51;

	mul.wide.s32 	%rd79, %r288, 4;
	add.s64 	%rd80, %rd3, %rd79;
	st.local.u32 	[%rd80], %r89;
	add.s32 	%r288, %r288, 1;

$L__BB0_51:
	add.s32 	%r92, %r283, 2;
	setp.ge.u32 	%p138, %r92, %r60;
	or.pred  	%p140, %p87, %p138;
	@%p140 bra 	$L__BB0_54;

	add.s32 	%r93, %r72, %r92;
	setp.eq.s32 	%p141, %r93, %r62;
	setp.eq.s32 	%p142, %r93, %r63;
	or.pred  	%p143, %p142, %p141;
	setp.eq.s32 	%p144, %r93, %r64;
	or.pred  	%p145, %p144, %p143;
	setp.eq.s32 	%p146, %r93, %r65;
	or.pred  	%p147, %p146, %p145;
	@%p147 bra 	$L__BB0_54;

	mul.wide.s32 	%rd81, %r288, 4;
	add.s64 	%rd82, %rd3, %rd81;
	st.local.u32 	[%rd82], %r93;
	add.s32 	%r288, %r288, 1;

$L__BB0_54:
	add.s32 	%r96, %r283, 3;
	setp.ge.u32 	%p148, %r96, %r60;
	or.pred  	%p150, %p87, %p148;
	@%p150 bra 	$L__BB0_57;

	add.s32 	%r97, %r72, %r96;
	setp.eq.s32 	%p151, %r97, %r62;
	setp.eq.s32 	%p152, %r97, %r63;
	or.pred  	%p153, %p152, %p151;
	setp.eq.s32 	%p154, %r97, %r64;
	or.pred  	%p155, %p154, %p153;
	setp.eq.s32 	%p156, %r97, %r65;
	or.pred  	%p157, %p156, %p155;
	@%p157 bra 	$L__BB0_57;

	mul.wide.s32 	%rd83, %r288, 4;
	add.s64 	%rd84, %rd3, %rd83;
	st.local.u32 	[%rd84], %r97;
	add.s32 	%r288, %r288, 1;

$L__BB0_57:
	add.s32 	%r283, %r283, 4;
	setp.lt.u32 	%p158, %r283, %r58;
	@%p158 bra 	$L__BB0_45;

$L__BB0_58:
	add.s32 	%r279, %r279, 1;
	setp.lt.u32 	%p159, %r279, %r12;
	@%p159 bra 	$L__BB0_32;

$L__BB0_59:
	ld.param.u32 	%r255, [_Z18computeImpulsivityPKdPdjjjjj_param_4];
	setp.ge.u32 	%p266, %r293, %r12;
	add.s32 	%r214, %r255, -1;
	setp.le.u32 	%p160, %r214, %r2;
	or.pred  	%p162, %p160, %p266;
	@%p162 bra 	$L__BB0_88;

	ld.param.u32 	%r260, [_Z18computeImpulsivityPKdPdjjjjj_param_2];
	mul.lo.s32 	%r259, %r175, %r260;
	ld.param.u32 	%r258, [_Z18computeImpulsivityPKdPdjjjjj_param_6];
	shl.b32 	%r257, %r258, 1;
	add.s32 	%r215, %r257, 1;
	add.s32 	%r104, %r11, %r215;
	add.s32 	%r105, %r8, %r293;
	add.s32 	%r106, %r9, %r11;
	add.s32 	%r107, %r3, %r259;
	sub.s32 	%r216, %r259, %r175;
	add.s32 	%r108, %r216, %r4;
	add.s32 	%r217, %r259, %r175;
	add.s32 	%r109, %r217, %r4;
	add.s32 	%r218, %r259, %r4;
	add.s32 	%r110, %r218, -1;
	add.s32 	%r111, %r218, 1;
	and.b32  	%r112, %r215, 3;
	add.s32 	%r113, %r11, 1;
	add.s32 	%r114, %r11, 2;
	add.s32 	%r115, %r11, 3;
	setp.ge.u32 	%p163, %r11, %r104;
	setp.ge.u32 	%p164, %r11, %r106;
	setp.eq.s32 	%p174, %r112, 1;
	setp.lt.u32 	%p195, %r257, 3;

$L__BB0_61:
	@%p163 bra 	$L__BB0_87;

	mad.lo.s32 	%r118, %r293, %r175, %r107;
	setp.ge.u32 	%p165, %r293, %r105;
	or.pred  	%p166, %p165, %p164;
	@%p166 bra 	$L__BB0_65;

	add.s32 	%r119, %r118, %r11;
	setp.eq.s32 	%p167, %r119, %r108;
	setp.eq.s32 	%p168, %r119, %r109;
	or.pred  	%p169, %p168, %p167;
	setp.eq.s32 	%p170, %r119, %r110;
	or.pred  	%p171, %p170, %p169;
	setp.eq.s32 	%p172, %r119, %r111;
	or.pred  	%p173, %p172, %p171;
	@%p173 bra 	$L__BB0_65;

	mul.wide.s32 	%rd85, %r288, 4;
	add.s64 	%rd86, %rd3, %rd85;
	st.local.u32 	[%rd86], %r119;
	add.s32 	%r288, %r288, 1;

$L__BB0_65:
	mov.u32 	%r297, %r113;
	@%p174 bra 	$L__BB0_72;

	setp.ge.u32 	%p176, %r113, %r106;
	or.pred  	%p177, %p165, %p176;
	@%p177 bra 	$L__BB0_69;

	add.s32 	%r122, %r118, %r113;
	setp.eq.s32 	%p178, %r122, %r108;
	setp.eq.s32 	%p179, %r122, %r109;
	or.pred  	%p180, %p179, %p178;
	setp.eq.s32 	%p181, %r122, %r110;
	or.pred  	%p182, %p181, %p180;
	setp.eq.s32 	%p183, %r122, %r111;
	or.pred  	%p184, %p183, %p182;
	@%p184 bra 	$L__BB0_69;

	mul.wide.s32 	%rd87, %r288, 4;
	add.s64 	%rd88, %rd3, %rd87;
	st.local.u32 	[%rd88], %r122;
	add.s32 	%r288, %r288, 1;

$L__BB0_69:
	setp.ge.u32 	%p185, %r114, %r106;
	or.pred  	%p187, %p165, %p185;
	mov.u32 	%r297, %r115;
	@%p187 bra 	$L__BB0_72;

	add.s32 	%r125, %r118, %r114;
	setp.eq.s32 	%p188, %r125, %r108;
	setp.eq.s32 	%p189, %r125, %r109;
	or.pred  	%p190, %p189, %p188;
	setp.eq.s32 	%p191, %r125, %r110;
	or.pred  	%p192, %p191, %p190;
	setp.eq.s32 	%p193, %r125, %r111;
	or.pred  	%p194, %p193, %p192;
	mov.u32 	%r297, %r115;
	@%p194 bra 	$L__BB0_72;

	mul.wide.s32 	%rd89, %r288, 4;
	add.s64 	%rd90, %rd3, %rd89;
	st.local.u32 	[%rd90], %r125;
	add.s32 	%r288, %r288, 1;
	mov.u32 	%r297, %r115;

$L__BB0_72:
	@%p195 bra 	$L__BB0_87;

$L__BB0_74:
	setp.ge.u32 	%p196, %r297, %r106;
	or.pred  	%p198, %p165, %p196;
	@%p198 bra 	$L__BB0_77;

	add.s32 	%r131, %r118, %r297;
	setp.eq.s32 	%p199, %r131, %r108;
	setp.eq.s32 	%p200, %r131, %r109;
	or.pred  	%p201, %p200, %p199;
	setp.eq.s32 	%p202, %r131, %r110;
	or.pred  	%p203, %p202, %p201;
	setp.eq.s32 	%p204, %r131, %r111;
	or.pred  	%p205, %p204, %p203;
	@%p205 bra 	$L__BB0_77;

	mul.wide.s32 	%rd91, %r288, 4;
	add.s64 	%rd92, %rd3, %rd91;
	st.local.u32 	[%rd92], %r131;
	add.s32 	%r288, %r288, 1;

$L__BB0_77:
	add.s32 	%r134, %r297, 1;
	setp.ge.u32 	%p206, %r134, %r106;
	or.pred  	%p208, %p165, %p206;
	@%p208 bra 	$L__BB0_80;

	add.s32 	%r135, %r118, %r134;
	setp.eq.s32 	%p209, %r135, %r108;
	setp.eq.s32 	%p210, %r135, %r109;
	or.pred  	%p211, %p210, %p209;
	setp.eq.s32 	%p212, %r135, %r110;
	or.pred  	%p213, %p212, %p211;
	setp.eq.s32 	%p214, %r135, %r111;
	or.pred  	%p215, %p214, %p213;
	@%p215 bra 	$L__BB0_80;

	mul.wide.s32 	%rd93, %r288, 4;
	add.s64 	%rd94, %rd3, %rd93;
	st.local.u32 	[%rd94], %r135;
	add.s32 	%r288, %r288, 1;

$L__BB0_80:
	add.s32 	%r138, %r297, 2;
	setp.ge.u32 	%p216, %r138, %r106;
	or.pred  	%p218, %p165, %p216;
	@%p218 bra 	$L__BB0_83;

	add.s32 	%r139, %r118, %r138;
	setp.eq.s32 	%p219, %r139, %r108;
	setp.eq.s32 	%p220, %r139, %r109;
	or.pred  	%p221, %p220, %p219;
	setp.eq.s32 	%p222, %r139, %r110;
	or.pred  	%p223, %p222, %p221;
	setp.eq.s32 	%p224, %r139, %r111;
	or.pred  	%p225, %p224, %p223;
	@%p225 bra 	$L__BB0_83;

	mul.wide.s32 	%rd95, %r288, 4;
	add.s64 	%rd96, %rd3, %rd95;
	st.local.u32 	[%rd96], %r139;
	add.s32 	%r288, %r288, 1;

$L__BB0_83:
	add.s32 	%r142, %r297, 3;
	setp.ge.u32 	%p226, %r142, %r106;
	or.pred  	%p228, %p165, %p226;
	@%p228 bra 	$L__BB0_86;

	add.s32 	%r143, %r118, %r142;
	setp.eq.s32 	%p229, %r143, %r108;
	setp.eq.s32 	%p230, %r143, %r109;
	or.pred  	%p231, %p230, %p229;
	setp.eq.s32 	%p232, %r143, %r110;
	or.pred  	%p233, %p232, %p231;
	setp.eq.s32 	%p234, %r143, %r111;
	or.pred  	%p235, %p234, %p233;
	@%p235 bra 	$L__BB0_86;

	mul.wide.s32 	%rd97, %r288, 4;
	add.s64 	%rd98, %rd3, %rd97;
	st.local.u32 	[%rd98], %r143;
	add.s32 	%r288, %r288, 1;

$L__BB0_86:
	add.s32 	%r297, %r297, 4;
	setp.lt.u32 	%p236, %r297, %r104;
	@%p236 bra 	$L__BB0_74;

$L__BB0_87:
	add.s32 	%r293, %r293, 1;
	setp.lt.u32 	%p237, %r293, %r12;
	@%p237 bra 	$L__BB0_61;

$L__BB0_88:
	ld.param.u32 	%r256, [_Z18computeImpulsivityPKdPdjjjjj_param_5];
	and.b32  	%r219, %r7, 255;
	and.b32  	%r150, %r288, 255;
	min.u32 	%r151, %r150, %r219;
	min.u32 	%r152, %r151, %r256;
	setp.eq.s32 	%p238, %r151, 0;
	@%p238 bra 	$L__BB0_95;

	mul.wide.u32 	%rd99, %r4, 8;
	add.s64 	%rd100, %rd1, %rd99;
	ld.global.f64 	%fd1, [%rd100];
	or.b32  	%r221, %r7, -256;
	xor.b32  	%r222, %r221, 255;
	not.b32 	%r223, %r150;
	max.u32 	%r224, %r222, %r223;
	mov.u32 	%r225, -2;
	sub.s32 	%r226, %r225, %r224;
	and.b32  	%r310, %r151, 3;
	setp.lt.u32 	%p239, %r226, 3;
	mov.u32 	%r309, 0;
	@%p239 bra 	$L__BB0_92;

	sub.s32 	%r308, %r151, %r310;

$L__BB0_91:
	mul.wide.u32 	%rd101, %r309, 4;
	add.s64 	%rd102, %rd3, %rd101;
	ld.local.u32 	%r228, [%rd102];
	mul.wide.u32 	%rd103, %r228, 8;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.f64 	%fd11, [%rd104];
	sub.f64 	%fd12, %fd1, %fd11;
	abs.f64 	%fd13, %fd12;
	cvt.rn.f32.f64 	%f17, %fd13;
	add.s64 	%rd105, %rd130, %rd101;
	st.local.f32 	[%rd105], %f17;
	ld.local.u32 	%r229, [%rd102+4];
	mul.wide.u32 	%rd106, %r229, 8;
	add.s64 	%rd107, %rd1, %rd106;
	ld.global.f64 	%fd14, [%rd107];
	sub.f64 	%fd15, %fd1, %fd14;
	abs.f64 	%fd16, %fd15;
	cvt.rn.f32.f64 	%f18, %fd16;
	st.local.f32 	[%rd105+4], %f18;
	ld.local.u32 	%r230, [%rd102+8];
	mul.wide.u32 	%rd108, %r230, 8;
	add.s64 	%rd109, %rd1, %rd108;
	ld.global.f64 	%fd17, [%rd109];
	sub.f64 	%fd18, %fd1, %fd17;
	abs.f64 	%fd19, %fd18;
	cvt.rn.f32.f64 	%f19, %fd19;
	st.local.f32 	[%rd105+8], %f19;
	ld.local.u32 	%r231, [%rd102+12];
	mul.wide.u32 	%rd110, %r231, 8;
	add.s64 	%rd111, %rd1, %rd110;
	ld.global.f64 	%fd20, [%rd111];
	sub.f64 	%fd21, %fd1, %fd20;
	abs.f64 	%fd22, %fd21;
	cvt.rn.f32.f64 	%f20, %fd22;
	st.local.f32 	[%rd105+12], %f20;
	add.s32 	%r309, %r309, 4;
	add.s32 	%r308, %r308, -4;
	setp.ne.s32 	%p240, %r308, 0;
	@%p240 bra 	$L__BB0_91;

$L__BB0_92:
	setp.eq.s32 	%p241, %r310, 0;
	@%p241 bra 	$L__BB0_95;

	mul.wide.u32 	%rd112, %r309, 4;
	add.s64 	%rd127, %rd130, %rd112;
	add.s64 	%rd126, %rd3, %rd112;

$L__BB0_94:
	.pragma "nounroll";
	ld.local.u32 	%r232, [%rd126];
	mul.wide.u32 	%rd113, %r232, 8;
	add.s64 	%rd114, %rd1, %rd113;
	ld.global.f64 	%fd23, [%rd114];
	sub.f64 	%fd24, %fd1, %fd23;
	abs.f64 	%fd25, %fd24;
	cvt.rn.f32.f64 	%f21, %fd25;
	st.local.f32 	[%rd127], %f21;
	add.s64 	%rd127, %rd127, 4;
	add.s64 	%rd126, %rd126, 4;
	add.s32 	%r310, %r310, -1;
	setp.ne.s32 	%p242, %r310, 0;
	@%p242 bra 	$L__BB0_94;

$L__BB0_95:
	add.s32 	%r162, %r151, -1;
	setp.lt.s32 	%p243, %r151, 2;
	@%p243 bra 	$L__BB0_119;

	add.s32 	%r163, %r151, -2;
	mov.u32 	%r311, 0;

$L__BB0_97:
	sub.s32 	%r165, %r162, %r311;
	not.b32 	%r234, %r311;
	add.s32 	%r235, %r151, %r234;
	setp.lt.s32 	%p244, %r235, 1;
	@%p244 bra 	$L__BB0_118;

	and.b32  	%r166, %r165, 3;
	sub.s32 	%r237, %r163, %r311;
	setp.lt.u32 	%p245, %r237, 3;
	mov.u32 	%r314, 0;
	@%p245 bra 	$L__BB0_109;

	sub.s32 	%r313, %r165, %r166;
	ld.local.f32 	%f28, [%rd130];

$L__BB0_100:
	cvt.s64.s32 	%rd10, %r314;
	mul.wide.s32 	%rd115, %r314, 4;
	add.s64 	%rd11, %rd130, %rd115;
	ld.local.f32 	%f29, [%rd11+4];
	setp.leu.f32 	%p246, %f28, %f29;
	add.s64 	%rd12, %rd3, %rd115;
	@%p246 bra 	$L__BB0_102;

	st.local.f32 	[%rd11], %f29;
	st.local.f32 	[%rd11+4], %f28;
	ld.local.u32 	%r239, [%rd12];
	ld.local.u32 	%r240, [%rd12+4];
	st.local.u32 	[%rd12], %r240;
	st.local.u32 	[%rd12+4], %r239;
	mov.f32 	%f29, %f28;

$L__BB0_102:
	ld.local.f32 	%f30, [%rd11+8];
	setp.leu.f32 	%p247, %f29, %f30;
	@%p247 bra 	$L__BB0_104;

	st.local.f32 	[%rd11+4], %f30;
	st.local.f32 	[%rd11+8], %f29;
	ld.local.u32 	%r241, [%rd12+4];
	ld.local.u32 	%r242, [%rd12+8];
	st.local.u32 	[%rd12+4], %r242;
	st.local.u32 	[%rd12+8], %r241;
	mov.f32 	%f30, %f29;

$L__BB0_104:
	ld.local.f32 	%f31, [%rd11+12];
	setp.leu.f32 	%p248, %f30, %f31;
	@%p248 bra 	$L__BB0_106;

	st.local.f32 	[%rd11+8], %f31;
	st.local.f32 	[%rd11+12], %f30;
	ld.local.u32 	%r243, [%rd12+8];
	ld.local.u32 	%r244, [%rd12+12];
	st.local.u32 	[%rd12+8], %r244;
	st.local.u32 	[%rd12+12], %r243;
	mov.f32 	%f31, %f30;

$L__BB0_106:
	cvt.u32.u64 	%r245, %rd10;
	add.s32 	%r314, %r245, 4;
	ld.local.f32 	%f28, [%rd11+16];
	setp.leu.f32 	%p249, %f31, %f28;
	@%p249 bra 	$L__BB0_108;

	st.local.f32 	[%rd11+12], %f28;
	st.local.f32 	[%rd11+16], %f31;
	ld.local.u32 	%r246, [%rd12+12];
	ld.local.u32 	%r247, [%rd12+16];
	st.local.u32 	[%rd12+12], %r247;
	st.local.u32 	[%rd12+16], %r246;
	mov.f32 	%f28, %f31;

$L__BB0_108:
	add.s32 	%r313, %r313, -4;
	setp.ne.s32 	%p250, %r313, 0;
	@%p250 bra 	$L__BB0_100;

$L__BB0_109:
	setp.eq.s32 	%p251, %r166, 0;
	@%p251 bra 	$L__BB0_118;

	mul.wide.s32 	%rd116, %r314, 4;
	add.s64 	%rd13, %rd130, %rd116;
	ld.local.f32 	%f33, [%rd13+4];
	ld.local.f32 	%f12, [%rd13];
	setp.leu.f32 	%p252, %f12, %f33;
	add.s64 	%rd14, %rd3, %rd116;
	@%p252 bra 	$L__BB0_112;

	st.local.f32 	[%rd13], %f33;
	st.local.f32 	[%rd13+4], %f12;
	ld.local.u32 	%r248, [%rd14];
	ld.local.u32 	%r249, [%rd14+4];
	st.local.u32 	[%rd14], %r249;
	st.local.u32 	[%rd14+4], %r248;
	mov.f32 	%f33, %f12;

$L__BB0_112:
	setp.eq.s32 	%p253, %r166, 1;
	@%p253 bra 	$L__BB0_118;

	ld.local.f32 	%f34, [%rd13+8];
	setp.leu.f32 	%p254, %f33, %f34;
	@%p254 bra 	$L__BB0_115;

	st.local.f32 	[%rd13+4], %f34;
	st.local.f32 	[%rd13+8], %f33;
	ld.local.u32 	%r250, [%rd14+4];
	ld.local.u32 	%r251, [%rd14+8];
	st.local.u32 	[%rd14+4], %r251;
	st.local.u32 	[%rd14+8], %r250;
	mov.f32 	%f34, %f33;

$L__BB0_115:
	setp.eq.s32 	%p255, %r166, 2;
	@%p255 bra 	$L__BB0_118;

	ld.local.f32 	%f16, [%rd13+12];
	setp.leu.f32 	%p256, %f34, %f16;
	@%p256 bra 	$L__BB0_118;

	st.local.f32 	[%rd13+8], %f16;
	st.local.f32 	[%rd13+12], %f34;
	ld.local.u32 	%r252, [%rd14+8];
	ld.local.u32 	%r253, [%rd14+12];
	st.local.u32 	[%rd14+8], %r253;
	st.local.u32 	[%rd14+12], %r252;

$L__BB0_118:
	add.s32 	%r311, %r311, 1;
	setp.lt.s32 	%p257, %r311, %r162;
	@%p257 bra 	$L__BB0_97;

$L__BB0_119:
	mul.wide.u32 	%rd117, %r152, 4;
	add.s64 	%rd15, %rd130, %rd117;
	setp.eq.s64 	%p258, %rd117, 0;
	mov.f64 	%fd10, 0d0000000000000000;
	@%p258 bra 	$L__BB0_127;

	shl.b32 	%r254, %r152, 2;
	cvt.u64.u32 	%rd118, %r254;
	add.s64 	%rd17, %rd118, -4;
	shr.u64 	%rd119, %rd17, 2;
	add.s64 	%rd120, %rd119, 1;
	and.b64  	%rd18, %rd120, 3;
	setp.eq.s64 	%p259, %rd18, 0;
	mov.f64 	%fd44, 0d0000000000000000;
	@%p259 bra 	$L__BB0_123;

	neg.s64 	%rd128, %rd18;

$L__BB0_122:
	.pragma "nounroll";
	ld.local.f32 	%f22, [%rd130];
	cvt.f64.f32 	%fd30, %f22;
	add.f64 	%fd44, %fd44, %fd30;
	add.s64 	%rd130, %rd130, 4;
	add.s64 	%rd128, %rd128, 1;
	setp.ne.s64 	%p260, %rd128, 0;
	@%p260 bra 	$L__BB0_122;

$L__BB0_123:
	setp.lt.u64 	%p261, %rd17, 12;
	@%p261 bra 	$L__BB0_126;

	sub.s64 	%rd131, %rd15, %rd130;

$L__BB0_125:
	ld.local.f32 	%f23, [%rd130];
	cvt.f64.f32 	%fd31, %f23;
	add.f64 	%fd32, %fd44, %fd31;
	ld.local.f32 	%f24, [%rd130+4];
	cvt.f64.f32 	%fd33, %f24;
	add.f64 	%fd34, %fd32, %fd33;
	ld.local.f32 	%f25, [%rd130+8];
	cvt.f64.f32 	%fd35, %f25;
	add.f64 	%fd36, %fd34, %fd35;
	ld.local.f32 	%f26, [%rd130+12];
	cvt.f64.f32 	%fd37, %f26;
	add.f64 	%fd44, %fd36, %fd37;
	add.s64 	%rd131, %rd131, -16;
	setp.ne.s64 	%p262, %rd131, 0;
	add.s64 	%rd130, %rd130, 16;
	@%p262 bra 	$L__BB0_125;

$L__BB0_126:
	cvt.rn.f32.f64 	%f27, %fd44;
	cvt.f64.f32 	%fd10, %f27;

$L__BB0_127:
	ld.param.u64 	%rd125, [_Z18computeImpulsivityPKdPdjjjjj_param_1];
	cvta.to.global.u64 	%rd121, %rd125;
	mul.wide.u32 	%rd122, %r4, 8;
	add.s64 	%rd32, %rd121, %rd122;
	setp.gtu.f64 	%p263, %fd10, 0d3FA1EB851EB851EC;
	@%p263 bra 	$L__BB0_129;
	bra.uni 	$L__BB0_128;

$L__BB0_129:
	setp.ltu.f64 	%p264, %fd10, 0d3FAEB851EB851EB8;
	@%p264 bra 	$L__BB0_131;
	bra.uni 	$L__BB0_130;

$L__BB0_131:
	add.f64 	%fd38, %fd10, 0dBFA1EB851EB851EC;
	div.rn.f64 	%fd39, %fd38, 0d3F99999999999998;
	st.global.f64 	[%rd32], %fd39;
	bra.uni 	$L__BB0_132;

$L__BB0_128:
	mov.u64 	%rd123, 0;
	st.global.u64 	[%rd32], %rd123;
	bra.uni 	$L__BB0_132;

$L__BB0_130:
	mov.u64 	%rd124, 4607182418800017408;
	st.global.u64 	[%rd32], %rd124;

$L__BB0_132:
	ret;

}
	// .globl	_Z11fuzzyFilterPdjPKdjjjj
.visible .entry _Z11fuzzyFilterPdjPKdjjjj(
	.param .u64 _Z11fuzzyFilterPdjPKdjjjj_param_0,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_1,
	.param .u64 _Z11fuzzyFilterPdjPKdjjjj_param_2,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_3,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_4,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_5,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_6
)
{
	.local .align 4 .b8 	__local_depot1[120];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<273>;
	.reg .f32 	%f<47>;
	.reg .b32 	%r<309>;
	.reg .f64 	%fd<107>;
	.reg .b64 	%rd<114>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd20, [_Z11fuzzyFilterPdjPKdjjjj_param_0];
	ld.param.u32 	%r174, [_Z11fuzzyFilterPdjPKdjjjj_param_1];
	ld.param.u64 	%rd21, [_Z11fuzzyFilterPdjPKdjjjj_param_2];
	ld.param.u32 	%r175, [_Z11fuzzyFilterPdjPKdjjjj_param_3];
	ld.param.u32 	%r176, [_Z11fuzzyFilterPdjPKdjjjj_param_4];
	ld.param.u32 	%r177, [_Z11fuzzyFilterPdjPKdjjjj_param_5];
	cvta.to.global.u64 	%rd1, %rd21;
	cvta.to.global.u64 	%rd2, %rd20;
	add.u64 	%rd3, %SPL, 0;
	add.u64 	%rd113, %SPL, 60;
	mov.u32 	%r179, %ntid.x;
	mov.u32 	%r180, %ctaid.x;
	mov.u32 	%r181, %tid.x;
	mad.lo.s32 	%r1, %r180, %r179, %r181;
	mov.u32 	%r182, %ntid.y;
	mov.u32 	%r183, %ctaid.y;
	mov.u32 	%r184, %tid.y;
	mad.lo.s32 	%r2, %r183, %r182, %r184;
	mul.lo.s32 	%r185, %r2, %r175;
	mul.lo.s32 	%r3, %r185, %r176;
	add.s32 	%r4, %r3, %r1;
	mul.lo.s32 	%r5, %r176, %r175;
	setp.ge.u32 	%p1, %r1, %r5;
	setp.ge.u32 	%p2, %r2, %r177;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_128;

	mul.wide.u32 	%rd24, %r4, 8;
	add.s64 	%rd6, %rd2, %rd24;
	ld.global.f64 	%fd12, [%rd6];
	cvt.rn.f32.f64 	%f1, %fd12;
	div.u32 	%r187, %r1, %r176;
	cvt.u64.u32 	%rd25, %r187;
	cvt.u64.u32 	%rd26, %r174;
	sub.s64 	%rd27, %rd25, %rd26;
	neg.s64 	%rd28, %rd27;
	shr.s64 	%rd29, %rd27, 63;
	cvt.u32.u64 	%r188, %rd28;
	cvt.u32.u64 	%r189, %rd29;
	and.b32  	%r190, %r189, %r188;
	add.s64 	%rd30, %rd25, %rd26;
	cvt.u64.u32 	%rd31, %r175;
	setp.lt.u64 	%p4, %rd30, %rd31;
	mov.u64 	%rd32, 1;
	sub.s64 	%rd33, %rd32, %rd31;
	add.s64 	%rd34, %rd33, %rd30;
	selp.b64 	%rd35, 0, %rd34, %p4;
	cvt.u32.u64 	%r191, %rd35;
	mul.lo.s32 	%r192, %r187, %r176;
	sub.s32 	%r193, %r1, %r192;
	cvt.u64.u32 	%rd36, %r193;
	sub.s64 	%rd37, %rd36, %rd26;
	neg.s64 	%rd38, %rd37;
	shr.s64 	%rd39, %rd37, 63;
	cvt.u32.u64 	%r194, %rd38;
	cvt.u32.u64 	%r195, %rd39;
	and.b32  	%r196, %r195, %r194;
	add.s64 	%rd40, %rd36, %rd26;
	cvt.u64.u32 	%rd41, %r176;
	setp.lt.u64 	%p5, %rd40, %rd41;
	sub.s64 	%rd42, %rd32, %rd41;
	add.s64 	%rd43, %rd42, %rd40;
	selp.b64 	%rd44, 0, %rd43, %p5;
	cvt.u32.u64 	%r197, %rd44;
	shl.b32 	%r6, %r174, 1;
	or.b32  	%r198, %r6, 1;
	sub.s32 	%r199, %r198, %r190;
	sub.s32 	%r7, %r199, %r191;
	sub.s32 	%r200, %r198, %r196;
	sub.s32 	%r8, %r200, %r197;
	sub.s32 	%r201, %r187, %r174;
	add.s32 	%r286, %r201, %r190;
	sub.s32 	%r202, %r193, %r174;
	add.s32 	%r10, %r202, %r196;
	add.s32 	%r11, %r286, %r198;
	setp.ge.u32 	%p6, %r286, %r11;
	mov.u32 	%r281, 0;
	@%p6 bra 	$L__BB1_29;

	add.s32 	%r204, %r6, 1;
	add.s32 	%r12, %r10, %r204;
	add.s32 	%r13, %r7, %r286;
	add.s32 	%r14, %r8, %r10;
	sub.s32 	%r205, %r4, %r176;
	add.s32 	%r15, %r205, -1;
	add.s32 	%r16, %r205, 1;
	add.s32 	%r206, %r4, %r176;
	add.s32 	%r17, %r206, -1;
	add.s32 	%r18, %r206, 1;
	and.b32  	%r19, %r204, 3;
	add.s32 	%r20, %r10, 1;
	add.s32 	%r21, %r10, 2;
	add.s32 	%r22, %r10, 3;
	setp.ge.u32 	%p7, %r10, %r12;
	setp.ge.u32 	%p8, %r10, %r14;
	setp.eq.s32 	%p18, %r19, 1;
	setp.lt.u32 	%p39, %r6, 3;
	mov.u32 	%r258, %r286;

$L__BB1_3:
	@%p7 bra 	$L__BB1_28;

	mad.lo.s32 	%r25, %r258, %r176, %r3;
	setp.ge.u32 	%p9, %r258, %r13;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB1_7;

	add.s32 	%r26, %r25, %r10;
	setp.eq.s32 	%p11, %r26, %r15;
	setp.eq.s32 	%p12, %r26, %r16;
	or.pred  	%p13, %p12, %p11;
	setp.eq.s32 	%p14, %r26, %r17;
	or.pred  	%p15, %p14, %p13;
	setp.eq.s32 	%p16, %r26, %r18;
	or.pred  	%p17, %p16, %p15;
	@%p17 bra 	$L__BB1_7;

	mul.wide.s32 	%rd45, %r281, 4;
	add.s64 	%rd46, %rd113, %rd45;
	st.local.u32 	[%rd46], %r26;
	add.s32 	%r281, %r281, 1;

$L__BB1_7:
	mov.u32 	%r262, %r20;
	@%p18 bra 	$L__BB1_14;

	setp.ge.u32 	%p20, %r20, %r14;
	or.pred  	%p21, %p9, %p20;
	@%p21 bra 	$L__BB1_11;

	add.s32 	%r29, %r25, %r20;
	setp.eq.s32 	%p22, %r29, %r15;
	setp.eq.s32 	%p23, %r29, %r16;
	or.pred  	%p24, %p23, %p22;
	setp.eq.s32 	%p25, %r29, %r17;
	or.pred  	%p26, %p25, %p24;
	setp.eq.s32 	%p27, %r29, %r18;
	or.pred  	%p28, %p27, %p26;
	@%p28 bra 	$L__BB1_11;

	mul.wide.s32 	%rd47, %r281, 4;
	add.s64 	%rd48, %rd113, %rd47;
	st.local.u32 	[%rd48], %r29;
	add.s32 	%r281, %r281, 1;

$L__BB1_11:
	setp.ge.u32 	%p29, %r21, %r14;
	or.pred  	%p31, %p9, %p29;
	mov.u32 	%r262, %r22;
	@%p31 bra 	$L__BB1_14;

	add.s32 	%r32, %r25, %r21;
	setp.eq.s32 	%p32, %r32, %r15;
	setp.eq.s32 	%p33, %r32, %r16;
	or.pred  	%p34, %p33, %p32;
	setp.eq.s32 	%p35, %r32, %r17;
	or.pred  	%p36, %p35, %p34;
	setp.eq.s32 	%p37, %r32, %r18;
	or.pred  	%p38, %p37, %p36;
	mov.u32 	%r262, %r22;
	@%p38 bra 	$L__BB1_14;

	mul.wide.s32 	%rd49, %r281, 4;
	add.s64 	%rd50, %rd113, %rd49;
	st.local.u32 	[%rd50], %r32;
	add.s32 	%r281, %r281, 1;
	mov.u32 	%r262, %r22;

$L__BB1_14:
	@%p39 bra 	$L__BB1_28;

$L__BB1_15:
	setp.ge.u32 	%p40, %r262, %r14;
	or.pred  	%p42, %p9, %p40;
	@%p42 bra 	$L__BB1_18;

	add.s32 	%r38, %r25, %r262;
	setp.eq.s32 	%p43, %r38, %r15;
	setp.eq.s32 	%p44, %r38, %r16;
	or.pred  	%p45, %p44, %p43;
	setp.eq.s32 	%p46, %r38, %r17;
	or.pred  	%p47, %p46, %p45;
	setp.eq.s32 	%p48, %r38, %r18;
	or.pred  	%p49, %p48, %p47;
	@%p49 bra 	$L__BB1_18;

	mul.wide.s32 	%rd51, %r281, 4;
	add.s64 	%rd52, %rd113, %rd51;
	st.local.u32 	[%rd52], %r38;
	add.s32 	%r281, %r281, 1;

$L__BB1_18:
	add.s32 	%r41, %r262, 1;
	setp.ge.u32 	%p50, %r41, %r14;
	or.pred  	%p52, %p9, %p50;
	@%p52 bra 	$L__BB1_21;

	add.s32 	%r42, %r25, %r41;
	setp.eq.s32 	%p53, %r42, %r15;
	setp.eq.s32 	%p54, %r42, %r16;
	or.pred  	%p55, %p54, %p53;
	setp.eq.s32 	%p56, %r42, %r17;
	or.pred  	%p57, %p56, %p55;
	setp.eq.s32 	%p58, %r42, %r18;
	or.pred  	%p59, %p58, %p57;
	@%p59 bra 	$L__BB1_21;

	mul.wide.s32 	%rd53, %r281, 4;
	add.s64 	%rd54, %rd113, %rd53;
	st.local.u32 	[%rd54], %r42;
	add.s32 	%r281, %r281, 1;

$L__BB1_21:
	add.s32 	%r45, %r262, 2;
	setp.ge.u32 	%p60, %r45, %r14;
	or.pred  	%p62, %p9, %p60;
	@%p62 bra 	$L__BB1_24;

	add.s32 	%r46, %r25, %r45;
	setp.eq.s32 	%p63, %r46, %r15;
	setp.eq.s32 	%p64, %r46, %r16;
	or.pred  	%p65, %p64, %p63;
	setp.eq.s32 	%p66, %r46, %r17;
	or.pred  	%p67, %p66, %p65;
	setp.eq.s32 	%p68, %r46, %r18;
	or.pred  	%p69, %p68, %p67;
	@%p69 bra 	$L__BB1_24;

	mul.wide.s32 	%rd55, %r281, 4;
	add.s64 	%rd56, %rd113, %rd55;
	st.local.u32 	[%rd56], %r46;
	add.s32 	%r281, %r281, 1;

$L__BB1_24:
	add.s32 	%r49, %r262, 3;
	setp.ge.u32 	%p70, %r49, %r14;
	or.pred  	%p72, %p9, %p70;
	@%p72 bra 	$L__BB1_27;

	add.s32 	%r50, %r25, %r49;
	setp.eq.s32 	%p73, %r50, %r15;
	setp.eq.s32 	%p74, %r50, %r16;
	or.pred  	%p75, %p74, %p73;
	setp.eq.s32 	%p76, %r50, %r17;
	or.pred  	%p77, %p76, %p75;
	setp.eq.s32 	%p78, %r50, %r18;
	or.pred  	%p79, %p78, %p77;
	@%p79 bra 	$L__BB1_27;

	mul.wide.s32 	%rd57, %r281, 4;
	add.s64 	%rd58, %rd113, %rd57;
	st.local.u32 	[%rd58], %r50;
	add.s32 	%r281, %r281, 1;

$L__BB1_27:
	add.s32 	%r262, %r262, 4;
	setp.lt.u32 	%p80, %r262, %r12;
	@%p80 bra 	$L__BB1_15;

$L__BB1_28:
	add.s32 	%r258, %r258, 1;
	setp.lt.u32 	%p81, %r258, %r11;
	@%p81 bra 	$L__BB1_3;

$L__BB1_29:
	setp.ge.u32 	%p271, %r286, %r11;
	setp.eq.s32 	%p82, %r2, 0;
	or.pred  	%p84, %p82, %p271;
	@%p84 bra 	$L__BB1_57;

	ld.param.u32 	%r257, [_Z11fuzzyFilterPdjPKdjjjj_param_3];
	mul.lo.s32 	%r256, %r176, %r257;
	ld.param.u32 	%r255, [_Z11fuzzyFilterPdjPKdjjjj_param_1];
	shl.b32 	%r254, %r255, 1;
	add.s32 	%r207, %r254, 1;
	mov.u32 	%r208, 1;
	add.s32 	%r57, %r10, %r207;
	add.s32 	%r58, %r7, %r286;
	add.s32 	%r59, %r8, %r10;
	sub.s32 	%r60, %r3, %r256;
	add.s32 	%r209, %r256, %r176;
	sub.s32 	%r61, %r4, %r209;
	sub.s32 	%r210, %r176, %r256;
	add.s32 	%r62, %r210, %r4;
	not.b32 	%r211, %r256;
	add.s32 	%r63, %r4, %r211;
	sub.s32 	%r212, %r208, %r256;
	add.s32 	%r64, %r212, %r4;
	and.b32  	%r65, %r207, 3;
	add.s32 	%r66, %r10, 1;
	add.s32 	%r67, %r10, 2;
	add.s32 	%r68, %r10, 3;
	setp.ge.u32 	%p85, %r10, %r57;
	setp.ge.u32 	%p86, %r10, %r59;
	setp.eq.s32 	%p96, %r65, 1;
	setp.lt.u32 	%p117, %r254, 3;
	mov.u32 	%r272, %r286;

$L__BB1_31:
	@%p85 bra 	$L__BB1_56;

	mad.lo.s32 	%r71, %r272, %r176, %r60;
	setp.ge.u32 	%p87, %r272, %r58;
	or.pred  	%p88, %p87, %p86;
	@%p88 bra 	$L__BB1_35;

	add.s32 	%r72, %r71, %r10;
	setp.eq.s32 	%p89, %r72, %r61;
	setp.eq.s32 	%p90, %r72, %r62;
	or.pred  	%p91, %p90, %p89;
	setp.eq.s32 	%p92, %r72, %r63;
	or.pred  	%p93, %p92, %p91;
	setp.eq.s32 	%p94, %r72, %r64;
	or.pred  	%p95, %p94, %p93;
	@%p95 bra 	$L__BB1_35;

	mul.wide.s32 	%rd59, %r281, 4;
	add.s64 	%rd60, %rd113, %rd59;
	st.local.u32 	[%rd60], %r72;
	add.s32 	%r281, %r281, 1;

$L__BB1_35:
	mov.u32 	%r276, %r66;
	@%p96 bra 	$L__BB1_42;

	setp.ge.u32 	%p98, %r66, %r59;
	or.pred  	%p99, %p87, %p98;
	@%p99 bra 	$L__BB1_39;

	add.s32 	%r75, %r71, %r66;
	setp.eq.s32 	%p100, %r75, %r61;
	setp.eq.s32 	%p101, %r75, %r62;
	or.pred  	%p102, %p101, %p100;
	setp.eq.s32 	%p103, %r75, %r63;
	or.pred  	%p104, %p103, %p102;
	setp.eq.s32 	%p105, %r75, %r64;
	or.pred  	%p106, %p105, %p104;
	@%p106 bra 	$L__BB1_39;

	mul.wide.s32 	%rd61, %r281, 4;
	add.s64 	%rd62, %rd113, %rd61;
	st.local.u32 	[%rd62], %r75;
	add.s32 	%r281, %r281, 1;

$L__BB1_39:
	setp.ge.u32 	%p107, %r67, %r59;
	or.pred  	%p109, %p87, %p107;
	mov.u32 	%r276, %r68;
	@%p109 bra 	$L__BB1_42;

	add.s32 	%r78, %r71, %r67;
	setp.eq.s32 	%p110, %r78, %r61;
	setp.eq.s32 	%p111, %r78, %r62;
	or.pred  	%p112, %p111, %p110;
	setp.eq.s32 	%p113, %r78, %r63;
	or.pred  	%p114, %p113, %p112;
	setp.eq.s32 	%p115, %r78, %r64;
	or.pred  	%p116, %p115, %p114;
	mov.u32 	%r276, %r68;
	@%p116 bra 	$L__BB1_42;

	mul.wide.s32 	%rd63, %r281, 4;
	add.s64 	%rd64, %rd113, %rd63;
	st.local.u32 	[%rd64], %r78;
	add.s32 	%r281, %r281, 1;
	mov.u32 	%r276, %r68;

$L__BB1_42:
	@%p117 bra 	$L__BB1_56;

$L__BB1_43:
	setp.ge.u32 	%p118, %r276, %r59;
	or.pred  	%p120, %p87, %p118;
	@%p120 bra 	$L__BB1_46;

	add.s32 	%r84, %r71, %r276;
	setp.eq.s32 	%p121, %r84, %r61;
	setp.eq.s32 	%p122, %r84, %r62;
	or.pred  	%p123, %p122, %p121;
	setp.eq.s32 	%p124, %r84, %r63;
	or.pred  	%p125, %p124, %p123;
	setp.eq.s32 	%p126, %r84, %r64;
	or.pred  	%p127, %p126, %p125;
	@%p127 bra 	$L__BB1_46;

	mul.wide.s32 	%rd65, %r281, 4;
	add.s64 	%rd66, %rd113, %rd65;
	st.local.u32 	[%rd66], %r84;
	add.s32 	%r281, %r281, 1;

$L__BB1_46:
	add.s32 	%r87, %r276, 1;
	setp.ge.u32 	%p128, %r87, %r59;
	or.pred  	%p130, %p87, %p128;
	@%p130 bra 	$L__BB1_49;

	add.s32 	%r88, %r71, %r87;
	setp.eq.s32 	%p131, %r88, %r61;
	setp.eq.s32 	%p132, %r88, %r62;
	or.pred  	%p133, %p132, %p131;
	setp.eq.s32 	%p134, %r88, %r63;
	or.pred  	%p135, %p134, %p133;
	setp.eq.s32 	%p136, %r88, %r64;
	or.pred  	%p137, %p136, %p135;
	@%p137 bra 	$L__BB1_49;

	mul.wide.s32 	%rd67, %r281, 4;
	add.s64 	%rd68, %rd113, %rd67;
	st.local.u32 	[%rd68], %r88;
	add.s32 	%r281, %r281, 1;

$L__BB1_49:
	add.s32 	%r91, %r276, 2;
	setp.ge.u32 	%p138, %r91, %r59;
	or.pred  	%p140, %p87, %p138;
	@%p140 bra 	$L__BB1_52;

	add.s32 	%r92, %r71, %r91;
	setp.eq.s32 	%p141, %r92, %r61;
	setp.eq.s32 	%p142, %r92, %r62;
	or.pred  	%p143, %p142, %p141;
	setp.eq.s32 	%p144, %r92, %r63;
	or.pred  	%p145, %p144, %p143;
	setp.eq.s32 	%p146, %r92, %r64;
	or.pred  	%p147, %p146, %p145;
	@%p147 bra 	$L__BB1_52;

	mul.wide.s32 	%rd69, %r281, 4;
	add.s64 	%rd70, %rd113, %rd69;
	st.local.u32 	[%rd70], %r92;
	add.s32 	%r281, %r281, 1;

$L__BB1_52:
	add.s32 	%r95, %r276, 3;
	setp.ge.u32 	%p148, %r95, %r59;
	or.pred  	%p150, %p87, %p148;
	@%p150 bra 	$L__BB1_55;

	add.s32 	%r96, %r71, %r95;
	setp.eq.s32 	%p151, %r96, %r61;
	setp.eq.s32 	%p152, %r96, %r62;
	or.pred  	%p153, %p152, %p151;
	setp.eq.s32 	%p154, %r96, %r63;
	or.pred  	%p155, %p154, %p153;
	setp.eq.s32 	%p156, %r96, %r64;
	or.pred  	%p157, %p156, %p155;
	@%p157 bra 	$L__BB1_55;

	mul.wide.s32 	%rd71, %r281, 4;
	add.s64 	%rd72, %rd113, %rd71;
	st.local.u32 	[%rd72], %r96;
	add.s32 	%r281, %r281, 1;

$L__BB1_55:
	add.s32 	%r276, %r276, 4;
	setp.lt.u32 	%p158, %r276, %r57;
	@%p158 bra 	$L__BB1_43;

$L__BB1_56:
	add.s32 	%r272, %r272, 1;
	setp.lt.u32 	%p159, %r272, %r11;
	@%p159 bra 	$L__BB1_31;

$L__BB1_57:
	ld.param.u32 	%r248, [_Z11fuzzyFilterPdjPKdjjjj_param_5];
	setp.ge.u32 	%p272, %r286, %r11;
	add.s32 	%r213, %r248, -1;
	setp.le.u32 	%p160, %r213, %r2;
	or.pred  	%p162, %p160, %p272;
	@%p162 bra 	$L__BB1_85;

	ld.param.u32 	%r253, [_Z11fuzzyFilterPdjPKdjjjj_param_3];
	mul.lo.s32 	%r252, %r176, %r253;
	ld.param.u32 	%r251, [_Z11fuzzyFilterPdjPKdjjjj_param_1];
	shl.b32 	%r250, %r251, 1;
	add.s32 	%r214, %r250, 1;
	add.s32 	%r103, %r10, %r214;
	add.s32 	%r104, %r7, %r286;
	add.s32 	%r105, %r8, %r10;
	add.s32 	%r106, %r3, %r252;
	sub.s32 	%r215, %r252, %r176;
	add.s32 	%r107, %r215, %r4;
	add.s32 	%r216, %r252, %r176;
	add.s32 	%r108, %r216, %r4;
	add.s32 	%r217, %r252, %r4;
	add.s32 	%r109, %r217, -1;
	add.s32 	%r110, %r217, 1;
	and.b32  	%r111, %r214, 3;
	add.s32 	%r112, %r10, 1;
	add.s32 	%r113, %r10, 2;
	add.s32 	%r114, %r10, 3;
	setp.ge.u32 	%p163, %r10, %r103;
	setp.ge.u32 	%p164, %r10, %r105;
	setp.eq.s32 	%p174, %r111, 1;
	setp.lt.u32 	%p195, %r250, 3;

$L__BB1_59:
	@%p163 bra 	$L__BB1_84;

	mad.lo.s32 	%r117, %r286, %r176, %r106;
	setp.ge.u32 	%p165, %r286, %r104;
	or.pred  	%p166, %p165, %p164;
	@%p166 bra 	$L__BB1_63;

	add.s32 	%r118, %r117, %r10;
	setp.eq.s32 	%p167, %r118, %r107;
	setp.eq.s32 	%p168, %r118, %r108;
	or.pred  	%p169, %p168, %p167;
	setp.eq.s32 	%p170, %r118, %r109;
	or.pred  	%p171, %p170, %p169;
	setp.eq.s32 	%p172, %r118, %r110;
	or.pred  	%p173, %p172, %p171;
	@%p173 bra 	$L__BB1_63;

	mul.wide.s32 	%rd73, %r281, 4;
	add.s64 	%rd74, %rd113, %rd73;
	st.local.u32 	[%rd74], %r118;
	add.s32 	%r281, %r281, 1;

$L__BB1_63:
	mov.u32 	%r290, %r112;
	@%p174 bra 	$L__BB1_70;

	setp.ge.u32 	%p176, %r112, %r105;
	or.pred  	%p177, %p165, %p176;
	@%p177 bra 	$L__BB1_67;

	add.s32 	%r121, %r117, %r112;
	setp.eq.s32 	%p178, %r121, %r107;
	setp.eq.s32 	%p179, %r121, %r108;
	or.pred  	%p180, %p179, %p178;
	setp.eq.s32 	%p181, %r121, %r109;
	or.pred  	%p182, %p181, %p180;
	setp.eq.s32 	%p183, %r121, %r110;
	or.pred  	%p184, %p183, %p182;
	@%p184 bra 	$L__BB1_67;

	mul.wide.s32 	%rd75, %r281, 4;
	add.s64 	%rd76, %rd113, %rd75;
	st.local.u32 	[%rd76], %r121;
	add.s32 	%r281, %r281, 1;

$L__BB1_67:
	setp.ge.u32 	%p185, %r113, %r105;
	or.pred  	%p187, %p165, %p185;
	mov.u32 	%r290, %r114;
	@%p187 bra 	$L__BB1_70;

	add.s32 	%r124, %r117, %r113;
	setp.eq.s32 	%p188, %r124, %r107;
	setp.eq.s32 	%p189, %r124, %r108;
	or.pred  	%p190, %p189, %p188;
	setp.eq.s32 	%p191, %r124, %r109;
	or.pred  	%p192, %p191, %p190;
	setp.eq.s32 	%p193, %r124, %r110;
	or.pred  	%p194, %p193, %p192;
	mov.u32 	%r290, %r114;
	@%p194 bra 	$L__BB1_70;

	mul.wide.s32 	%rd77, %r281, 4;
	add.s64 	%rd78, %rd113, %rd77;
	st.local.u32 	[%rd78], %r124;
	add.s32 	%r281, %r281, 1;
	mov.u32 	%r290, %r114;

$L__BB1_70:
	@%p195 bra 	$L__BB1_84;

$L__BB1_71:
	setp.ge.u32 	%p196, %r290, %r105;
	or.pred  	%p198, %p165, %p196;
	@%p198 bra 	$L__BB1_74;

	add.s32 	%r130, %r117, %r290;
	setp.eq.s32 	%p199, %r130, %r107;
	setp.eq.s32 	%p200, %r130, %r108;
	or.pred  	%p201, %p200, %p199;
	setp.eq.s32 	%p202, %r130, %r109;
	or.pred  	%p203, %p202, %p201;
	setp.eq.s32 	%p204, %r130, %r110;
	or.pred  	%p205, %p204, %p203;
	@%p205 bra 	$L__BB1_74;

	mul.wide.s32 	%rd79, %r281, 4;
	add.s64 	%rd80, %rd113, %rd79;
	st.local.u32 	[%rd80], %r130;
	add.s32 	%r281, %r281, 1;

$L__BB1_74:
	add.s32 	%r133, %r290, 1;
	setp.ge.u32 	%p206, %r133, %r105;
	or.pred  	%p208, %p165, %p206;
	@%p208 bra 	$L__BB1_77;

	add.s32 	%r134, %r117, %r133;
	setp.eq.s32 	%p209, %r134, %r107;
	setp.eq.s32 	%p210, %r134, %r108;
	or.pred  	%p211, %p210, %p209;
	setp.eq.s32 	%p212, %r134, %r109;
	or.pred  	%p213, %p212, %p211;
	setp.eq.s32 	%p214, %r134, %r110;
	or.pred  	%p215, %p214, %p213;
	@%p215 bra 	$L__BB1_77;

	mul.wide.s32 	%rd81, %r281, 4;
	add.s64 	%rd82, %rd113, %rd81;
	st.local.u32 	[%rd82], %r134;
	add.s32 	%r281, %r281, 1;

$L__BB1_77:
	add.s32 	%r137, %r290, 2;
	setp.ge.u32 	%p216, %r137, %r105;
	or.pred  	%p218, %p165, %p216;
	@%p218 bra 	$L__BB1_80;

	add.s32 	%r138, %r117, %r137;
	setp.eq.s32 	%p219, %r138, %r107;
	setp.eq.s32 	%p220, %r138, %r108;
	or.pred  	%p221, %p220, %p219;
	setp.eq.s32 	%p222, %r138, %r109;
	or.pred  	%p223, %p222, %p221;
	setp.eq.s32 	%p224, %r138, %r110;
	or.pred  	%p225, %p224, %p223;
	@%p225 bra 	$L__BB1_80;

	mul.wide.s32 	%rd83, %r281, 4;
	add.s64 	%rd84, %rd113, %rd83;
	st.local.u32 	[%rd84], %r138;
	add.s32 	%r281, %r281, 1;

$L__BB1_80:
	add.s32 	%r141, %r290, 3;
	setp.ge.u32 	%p226, %r141, %r105;
	or.pred  	%p228, %p165, %p226;
	@%p228 bra 	$L__BB1_83;

	add.s32 	%r142, %r117, %r141;
	setp.eq.s32 	%p229, %r142, %r107;
	setp.eq.s32 	%p230, %r142, %r108;
	or.pred  	%p231, %p230, %p229;
	setp.eq.s32 	%p232, %r142, %r109;
	or.pred  	%p233, %p232, %p231;
	setp.eq.s32 	%p234, %r142, %r110;
	or.pred  	%p235, %p234, %p233;
	@%p235 bra 	$L__BB1_83;

	mul.wide.s32 	%rd85, %r281, 4;
	add.s64 	%rd86, %rd113, %rd85;
	st.local.u32 	[%rd86], %r142;
	add.s32 	%r281, %r281, 1;

$L__BB1_83:
	add.s32 	%r290, %r290, 4;
	setp.lt.u32 	%p236, %r290, %r103;
	@%p236 bra 	$L__BB1_71;

$L__BB1_84:
	add.s32 	%r286, %r286, 1;
	setp.lt.u32 	%p237, %r286, %r11;
	@%p237 bra 	$L__BB1_59;

$L__BB1_85:
	and.b32  	%r149, %r281, 255;
	setp.eq.s32 	%p238, %r149, 0;
	add.s32 	%r150, %r149, -1;
	@%p238 bra 	$L__BB1_92;

	cvt.f64.f32 	%fd1, %f1;
	and.b32  	%r303, %r281, 3;
	setp.lt.u32 	%p239, %r150, 3;
	mov.u32 	%r302, 0;
	@%p239 bra 	$L__BB1_89;

	sub.s32 	%r301, %r149, %r303;

$L__BB1_88:
	mul.wide.u32 	%rd87, %r302, 4;
	add.s64 	%rd88, %rd113, %rd87;
	ld.local.u32 	%r220, [%rd88];
	mul.wide.u32 	%rd89, %r220, 8;
	add.s64 	%rd90, %rd2, %rd89;
	ld.global.f64 	%fd13, [%rd90];
	sub.f64 	%fd14, %fd1, %fd13;
	abs.f64 	%fd15, %fd14;
	cvt.rn.f32.f64 	%f25, %fd15;
	add.s64 	%rd91, %rd3, %rd87;
	st.local.f32 	[%rd91], %f25;
	ld.local.u32 	%r221, [%rd88+4];
	mul.wide.u32 	%rd92, %r221, 8;
	add.s64 	%rd93, %rd2, %rd92;
	ld.global.f64 	%fd16, [%rd93];
	sub.f64 	%fd17, %fd1, %fd16;
	abs.f64 	%fd18, %fd17;
	cvt.rn.f32.f64 	%f26, %fd18;
	st.local.f32 	[%rd91+4], %f26;
	ld.local.u32 	%r222, [%rd88+8];
	mul.wide.u32 	%rd94, %r222, 8;
	add.s64 	%rd95, %rd2, %rd94;
	ld.global.f64 	%fd19, [%rd95];
	sub.f64 	%fd20, %fd1, %fd19;
	abs.f64 	%fd21, %fd20;
	cvt.rn.f32.f64 	%f27, %fd21;
	st.local.f32 	[%rd91+8], %f27;
	ld.local.u32 	%r223, [%rd88+12];
	mul.wide.u32 	%rd96, %r223, 8;
	add.s64 	%rd97, %rd2, %rd96;
	ld.global.f64 	%fd22, [%rd97];
	sub.f64 	%fd23, %fd1, %fd22;
	abs.f64 	%fd24, %fd23;
	cvt.rn.f32.f64 	%f28, %fd24;
	st.local.f32 	[%rd91+12], %f28;
	add.s32 	%r302, %r302, 4;
	add.s32 	%r301, %r301, -4;
	setp.ne.s32 	%p240, %r301, 0;
	@%p240 bra 	$L__BB1_88;

$L__BB1_89:
	setp.eq.s32 	%p241, %r303, 0;
	@%p241 bra 	$L__BB1_92;

	mul.wide.u32 	%rd98, %r302, 4;
	add.s64 	%rd112, %rd3, %rd98;
	add.s64 	%rd111, %rd113, %rd98;

$L__BB1_91:
	.pragma "nounroll";
	ld.local.u32 	%r224, [%rd111];
	mul.wide.u32 	%rd99, %r224, 8;
	add.s64 	%rd100, %rd2, %rd99;
	ld.global.f64 	%fd25, [%rd100];
	sub.f64 	%fd26, %fd1, %fd25;
	abs.f64 	%fd27, %fd26;
	cvt.rn.f32.f64 	%f29, %fd27;
	st.local.f32 	[%rd112], %f29;
	add.s64 	%rd112, %rd112, 4;
	add.s64 	%rd111, %rd111, 4;
	add.s32 	%r303, %r303, -1;
	setp.ne.s32 	%p242, %r303, 0;
	@%p242 bra 	$L__BB1_91;

$L__BB1_92:
	setp.lt.u32 	%p243, %r149, 2;
	@%p243 bra 	$L__BB1_116;

	add.s32 	%r160, %r149, -2;
	mov.u32 	%r304, 0;

$L__BB1_94:
	sub.s32 	%r162, %r150, %r304;
	not.b32 	%r226, %r304;
	add.s32 	%r227, %r149, %r226;
	setp.lt.s32 	%p244, %r227, 1;
	@%p244 bra 	$L__BB1_115;

	and.b32  	%r163, %r162, 3;
	sub.s32 	%r229, %r160, %r304;
	setp.lt.u32 	%p245, %r229, 3;
	mov.u32 	%r307, 0;
	@%p245 bra 	$L__BB1_106;

	sub.s32 	%r306, %r162, %r163;
	ld.local.f32 	%f36, [%rd3];

$L__BB1_97:
	cvt.s64.s32 	%rd13, %r307;
	mul.wide.s32 	%rd101, %r307, 4;
	add.s64 	%rd14, %rd3, %rd101;
	ld.local.f32 	%f37, [%rd14+4];
	setp.leu.f32 	%p246, %f36, %f37;
	add.s64 	%rd15, %rd113, %rd101;
	@%p246 bra 	$L__BB1_99;

	st.local.f32 	[%rd14], %f37;
	st.local.f32 	[%rd14+4], %f36;
	ld.local.u32 	%r231, [%rd15];
	ld.local.u32 	%r232, [%rd15+4];
	st.local.u32 	[%rd15], %r232;
	st.local.u32 	[%rd15+4], %r231;
	mov.f32 	%f37, %f36;

$L__BB1_99:
	ld.local.f32 	%f38, [%rd14+8];
	setp.leu.f32 	%p247, %f37, %f38;
	@%p247 bra 	$L__BB1_101;

	st.local.f32 	[%rd14+4], %f38;
	st.local.f32 	[%rd14+8], %f37;
	ld.local.u32 	%r233, [%rd15+4];
	ld.local.u32 	%r234, [%rd15+8];
	st.local.u32 	[%rd15+4], %r234;
	st.local.u32 	[%rd15+8], %r233;
	mov.f32 	%f38, %f37;

$L__BB1_101:
	ld.local.f32 	%f39, [%rd14+12];
	setp.leu.f32 	%p248, %f38, %f39;
	@%p248 bra 	$L__BB1_103;

	st.local.f32 	[%rd14+8], %f39;
	st.local.f32 	[%rd14+12], %f38;
	ld.local.u32 	%r235, [%rd15+8];
	ld.local.u32 	%r236, [%rd15+12];
	st.local.u32 	[%rd15+8], %r236;
	st.local.u32 	[%rd15+12], %r235;
	mov.f32 	%f39, %f38;

$L__BB1_103:
	cvt.u32.u64 	%r237, %rd13;
	add.s32 	%r307, %r237, 4;
	ld.local.f32 	%f36, [%rd14+16];
	setp.leu.f32 	%p249, %f39, %f36;
	@%p249 bra 	$L__BB1_105;

	st.local.f32 	[%rd14+12], %f36;
	st.local.f32 	[%rd14+16], %f39;
	ld.local.u32 	%r238, [%rd15+12];
	ld.local.u32 	%r239, [%rd15+16];
	st.local.u32 	[%rd15+12], %r239;
	st.local.u32 	[%rd15+16], %r238;
	mov.f32 	%f36, %f39;

$L__BB1_105:
	add.s32 	%r306, %r306, -4;
	setp.ne.s32 	%p250, %r306, 0;
	@%p250 bra 	$L__BB1_97;

$L__BB1_106:
	setp.eq.s32 	%p251, %r163, 0;
	@%p251 bra 	$L__BB1_115;

	mul.wide.s32 	%rd102, %r307, 4;
	add.s64 	%rd16, %rd3, %rd102;
	ld.local.f32 	%f41, [%rd16+4];
	ld.local.f32 	%f13, [%rd16];
	setp.leu.f32 	%p252, %f13, %f41;
	add.s64 	%rd17, %rd113, %rd102;
	@%p252 bra 	$L__BB1_109;

	st.local.f32 	[%rd16], %f41;
	st.local.f32 	[%rd16+4], %f13;
	ld.local.u32 	%r240, [%rd17];
	ld.local.u32 	%r241, [%rd17+4];
	st.local.u32 	[%rd17], %r241;
	st.local.u32 	[%rd17+4], %r240;
	mov.f32 	%f41, %f13;

$L__BB1_109:
	setp.eq.s32 	%p253, %r163, 1;
	@%p253 bra 	$L__BB1_115;

	ld.local.f32 	%f42, [%rd16+8];
	setp.leu.f32 	%p254, %f41, %f42;
	@%p254 bra 	$L__BB1_112;

	st.local.f32 	[%rd16+4], %f42;
	st.local.f32 	[%rd16+8], %f41;
	ld.local.u32 	%r242, [%rd17+4];
	ld.local.u32 	%r243, [%rd17+8];
	st.local.u32 	[%rd17+4], %r243;
	st.local.u32 	[%rd17+8], %r242;
	mov.f32 	%f42, %f41;

$L__BB1_112:
	setp.eq.s32 	%p255, %r163, 2;
	@%p255 bra 	$L__BB1_115;

	ld.local.f32 	%f17, [%rd16+12];
	setp.leu.f32 	%p256, %f42, %f17;
	@%p256 bra 	$L__BB1_115;

	st.local.f32 	[%rd16+8], %f17;
	st.local.f32 	[%rd16+12], %f42;
	ld.local.u32 	%r244, [%rd17+8];
	ld.local.u32 	%r245, [%rd17+12];
	st.local.u32 	[%rd17+8], %r245;
	st.local.u32 	[%rd17+12], %r244;

$L__BB1_115:
	add.s32 	%r304, %r304, 1;
	setp.lt.s32 	%p257, %r304, %r150;
	@%p257 bra 	$L__BB1_94;

$L__BB1_116:
	ld.param.u32 	%r249, [_Z11fuzzyFilterPdjPKdjjjj_param_6];
	min.u32 	%r171, %r149, %r249;
	setp.eq.s32 	%p258, %r171, 0;
	mov.f32 	%f45, 0f00000000;
	mov.f32 	%f46, %f45;
	@%p258 bra 	$L__BB1_127;

	cvt.u64.u32 	%rd108, %r4;
	cvt.f64.f32 	%fd2, %f1;
	shl.b64 	%rd103, %rd108, 3;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.f64 	%fd3, [%rd104];
	mov.f64 	%fd28, 0d3FF0000000000000;
	sub.f64 	%fd4, %fd28, %fd3;
	mov.u32 	%r308, 0;

$L__BB1_118:
	ld.local.u32 	%r247, [%rd113];
	mul.wide.u32 	%rd105, %r247, 8;
	add.s64 	%rd106, %rd2, %rd105;
	ld.global.f64 	%fd30, [%rd106];
	cvt.rn.f32.f64 	%f20, %fd30;
	cvt.f64.f32 	%fd31, %f20;
	add.s64 	%rd107, %rd1, %rd105;
	ld.global.f64 	%fd5, [%rd107];
	sub.f64 	%fd32, %fd2, %fd31;
	abs.f64 	%fd6, %fd32;
	setp.le.f64 	%p259, %fd6, 0d3F7F7BA49A5CEC48;
	mov.f64 	%fd105, %fd28;
	@%p259 bra 	$L__BB1_121;

	setp.ge.f64 	%p260, %fd6, 0d3F9F7BA49A5CEC48;
	mov.f64 	%fd105, 0d0000000000000000;
	@%p260 bra 	$L__BB1_121;

	div.rn.f64 	%fd34, %fd6, 0dBF979CBB73C5B136;
	add.f64 	%fd105, %fd34, 0d3FF5555555555555;

$L__BB1_121:
	setp.lt.f64 	%p261, %fd6, 0d3F8F7BA49A5CEC48;
	setp.gt.f64 	%p262, %fd6, 0d3F7F7BA49A5CEC48;
	and.pred  	%p263, %p262, %p261;
	@%p263 bra 	$L__BB1_125;
	bra.uni 	$L__BB1_122;

$L__BB1_125:
	add.f64 	%fd39, %fd6, 0dBF7F7BA49A5CEC48;
	div.rn.f64 	%fd40, %fd39, 0d3FFF5C28F5C28F5C;
	div.rn.f64 	%fd106, %fd40, 0d406FE00000000000;
	bra.uni 	$L__BB1_126;

$L__BB1_122:
	setp.ge.f64 	%p264, %fd6, 0d3F8F7BA49A5CEC48;
	setp.le.f64 	%p265, %fd6, 0d3F979CBB73C5B136;
	and.pred  	%p266, %p264, %p265;
	mov.f64 	%fd106, 0d3FF0000000000000;
	@%p266 bra 	$L__BB1_126;

	setp.leu.f64 	%p267, %fd6, 0d3F979CBB73C5B136;
	setp.geu.f64 	%p268, %fd6, 0d3F9F7BA49A5CEC48;
	mov.f64 	%fd106, 0d0000000000000000;
	or.pred  	%p269, %p267, %p268;
	@%p269 bra 	$L__BB1_126;

	div.rn.f64 	%fd37, %fd6, 0dBFFF5C28F5C28F5C;
	div.rn.f64 	%fd38, %fd37, 0d406FE00000000000;
	add.f64 	%fd106, %fd38, 0d4010000000000000;

$L__BB1_126:
	mov.f64 	%fd41, 0d3FF0000000000000;
	sub.f64 	%fd42, %fd41, %fd5;
	sub.f64 	%fd43, %fd41, %fd42;
	mul.f64 	%fd44, %fd3, %fd42;
	mul.f64 	%fd45, %fd44, %fd105;
	mul.f64 	%fd46, %fd4, %fd42;
	mul.f64 	%fd47, %fd46, %fd106;
	sub.f64 	%fd48, %fd41, %fd105;
	mul.f64 	%fd49, %fd46, %fd48;
	add.f64 	%fd50, %fd49, %fd47;
	mul.f64 	%fd51, %fd49, %fd47;
	sub.f64 	%fd52, %fd50, %fd51;
	add.f64 	%fd53, %fd45, %fd52;
	mul.f64 	%fd54, %fd45, %fd52;
	sub.f64 	%fd55, %fd53, %fd54;
	add.f64 	%fd56, %fd43, %fd55;
	mul.f64 	%fd57, %fd43, %fd55;
	sub.f64 	%fd58, %fd56, %fd57;
	mul.f64 	%fd59, %fd44, %fd106;
	mul.f64 	%fd60, %fd44, %fd48;
	mul.f64 	%fd61, %fd46, %fd105;
	add.f64 	%fd62, %fd61, %fd60;
	mul.f64 	%fd63, %fd61, %fd60;
	sub.f64 	%fd64, %fd62, %fd63;
	sub.f64 	%fd65, %fd41, %fd58;
	mul.f64 	%fd66, %fd58, %fd65;
	mul.f64 	%fd67, %fd66, 0d3FB9999999999998;
	mul.f64 	%fd68, %fd58, %fd58;
	mul.f64 	%fd69, %fd68, 0d3FB9999999999998;
	mul.f64 	%fd70, %fd69, 0d3FE0000000000000;
	mov.f64 	%fd71, 0d4000000000000000;
	sub.f64 	%fd72, %fd71, %fd59;
	mul.f64 	%fd73, %fd59, %fd72;
	mul.f64 	%fd74, %fd73, 0d3FE999999999999A;
	mul.f64 	%fd75, %fd74, 0d3FE0000000000000;
	mul.f64 	%fd76, %fd64, 0d3FB9999999999998;
	mul.f64 	%fd77, %fd64, %fd76;
	mul.f64 	%fd78, %fd77, 0d3FE0000000000000;
	sub.f64 	%fd79, %fd41, %fd64;
	mul.f64 	%fd80, %fd64, %fd79;
	mul.f64 	%fd81, %fd80, 0d3FB9999999999998;
	mul.f64 	%fd82, %fd65, 0d3FB9999999999998;
	mul.f64 	%fd83, %fd82, 0d3FE0000000000000;
	add.f64 	%fd84, %fd58, %fd58;
	mov.f64 	%fd85, 0d4008000000000000;
	sub.f64 	%fd86, %fd85, %fd84;
	mul.f64 	%fd87, %fd86, 0d3FB9999999999998;
	div.rn.f64 	%fd88, %fd87, 0d4008000000000000;
	fma.rn.f64 	%fd89, %fd64, 0d4000000000000000, 0d400599999999999A;
	fma.rn.f64 	%fd90, %fd64, 0dBFFCCCCCCCCCCCCD, %fd89;
	div.rn.f64 	%fd91, %fd90, 0d4008000000000000;
	mul.f64 	%fd92, %fd79, 0d3FB9999999999998;
	mul.f64 	%fd93, %fd92, 0d3FE0000000000000;
	mul.f64 	%fd94, %fd70, %fd88;
	fma.rn.f64 	%fd95, %fd67, %fd83, %fd94;
	fma.rn.f64 	%fd96, %fd75, 0d3FE0000000000000, %fd95;
	fma.rn.f64 	%fd97, %fd78, %fd91, %fd96;
	fma.rn.f64 	%fd98, %fd81, %fd93, %fd97;
	add.f64 	%fd99, %fd67, %fd70;
	add.f64 	%fd100, %fd75, %fd99;
	add.f64 	%fd101, %fd78, %fd100;
	add.f64 	%fd102, %fd81, %fd101;
	div.rn.f64 	%fd103, %fd98, %fd102;
	cvt.rn.f32.f64 	%f34, %fd103;
	fma.rn.f32 	%f45, %f20, %f34, %f45;
	add.f32 	%f46, %f46, %f34;
	add.s64 	%rd113, %rd113, 4;
	add.s32 	%r308, %r308, 1;
	setp.lt.u32 	%p270, %r308, %r171;
	@%p270 bra 	$L__BB1_118;

$L__BB1_127:
	mul.wide.u32 	%rd110, %r4, 8;
	add.s64 	%rd109, %rd2, %rd110;
	div.rn.f32 	%f35, %f45, %f46;
	cvt.f64.f32 	%fd104, %f35;
	st.global.f64 	[%rd109], %fd104;

$L__BB1_128:
	ret;

}
	// .globl	_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1_
.visible .entry _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1_(
	.param .align 8 .b8 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_0[24],
	.param .u64 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_1
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_1];
	ld.param.u64 	%rd6, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_0+8];
	ld.param.u64 	%rd7, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_0];
	mov.u32 	%r1, %ctaid.x;
	mul.wide.u32 	%rd8, %r1, 512;
	sub.s64 	%rd1, %rd5, %rd8;
	setp.gt.s64 	%p1, %rd1, 511;
	cvta.to.global.u64 	%rd9, %rd7;
	cvta.to.global.u64 	%rd10, %rd6;
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd11, %r2;
	add.s64 	%rd12, %rd8, %rd11;
	shl.b64 	%rd13, %rd12, 3;
	add.s64 	%rd2, %rd9, %rd13;
	add.s64 	%rd3, %rd10, %rd13;
	@%p1 bra 	$L__BB2_5;
	bra.uni 	$L__BB2_1;

$L__BB2_5:
	ld.global.f64 	%fd3, [%rd2];
	st.global.f64 	[%rd3], %fd3;
	ld.global.f64 	%fd4, [%rd2+2048];
	st.global.f64 	[%rd3+2048], %fd4;
	bra.uni 	$L__BB2_6;

$L__BB2_1:
	cvt.s64.s32 	%rd4, %rd1;
	setp.le.s64 	%p2, %rd4, %rd11;
	@%p2 bra 	$L__BB2_3;

	ld.global.f64 	%fd1, [%rd2];
	st.global.f64 	[%rd3], %fd1;

$L__BB2_3:
	add.s32 	%r5, %r2, 256;
	cvt.u64.u32 	%rd15, %r5;
	setp.le.s64 	%p3, %rd4, %rd15;
	@%p3 bra 	$L__BB2_6;

	ld.global.f64 	%fd2, [%rd2+2048];
	st.global.f64 	[%rd3+2048], %fd2;

$L__BB2_6:
	ret;

}
	// .globl	_ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv()
{



	ret;

}
	// .globl	_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_
.visible .entry _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_(
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_0,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_1,
	.param .u32 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_2,
	.param .align 1 .b8 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_3[1],
	.param .f64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_4
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<97>;
	.reg .f32 	%f<143>;
	.reg .b32 	%r<646>;
	.reg .f64 	%fd<534>;
	.reg .b64 	%rd<244>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage[80];

	ld.param.u64 	%rd31, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_0];
	ld.param.u64 	%rd32, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_1];
	ld.param.u32 	%r74, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_2];
	ld.param.f64 	%fd59, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4__param_4];
	setp.eq.s32 	%p1, %r74, 0;
	@%p1 bra 	$L__BB4_78;

	and.b64  	%rd33, %rd31, 15;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB4_37;

	setp.lt.s32 	%p3, %r74, 5120;
	@%p3 bra 	$L__BB4_18;
	bra.uni 	$L__BB4_3;

$L__BB4_18:
	mov.u32 	%r629, %tid.x;
	setp.ge.s32 	%p19, %r629, %r74;
	@%p19 bra 	$L__BB4_20;

	mov.u32 	%r193, %tid.x;
	mul.wide.s32 	%rd93, %r193, 4;
	add.s64 	%rd92, %rd31, %rd93;
	// begin inline asm
	ld.global.nc.u32 %r192, [%rd92];
	// end inline asm
	mov.b32 	%f46, %r192;
	cvt.f64.f32 	%fd517, %f46;
	add.s32 	%r629, %r193, 256;

$L__BB4_20:
	setp.ge.s32 	%p20, %r629, %r74;
	@%p20 bra 	$L__BB4_27;

	not.b32 	%r194, %r629;
	add.s32 	%r19, %r194, %r74;
	shr.u32 	%r195, %r19, 8;
	add.s32 	%r196, %r195, 1;
	and.b32  	%r628, %r196, 3;
	setp.eq.s32 	%p21, %r628, 0;
	@%p21 bra 	$L__BB4_24;

	mul.wide.s32 	%rd94, %r629, 4;
	add.s64 	%rd237, %rd31, %rd94;

$L__BB4_23:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r197, [%rd237];
	// end inline asm
	mov.b32 	%f47, %r197;
	cvt.f64.f32 	%fd175, %f47;
	add.f64 	%fd517, %fd517, %fd175;
	add.s32 	%r629, %r629, 256;
	add.s64 	%rd237, %rd237, 1024;
	add.s32 	%r628, %r628, -1;
	setp.ne.s32 	%p22, %r628, 0;
	@%p22 bra 	$L__BB4_23;

$L__BB4_24:
	setp.lt.u32 	%p23, %r19, 768;
	@%p23 bra 	$L__BB4_27;

	mul.wide.s32 	%rd96, %r629, 4;
	add.s64 	%rd238, %rd31, %rd96;

$L__BB4_26:
	// begin inline asm
	ld.global.nc.u32 %r198, [%rd238];
	// end inline asm
	mov.b32 	%f48, %r198;
	cvt.f64.f32 	%fd176, %f48;
	add.f64 	%fd177, %fd517, %fd176;
	add.s64 	%rd98, %rd238, 1024;
	// begin inline asm
	ld.global.nc.u32 %r199, [%rd98];
	// end inline asm
	mov.b32 	%f49, %r199;
	cvt.f64.f32 	%fd178, %f49;
	add.f64 	%fd179, %fd177, %fd178;
	add.s64 	%rd99, %rd238, 2048;
	// begin inline asm
	ld.global.nc.u32 %r200, [%rd99];
	// end inline asm
	mov.b32 	%f50, %r200;
	cvt.f64.f32 	%fd180, %f50;
	add.f64 	%fd181, %fd179, %fd180;
	add.s64 	%rd100, %rd238, 3072;
	// begin inline asm
	ld.global.nc.u32 %r201, [%rd100];
	// end inline asm
	mov.b32 	%f51, %r201;
	cvt.f64.f32 	%fd182, %f51;
	add.f64 	%fd517, %fd181, %fd182;
	add.s64 	%rd238, %rd238, 4096;
	add.s32 	%r629, %r629, 1024;
	setp.lt.s32 	%p24, %r629, %r74;
	@%p24 bra 	$L__BB4_26;

$L__BB4_27:
	mov.u32 	%r203, %tid.x;
	shr.s32 	%r204, %r203, 31;
	shr.u32 	%r205, %r204, 27;
	add.s32 	%r206, %r203, %r205;
	shr.s32 	%r28, %r206, 5;
	// begin inline asm
	mov.u32 %r202, %laneid;
	// end inline asm
	mov.b64 	%rd101, %fd517;
	mov.b64 	{%r30, %r31}, %rd101;
	shl.b32 	%r207, %r28, 3;
	mov.u32 	%r208, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r209, %r208, %r207;
	setp.gt.s32 	%p25, %r74, 255;
	@%p25 bra 	$L__BB4_33;
	bra.uni 	$L__BB4_28;

$L__BB4_33:
	setp.ne.s32 	%p41, %r202, 0;
	// begin inline asm
	mov.u32 %r270, %laneid;
	// end inline asm
	mov.u32 	%r278, 1;
	mov.u32 	%r319, 31;
	mov.u32 	%r320, -1;
	// begin inline asm
	shfl.sync.down.b32 %r271, %r30, %r278, %r319, %r320;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r276, %r31, %r278, %r319, %r320;
	// end inline asm
	mov.b64 	%rd111, {%r271, %r276};
	mov.b64 	%fd215, %rd111;
	setp.gt.s32 	%p42, %r270, 30;
	add.f64 	%fd216, %fd517, %fd215;
	selp.f64 	%fd217, %fd517, %fd216, %p42;
	mov.b64 	%rd112, %fd217;
	mov.u32 	%r288, 2;
	mov.b64 	{%r282, %r287}, %rd112;
	// begin inline asm
	shfl.sync.down.b32 %r281, %r282, %r288, %r319, %r320;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r286, %r287, %r288, %r319, %r320;
	// end inline asm
	mov.b64 	%rd113, {%r281, %r286};
	mov.b64 	%fd218, %rd113;
	setp.gt.s32 	%p43, %r270, 29;
	add.f64 	%fd219, %fd217, %fd218;
	selp.f64 	%fd220, %fd217, %fd219, %p43;
	mov.b64 	%rd114, %fd220;
	mov.u32 	%r298, 4;
	mov.b64 	{%r292, %r297}, %rd114;
	// begin inline asm
	shfl.sync.down.b32 %r291, %r292, %r298, %r319, %r320;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r296, %r297, %r298, %r319, %r320;
	// end inline asm
	mov.b64 	%rd115, {%r291, %r296};
	mov.b64 	%fd221, %rd115;
	setp.gt.s32 	%p44, %r270, 27;
	add.f64 	%fd222, %fd220, %fd221;
	selp.f64 	%fd223, %fd220, %fd222, %p44;
	mov.b64 	%rd116, %fd223;
	mov.u32 	%r308, 8;
	mov.b64 	{%r302, %r307}, %rd116;
	// begin inline asm
	shfl.sync.down.b32 %r301, %r302, %r308, %r319, %r320;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r306, %r307, %r308, %r319, %r320;
	// end inline asm
	mov.b64 	%rd117, {%r301, %r306};
	mov.b64 	%fd224, %rd117;
	setp.gt.s32 	%p45, %r270, 23;
	add.f64 	%fd225, %fd223, %fd224;
	selp.f64 	%fd226, %fd223, %fd225, %p45;
	mov.b64 	%rd118, %fd226;
	mov.u32 	%r318, 16;
	mov.b64 	{%r312, %r317}, %rd118;
	// begin inline asm
	shfl.sync.down.b32 %r311, %r312, %r318, %r319, %r320;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r316, %r317, %r318, %r319, %r320;
	// end inline asm
	mov.b64 	%rd119, {%r311, %r316};
	mov.b64 	%fd227, %rd119;
	setp.gt.s32 	%p46, %r270, 15;
	add.f64 	%fd228, %fd226, %fd227;
	selp.f64 	%fd533, %fd226, %fd228, %p46;
	@%p41 bra 	$L__BB4_35;

	add.s32 	%r617, %r209, 8;
	st.shared.f64 	[%r617], %fd533;

$L__BB4_35:
	bar.sync 	0;
	setp.ne.s32 	%p47, %r203, 0;
	@%p47 bra 	$L__BB4_76;

	ld.shared.f64 	%fd229, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd230, %fd533, %fd229;
	ld.shared.f64 	%fd231, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd232, %fd230, %fd231;
	ld.shared.f64 	%fd233, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd234, %fd232, %fd233;
	ld.shared.f64 	%fd235, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd236, %fd234, %fd235;
	ld.shared.f64 	%fd237, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd238, %fd236, %fd237;
	ld.shared.f64 	%fd239, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd240, %fd238, %fd239;
	ld.shared.f64 	%fd241, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd240, %fd241;
	bra.uni 	$L__BB4_76;

$L__BB4_78:
	mov.u32 	%r615, %tid.x;
	setp.ne.s32 	%p96, %r615, 0;
	@%p96 bra 	$L__BB4_80;

	cvta.to.global.u64 	%rd233, %rd32;
	st.global.f64 	[%rd233], %fd59;
	bra.uni 	$L__BB4_80;

$L__BB4_37:
	setp.lt.s32 	%p48, %r74, 5120;
	@%p48 bra 	$L__BB4_57;
	bra.uni 	$L__BB4_38;

$L__BB4_57:
	mov.u32 	%r644, %tid.x;
	setp.ge.s32 	%p66, %r644, %r74;
	@%p66 bra 	$L__BB4_59;

	mov.u32 	%r485, %tid.x;
	mul.wide.s32 	%rd205, %r485, 4;
	add.s64 	%rd204, %rd31, %rd205;
	// begin inline asm
	ld.global.nc.u32 %r484, [%rd204];
	// end inline asm
	mov.b32 	%f137, %r484;
	cvt.f64.f32 	%fd532, %f137;
	add.s32 	%r644, %r485, 256;

$L__BB4_59:
	setp.ge.s32 	%p67, %r644, %r74;
	@%p67 bra 	$L__BB4_66;

	not.b32 	%r486, %r644;
	add.s32 	%r60, %r486, %r74;
	shr.u32 	%r487, %r60, 8;
	add.s32 	%r488, %r487, 1;
	and.b32  	%r643, %r488, 3;
	setp.eq.s32 	%p68, %r643, 0;
	@%p68 bra 	$L__BB4_63;

	mul.wide.s32 	%rd206, %r644, 4;
	add.s64 	%rd242, %rd31, %rd206;

$L__BB4_62:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r489, [%rd242];
	// end inline asm
	mov.b32 	%f138, %r489;
	cvt.f64.f32 	%fd437, %f138;
	add.f64 	%fd532, %fd532, %fd437;
	add.s32 	%r644, %r644, 256;
	add.s64 	%rd242, %rd242, 1024;
	add.s32 	%r643, %r643, -1;
	setp.ne.s32 	%p69, %r643, 0;
	@%p69 bra 	$L__BB4_62;

$L__BB4_63:
	setp.lt.u32 	%p70, %r60, 768;
	@%p70 bra 	$L__BB4_66;

	mul.wide.s32 	%rd208, %r644, 4;
	add.s64 	%rd243, %rd31, %rd208;

$L__BB4_65:
	// begin inline asm
	ld.global.nc.u32 %r490, [%rd243];
	// end inline asm
	mov.b32 	%f139, %r490;
	cvt.f64.f32 	%fd438, %f139;
	add.f64 	%fd439, %fd532, %fd438;
	add.s64 	%rd210, %rd243, 1024;
	// begin inline asm
	ld.global.nc.u32 %r491, [%rd210];
	// end inline asm
	mov.b32 	%f140, %r491;
	cvt.f64.f32 	%fd440, %f140;
	add.f64 	%fd441, %fd439, %fd440;
	add.s64 	%rd211, %rd243, 2048;
	// begin inline asm
	ld.global.nc.u32 %r492, [%rd211];
	// end inline asm
	mov.b32 	%f141, %r492;
	cvt.f64.f32 	%fd442, %f141;
	add.f64 	%fd443, %fd441, %fd442;
	add.s64 	%rd212, %rd243, 3072;
	// begin inline asm
	ld.global.nc.u32 %r493, [%rd212];
	// end inline asm
	mov.b32 	%f142, %r493;
	cvt.f64.f32 	%fd444, %f142;
	add.f64 	%fd532, %fd443, %fd444;
	add.s64 	%rd243, %rd243, 4096;
	add.s32 	%r644, %r644, 1024;
	setp.lt.s32 	%p71, %r644, %r74;
	@%p71 bra 	$L__BB4_65;

$L__BB4_66:
	mov.u32 	%r495, %tid.x;
	shr.s32 	%r496, %r495, 31;
	shr.u32 	%r497, %r496, 27;
	add.s32 	%r498, %r495, %r497;
	shr.s32 	%r69, %r498, 5;
	// begin inline asm
	mov.u32 %r494, %laneid;
	// end inline asm
	mov.b64 	%rd213, %fd532;
	mov.b64 	{%r71, %r72}, %rd213;
	shl.b32 	%r499, %r69, 3;
	mov.u32 	%r500, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r501, %r500, %r499;
	setp.gt.s32 	%p72, %r74, 255;
	@%p72 bra 	$L__BB4_72;
	bra.uni 	$L__BB4_67;

$L__BB4_72:
	setp.ne.s32 	%p88, %r494, 0;
	// begin inline asm
	mov.u32 %r562, %laneid;
	// end inline asm
	mov.u32 	%r570, 1;
	mov.u32 	%r611, 31;
	mov.u32 	%r612, -1;
	// begin inline asm
	shfl.sync.down.b32 %r563, %r71, %r570, %r611, %r612;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r568, %r72, %r570, %r611, %r612;
	// end inline asm
	mov.b64 	%rd223, {%r563, %r568};
	mov.b64 	%fd477, %rd223;
	setp.gt.s32 	%p89, %r562, 30;
	add.f64 	%fd478, %fd532, %fd477;
	selp.f64 	%fd479, %fd532, %fd478, %p89;
	mov.b64 	%rd224, %fd479;
	mov.u32 	%r580, 2;
	mov.b64 	{%r574, %r579}, %rd224;
	// begin inline asm
	shfl.sync.down.b32 %r573, %r574, %r580, %r611, %r612;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r578, %r579, %r580, %r611, %r612;
	// end inline asm
	mov.b64 	%rd225, {%r573, %r578};
	mov.b64 	%fd480, %rd225;
	setp.gt.s32 	%p90, %r562, 29;
	add.f64 	%fd481, %fd479, %fd480;
	selp.f64 	%fd482, %fd479, %fd481, %p90;
	mov.b64 	%rd226, %fd482;
	mov.u32 	%r590, 4;
	mov.b64 	{%r584, %r589}, %rd226;
	// begin inline asm
	shfl.sync.down.b32 %r583, %r584, %r590, %r611, %r612;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r588, %r589, %r590, %r611, %r612;
	// end inline asm
	mov.b64 	%rd227, {%r583, %r588};
	mov.b64 	%fd483, %rd227;
	setp.gt.s32 	%p91, %r562, 27;
	add.f64 	%fd484, %fd482, %fd483;
	selp.f64 	%fd485, %fd482, %fd484, %p91;
	mov.b64 	%rd228, %fd485;
	mov.u32 	%r600, 8;
	mov.b64 	{%r594, %r599}, %rd228;
	// begin inline asm
	shfl.sync.down.b32 %r593, %r594, %r600, %r611, %r612;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r598, %r599, %r600, %r611, %r612;
	// end inline asm
	mov.b64 	%rd229, {%r593, %r598};
	mov.b64 	%fd486, %rd229;
	setp.gt.s32 	%p92, %r562, 23;
	add.f64 	%fd487, %fd485, %fd486;
	selp.f64 	%fd488, %fd485, %fd487, %p92;
	mov.b64 	%rd230, %fd488;
	mov.u32 	%r610, 16;
	mov.b64 	{%r604, %r609}, %rd230;
	// begin inline asm
	shfl.sync.down.b32 %r603, %r604, %r610, %r611, %r612;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r608, %r609, %r610, %r611, %r612;
	// end inline asm
	mov.b64 	%rd231, {%r603, %r608};
	mov.b64 	%fd489, %rd231;
	setp.gt.s32 	%p93, %r562, 15;
	add.f64 	%fd490, %fd488, %fd489;
	selp.f64 	%fd533, %fd488, %fd490, %p93;
	@%p88 bra 	$L__BB4_74;

	add.s32 	%r619, %r501, 8;
	st.shared.f64 	[%r619], %fd533;

$L__BB4_74:
	bar.sync 	0;
	setp.ne.s32 	%p94, %r495, 0;
	@%p94 bra 	$L__BB4_76;

	ld.shared.f64 	%fd491, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd492, %fd533, %fd491;
	ld.shared.f64 	%fd493, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd494, %fd492, %fd493;
	ld.shared.f64 	%fd495, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd496, %fd494, %fd495;
	ld.shared.f64 	%fd497, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd498, %fd496, %fd497;
	ld.shared.f64 	%fd499, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd500, %fd498, %fd499;
	ld.shared.f64 	%fd501, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd502, %fd500, %fd501;
	ld.shared.f64 	%fd503, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd502, %fd503;
	bra.uni 	$L__BB4_76;

$L__BB4_3:
	mov.u32 	%r96, %tid.x;
	mul.wide.s32 	%rd54, %r96, 4;
	add.s64 	%rd234, %rd31, %rd54;
	// begin inline asm
	ld.global.nc.u32 %r75, [%rd234];
	// end inline asm
	mov.b32 	%f1, %r75;
	cvt.f64.f32 	%fd60, %f1;
	add.s64 	%rd35, %rd234, 1024;
	// begin inline asm
	ld.global.nc.u32 %r76, [%rd35];
	// end inline asm
	mov.b32 	%f2, %r76;
	cvt.f64.f32 	%fd61, %f2;
	add.s64 	%rd36, %rd234, 2048;
	// begin inline asm
	ld.global.nc.u32 %r77, [%rd36];
	// end inline asm
	mov.b32 	%f3, %r77;
	cvt.f64.f32 	%fd62, %f3;
	add.s64 	%rd37, %rd234, 3072;
	// begin inline asm
	ld.global.nc.u32 %r78, [%rd37];
	// end inline asm
	mov.b32 	%f4, %r78;
	cvt.f64.f32 	%fd63, %f4;
	add.s64 	%rd38, %rd234, 4096;
	// begin inline asm
	ld.global.nc.u32 %r79, [%rd38];
	// end inline asm
	mov.b32 	%f5, %r79;
	cvt.f64.f32 	%fd64, %f5;
	add.s64 	%rd39, %rd234, 5120;
	// begin inline asm
	ld.global.nc.u32 %r80, [%rd39];
	// end inline asm
	mov.b32 	%f6, %r80;
	cvt.f64.f32 	%fd65, %f6;
	add.s64 	%rd40, %rd234, 6144;
	// begin inline asm
	ld.global.nc.u32 %r81, [%rd40];
	// end inline asm
	mov.b32 	%f7, %r81;
	cvt.f64.f32 	%fd66, %f7;
	add.s64 	%rd41, %rd234, 7168;
	// begin inline asm
	ld.global.nc.u32 %r82, [%rd41];
	// end inline asm
	mov.b32 	%f8, %r82;
	cvt.f64.f32 	%fd67, %f8;
	add.s64 	%rd42, %rd234, 8192;
	// begin inline asm
	ld.global.nc.u32 %r83, [%rd42];
	// end inline asm
	mov.b32 	%f9, %r83;
	cvt.f64.f32 	%fd68, %f9;
	add.s64 	%rd43, %rd234, 9216;
	// begin inline asm
	ld.global.nc.u32 %r84, [%rd43];
	// end inline asm
	mov.b32 	%f10, %r84;
	cvt.f64.f32 	%fd69, %f10;
	add.s64 	%rd44, %rd234, 10240;
	// begin inline asm
	ld.global.nc.u32 %r85, [%rd44];
	// end inline asm
	mov.b32 	%f11, %r85;
	cvt.f64.f32 	%fd70, %f11;
	add.s64 	%rd45, %rd234, 11264;
	// begin inline asm
	ld.global.nc.u32 %r86, [%rd45];
	// end inline asm
	mov.b32 	%f12, %r86;
	cvt.f64.f32 	%fd71, %f12;
	add.s64 	%rd46, %rd234, 12288;
	// begin inline asm
	ld.global.nc.u32 %r87, [%rd46];
	// end inline asm
	mov.b32 	%f13, %r87;
	cvt.f64.f32 	%fd72, %f13;
	add.s64 	%rd47, %rd234, 13312;
	// begin inline asm
	ld.global.nc.u32 %r88, [%rd47];
	// end inline asm
	mov.b32 	%f14, %r88;
	cvt.f64.f32 	%fd73, %f14;
	add.s64 	%rd48, %rd234, 14336;
	// begin inline asm
	ld.global.nc.u32 %r89, [%rd48];
	// end inline asm
	mov.b32 	%f15, %r89;
	cvt.f64.f32 	%fd74, %f15;
	add.s64 	%rd49, %rd234, 15360;
	// begin inline asm
	ld.global.nc.u32 %r90, [%rd49];
	// end inline asm
	mov.b32 	%f16, %r90;
	cvt.f64.f32 	%fd75, %f16;
	add.s64 	%rd50, %rd234, 16384;
	// begin inline asm
	ld.global.nc.u32 %r91, [%rd50];
	// end inline asm
	mov.b32 	%f17, %r91;
	cvt.f64.f32 	%fd76, %f17;
	add.s64 	%rd51, %rd234, 17408;
	// begin inline asm
	ld.global.nc.u32 %r92, [%rd51];
	// end inline asm
	mov.b32 	%f18, %r92;
	cvt.f64.f32 	%fd77, %f18;
	add.s64 	%rd52, %rd234, 18432;
	// begin inline asm
	ld.global.nc.u32 %r93, [%rd52];
	// end inline asm
	mov.b32 	%f19, %r93;
	cvt.f64.f32 	%fd78, %f19;
	add.s64 	%rd53, %rd234, 19456;
	// begin inline asm
	ld.global.nc.u32 %r94, [%rd53];
	// end inline asm
	mov.b32 	%f20, %r94;
	cvt.f64.f32 	%fd79, %f20;
	add.f64 	%fd80, %fd60, %fd61;
	add.f64 	%fd81, %fd80, %fd62;
	add.f64 	%fd82, %fd81, %fd63;
	add.f64 	%fd83, %fd82, %fd64;
	add.f64 	%fd84, %fd83, %fd65;
	add.f64 	%fd85, %fd84, %fd66;
	add.f64 	%fd86, %fd85, %fd67;
	add.f64 	%fd87, %fd86, %fd68;
	add.f64 	%fd88, %fd87, %fd69;
	add.f64 	%fd89, %fd88, %fd70;
	add.f64 	%fd90, %fd89, %fd71;
	add.f64 	%fd91, %fd90, %fd72;
	add.f64 	%fd92, %fd91, %fd73;
	add.f64 	%fd93, %fd92, %fd74;
	add.f64 	%fd94, %fd93, %fd75;
	add.f64 	%fd95, %fd94, %fd76;
	add.f64 	%fd96, %fd95, %fd77;
	add.f64 	%fd97, %fd96, %fd78;
	add.f64 	%fd511, %fd97, %fd79;
	setp.lt.s32 	%p4, %r74, 10240;
	mov.u32 	%r621, 5120;
	@%p4 bra 	$L__BB4_6;

	mov.u32 	%r620, %r621;

$L__BB4_5:
	add.s64 	%rd55, %rd234, 20480;
	// begin inline asm
	ld.global.nc.u32 %r98, [%rd55];
	// end inline asm
	mov.b32 	%f21, %r98;
	cvt.f64.f32 	%fd98, %f21;
	add.s64 	%rd56, %rd234, 21504;
	// begin inline asm
	ld.global.nc.u32 %r99, [%rd56];
	// end inline asm
	mov.b32 	%f22, %r99;
	cvt.f64.f32 	%fd99, %f22;
	add.s64 	%rd57, %rd234, 22528;
	// begin inline asm
	ld.global.nc.u32 %r100, [%rd57];
	// end inline asm
	mov.b32 	%f23, %r100;
	cvt.f64.f32 	%fd100, %f23;
	add.s64 	%rd58, %rd234, 23552;
	// begin inline asm
	ld.global.nc.u32 %r101, [%rd58];
	// end inline asm
	mov.b32 	%f24, %r101;
	cvt.f64.f32 	%fd101, %f24;
	add.s64 	%rd59, %rd234, 24576;
	// begin inline asm
	ld.global.nc.u32 %r102, [%rd59];
	// end inline asm
	mov.b32 	%f25, %r102;
	cvt.f64.f32 	%fd102, %f25;
	add.s64 	%rd60, %rd234, 25600;
	// begin inline asm
	ld.global.nc.u32 %r103, [%rd60];
	// end inline asm
	mov.b32 	%f26, %r103;
	cvt.f64.f32 	%fd103, %f26;
	add.s64 	%rd61, %rd234, 26624;
	// begin inline asm
	ld.global.nc.u32 %r104, [%rd61];
	// end inline asm
	mov.b32 	%f27, %r104;
	cvt.f64.f32 	%fd104, %f27;
	add.s64 	%rd62, %rd234, 27648;
	// begin inline asm
	ld.global.nc.u32 %r105, [%rd62];
	// end inline asm
	mov.b32 	%f28, %r105;
	cvt.f64.f32 	%fd105, %f28;
	add.s64 	%rd63, %rd234, 28672;
	// begin inline asm
	ld.global.nc.u32 %r106, [%rd63];
	// end inline asm
	mov.b32 	%f29, %r106;
	cvt.f64.f32 	%fd106, %f29;
	add.s64 	%rd64, %rd234, 29696;
	// begin inline asm
	ld.global.nc.u32 %r107, [%rd64];
	// end inline asm
	mov.b32 	%f30, %r107;
	cvt.f64.f32 	%fd107, %f30;
	add.s64 	%rd65, %rd234, 30720;
	// begin inline asm
	ld.global.nc.u32 %r108, [%rd65];
	// end inline asm
	mov.b32 	%f31, %r108;
	cvt.f64.f32 	%fd108, %f31;
	add.s64 	%rd66, %rd234, 31744;
	// begin inline asm
	ld.global.nc.u32 %r109, [%rd66];
	// end inline asm
	mov.b32 	%f32, %r109;
	cvt.f64.f32 	%fd109, %f32;
	add.s64 	%rd67, %rd234, 32768;
	// begin inline asm
	ld.global.nc.u32 %r110, [%rd67];
	// end inline asm
	mov.b32 	%f33, %r110;
	cvt.f64.f32 	%fd110, %f33;
	add.s64 	%rd68, %rd234, 33792;
	// begin inline asm
	ld.global.nc.u32 %r111, [%rd68];
	// end inline asm
	mov.b32 	%f34, %r111;
	cvt.f64.f32 	%fd111, %f34;
	add.s64 	%rd69, %rd234, 34816;
	// begin inline asm
	ld.global.nc.u32 %r112, [%rd69];
	// end inline asm
	mov.b32 	%f35, %r112;
	cvt.f64.f32 	%fd112, %f35;
	add.s64 	%rd70, %rd234, 35840;
	// begin inline asm
	ld.global.nc.u32 %r113, [%rd70];
	// end inline asm
	mov.b32 	%f36, %r113;
	cvt.f64.f32 	%fd113, %f36;
	add.s64 	%rd71, %rd234, 36864;
	// begin inline asm
	ld.global.nc.u32 %r114, [%rd71];
	// end inline asm
	mov.b32 	%f37, %r114;
	cvt.f64.f32 	%fd114, %f37;
	add.s64 	%rd72, %rd234, 37888;
	// begin inline asm
	ld.global.nc.u32 %r115, [%rd72];
	// end inline asm
	mov.b32 	%f38, %r115;
	cvt.f64.f32 	%fd115, %f38;
	add.s64 	%rd73, %rd234, 38912;
	// begin inline asm
	ld.global.nc.u32 %r116, [%rd73];
	// end inline asm
	mov.b32 	%f39, %r116;
	cvt.f64.f32 	%fd116, %f39;
	add.s64 	%rd74, %rd234, 39936;
	// begin inline asm
	ld.global.nc.u32 %r117, [%rd74];
	// end inline asm
	mov.b32 	%f40, %r117;
	cvt.f64.f32 	%fd117, %f40;
	add.f64 	%fd118, %fd511, %fd98;
	add.f64 	%fd119, %fd118, %fd99;
	add.f64 	%fd120, %fd119, %fd100;
	add.f64 	%fd121, %fd120, %fd101;
	add.f64 	%fd122, %fd121, %fd102;
	add.f64 	%fd123, %fd122, %fd103;
	add.f64 	%fd124, %fd123, %fd104;
	add.f64 	%fd125, %fd124, %fd105;
	add.f64 	%fd126, %fd125, %fd106;
	add.f64 	%fd127, %fd126, %fd107;
	add.f64 	%fd128, %fd127, %fd108;
	add.f64 	%fd129, %fd128, %fd109;
	add.f64 	%fd130, %fd129, %fd110;
	add.f64 	%fd131, %fd130, %fd111;
	add.f64 	%fd132, %fd131, %fd112;
	add.f64 	%fd133, %fd132, %fd113;
	add.f64 	%fd134, %fd133, %fd114;
	add.f64 	%fd135, %fd134, %fd115;
	add.f64 	%fd136, %fd135, %fd116;
	add.f64 	%fd511, %fd136, %fd117;
	add.s32 	%r621, %r620, 5120;
	add.s32 	%r118, %r620, 10240;
	setp.le.s32 	%p5, %r118, %r74;
	mov.u64 	%rd234, %rd55;
	mov.u32 	%r620, %r621;
	@%p5 bra 	$L__BB4_5;

$L__BB4_6:
	setp.ge.s32 	%p6, %r621, %r74;
	@%p6 bra 	$L__BB4_14;

	sub.s32 	%r4, %r74, %r621;
	setp.ge.s32 	%p7, %r96, %r4;
	@%p7 bra 	$L__BB4_14;

	not.b32 	%r120, %r621;
	add.s32 	%r121, %r120, %r74;
	sub.s32 	%r6, %r121, %r96;
	shr.u32 	%r122, %r6, 8;
	add.s32 	%r123, %r122, 1;
	and.b32  	%r623, %r123, 3;
	setp.eq.s32 	%p8, %r623, 0;
	mov.u32 	%r624, %r96;
	@%p8 bra 	$L__BB4_11;

	mov.u32 	%r624, %tid.x;
	add.s32 	%r124, %r624, %r621;
	mul.wide.s32 	%rd75, %r124, 4;
	add.s64 	%rd235, %rd31, %rd75;

$L__BB4_10:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r125, [%rd235];
	// end inline asm
	mov.b32 	%f41, %r125;
	cvt.f64.f32 	%fd138, %f41;
	add.f64 	%fd511, %fd511, %fd138;
	add.s32 	%r624, %r624, 256;
	add.s64 	%rd235, %rd235, 1024;
	add.s32 	%r623, %r623, -1;
	setp.ne.s32 	%p9, %r623, 0;
	@%p9 bra 	$L__BB4_10;

$L__BB4_11:
	setp.lt.u32 	%p10, %r6, 768;
	@%p10 bra 	$L__BB4_14;

	add.s32 	%r126, %r624, %r621;
	mul.wide.s32 	%rd77, %r126, 4;
	add.s64 	%rd236, %rd31, %rd77;

$L__BB4_13:
	// begin inline asm
	ld.global.nc.u32 %r127, [%rd236];
	// end inline asm
	mov.b32 	%f42, %r127;
	cvt.f64.f32 	%fd139, %f42;
	add.f64 	%fd140, %fd511, %fd139;
	add.s64 	%rd79, %rd236, 1024;
	// begin inline asm
	ld.global.nc.u32 %r128, [%rd79];
	// end inline asm
	mov.b32 	%f43, %r128;
	cvt.f64.f32 	%fd141, %f43;
	add.f64 	%fd142, %fd140, %fd141;
	add.s64 	%rd80, %rd236, 2048;
	// begin inline asm
	ld.global.nc.u32 %r129, [%rd80];
	// end inline asm
	mov.b32 	%f44, %r129;
	cvt.f64.f32 	%fd143, %f44;
	add.f64 	%fd144, %fd142, %fd143;
	add.s64 	%rd81, %rd236, 3072;
	// begin inline asm
	ld.global.nc.u32 %r130, [%rd81];
	// end inline asm
	mov.b32 	%f45, %r130;
	cvt.f64.f32 	%fd145, %f45;
	add.f64 	%fd511, %fd144, %fd145;
	add.s64 	%rd236, %rd236, 4096;
	add.s32 	%r624, %r624, 1024;
	setp.lt.s32 	%p11, %r624, %r4;
	@%p11 bra 	$L__BB4_13;

$L__BB4_14:
	// begin inline asm
	mov.u32 %r131, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r132, %laneid;
	// end inline asm
	mov.b64 	%rd82, %fd511;
	mov.u32 	%r140, 1;
	mov.u32 	%r181, 31;
	mov.u32 	%r182, -1;
	mov.b64 	{%r134, %r139}, %rd82;
	// begin inline asm
	shfl.sync.down.b32 %r133, %r134, %r140, %r181, %r182;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r138, %r139, %r140, %r181, %r182;
	// end inline asm
	mov.b64 	%rd83, {%r133, %r138};
	mov.b64 	%fd146, %rd83;
	setp.gt.s32 	%p12, %r132, 30;
	add.f64 	%fd147, %fd511, %fd146;
	selp.f64 	%fd148, %fd511, %fd147, %p12;
	mov.b64 	%rd84, %fd148;
	mov.u32 	%r150, 2;
	mov.b64 	{%r144, %r149}, %rd84;
	// begin inline asm
	shfl.sync.down.b32 %r143, %r144, %r150, %r181, %r182;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r148, %r149, %r150, %r181, %r182;
	// end inline asm
	mov.b64 	%rd85, {%r143, %r148};
	mov.b64 	%fd149, %rd85;
	setp.gt.s32 	%p13, %r132, 29;
	add.f64 	%fd150, %fd148, %fd149;
	selp.f64 	%fd151, %fd148, %fd150, %p13;
	mov.b64 	%rd86, %fd151;
	mov.u32 	%r160, 4;
	mov.b64 	{%r154, %r159}, %rd86;
	// begin inline asm
	shfl.sync.down.b32 %r153, %r154, %r160, %r181, %r182;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r158, %r159, %r160, %r181, %r182;
	// end inline asm
	mov.b64 	%rd87, {%r153, %r158};
	mov.b64 	%fd152, %rd87;
	setp.gt.s32 	%p14, %r132, 27;
	add.f64 	%fd153, %fd151, %fd152;
	selp.f64 	%fd154, %fd151, %fd153, %p14;
	mov.b64 	%rd88, %fd154;
	mov.u32 	%r170, 8;
	mov.b64 	{%r164, %r169}, %rd88;
	// begin inline asm
	shfl.sync.down.b32 %r163, %r164, %r170, %r181, %r182;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r168, %r169, %r170, %r181, %r182;
	// end inline asm
	mov.b64 	%rd89, {%r163, %r168};
	mov.b64 	%fd155, %rd89;
	setp.gt.s32 	%p15, %r132, 23;
	add.f64 	%fd156, %fd154, %fd155;
	selp.f64 	%fd157, %fd154, %fd156, %p15;
	mov.b64 	%rd90, %fd157;
	mov.u32 	%r180, 16;
	mov.b64 	{%r174, %r179}, %rd90;
	// begin inline asm
	shfl.sync.down.b32 %r173, %r174, %r180, %r181, %r182;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r178, %r179, %r180, %r181, %r182;
	// end inline asm
	mov.b64 	%rd91, {%r173, %r178};
	mov.b64 	%fd158, %rd91;
	setp.gt.s32 	%p16, %r132, 15;
	add.f64 	%fd159, %fd157, %fd158;
	selp.f64 	%fd533, %fd157, %fd159, %p16;
	setp.ne.s32 	%p17, %r131, 0;
	@%p17 bra 	$L__BB4_16;

	shr.s32 	%r184, %r96, 31;
	shr.u32 	%r185, %r184, 27;
	add.s32 	%r186, %r96, %r185;
	shr.s32 	%r187, %r186, 5;
	shl.b32 	%r188, %r187, 3;
	mov.u32 	%r189, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r190, %r189, %r188;
	st.shared.f64 	[%r190+8], %fd533;

$L__BB4_16:
	bar.sync 	0;
	setp.ne.s32 	%p18, %r96, 0;
	@%p18 bra 	$L__BB4_76;

	ld.shared.f64 	%fd160, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd161, %fd533, %fd160;
	ld.shared.f64 	%fd162, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd163, %fd161, %fd162;
	ld.shared.f64 	%fd164, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd165, %fd163, %fd164;
	ld.shared.f64 	%fd166, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd167, %fd165, %fd166;
	ld.shared.f64 	%fd168, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd169, %fd167, %fd168;
	ld.shared.f64 	%fd170, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd171, %fd169, %fd170;
	ld.shared.f64 	%fd172, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd171, %fd172;
	bra.uni 	$L__BB4_76;

$L__BB4_38:
	mov.u32 	%r323, %tid.x;
	shl.b32 	%r324, %r323, 2;
	mul.wide.u32 	%rd135, %r324, 4;
	add.s64 	%rd122, %rd31, %rd135;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd120, %rd121}, [%rd122];
	// end inline asm
	mov.b64 	{%r325, %r326}, %rd121;
	mov.b64 	{%r327, %r328}, %rd120;
	mov.b32 	%f52, %r327;
	mov.b32 	%f53, %r328;
	mov.b32 	%f54, %r325;
	mov.b32 	%f55, %r326;
	add.s64 	%rd125, %rd122, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd123, %rd124}, [%rd125];
	// end inline asm
	mov.b64 	{%r329, %r330}, %rd124;
	mov.b64 	{%r331, %r332}, %rd123;
	mov.b32 	%f56, %r331;
	mov.b32 	%f57, %r332;
	mov.b32 	%f58, %r329;
	mov.b32 	%f59, %r330;
	add.s64 	%rd128, %rd122, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd126, %rd127}, [%rd128];
	// end inline asm
	mov.b64 	{%r333, %r334}, %rd127;
	mov.b64 	{%r335, %r336}, %rd126;
	mov.b32 	%f60, %r335;
	mov.b32 	%f61, %r336;
	mov.b32 	%f62, %r333;
	mov.b32 	%f63, %r334;
	add.s64 	%rd131, %rd122, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd129, %rd130}, [%rd131];
	// end inline asm
	mov.b64 	{%r337, %r338}, %rd130;
	mov.b64 	{%r339, %r340}, %rd129;
	mov.b32 	%f64, %r339;
	mov.b32 	%f65, %r340;
	mov.b32 	%f66, %r337;
	mov.b32 	%f67, %r338;
	add.s64 	%rd134, %rd122, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd132, %rd133}, [%rd134];
	// end inline asm
	mov.b64 	{%r341, %r342}, %rd133;
	mov.b64 	{%r343, %r344}, %rd132;
	mov.b32 	%f68, %r343;
	mov.b32 	%f69, %r344;
	mov.b32 	%f70, %r341;
	mov.b32 	%f71, %r342;
	cvt.f64.f32 	%fd242, %f52;
	cvt.f64.f32 	%fd243, %f53;
	cvt.f64.f32 	%fd244, %f54;
	cvt.f64.f32 	%fd245, %f55;
	cvt.f64.f32 	%fd246, %f56;
	cvt.f64.f32 	%fd247, %f57;
	cvt.f64.f32 	%fd248, %f58;
	cvt.f64.f32 	%fd249, %f59;
	cvt.f64.f32 	%fd250, %f60;
	cvt.f64.f32 	%fd251, %f61;
	cvt.f64.f32 	%fd252, %f62;
	cvt.f64.f32 	%fd253, %f63;
	cvt.f64.f32 	%fd254, %f64;
	cvt.f64.f32 	%fd255, %f65;
	cvt.f64.f32 	%fd256, %f66;
	cvt.f64.f32 	%fd257, %f67;
	cvt.f64.f32 	%fd258, %f68;
	cvt.f64.f32 	%fd259, %f69;
	cvt.f64.f32 	%fd260, %f70;
	cvt.f64.f32 	%fd261, %f71;
	add.f64 	%fd262, %fd242, %fd243;
	add.f64 	%fd263, %fd262, %fd244;
	add.f64 	%fd264, %fd263, %fd245;
	add.f64 	%fd265, %fd264, %fd246;
	add.f64 	%fd266, %fd265, %fd247;
	add.f64 	%fd267, %fd266, %fd248;
	add.f64 	%fd268, %fd267, %fd249;
	add.f64 	%fd269, %fd268, %fd250;
	add.f64 	%fd270, %fd269, %fd251;
	add.f64 	%fd271, %fd270, %fd252;
	add.f64 	%fd272, %fd271, %fd253;
	add.f64 	%fd273, %fd272, %fd254;
	add.f64 	%fd274, %fd273, %fd255;
	add.f64 	%fd275, %fd274, %fd256;
	add.f64 	%fd276, %fd275, %fd257;
	add.f64 	%fd277, %fd276, %fd258;
	add.f64 	%fd278, %fd277, %fd259;
	add.f64 	%fd279, %fd278, %fd260;
	add.f64 	%fd526, %fd279, %fd261;
	setp.lt.s32 	%p49, %r74, 10240;
	mov.u32 	%r636, 5120;
	@%p49 bra 	$L__BB4_45;

	add.s32 	%r347, %r74, -10240;
	mul.wide.u32 	%rd137, %r347, -858993459;
	shr.u64 	%rd138, %rd137, 44;
	cvt.u32.u64 	%r348, %rd138;
	add.s32 	%r33, %r348, 1;
	and.b32  	%r34, %r33, 1;
	setp.lt.u32 	%p50, %r347, 5120;
	mov.u64 	%rd239, 5120;
	mov.u32 	%r631, 10240;
	@%p50 bra 	$L__BB4_43;

	sub.s32 	%r633, %r33, %r34;
	mov.u32 	%r636, 5120;

$L__BB4_41:
	mul.wide.s32 	%rd169, %r636, 4;
	add.s64 	%rd141, %rd122, %rd169;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd139, %rd140}, [%rd141];
	// end inline asm
	mov.b64 	{%r351, %r352}, %rd140;
	mov.b64 	{%r353, %r354}, %rd139;
	mov.b32 	%f72, %r353;
	mov.b32 	%f73, %r354;
	mov.b32 	%f74, %r351;
	mov.b32 	%f75, %r352;
	add.s64 	%rd144, %rd141, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd142, %rd143}, [%rd144];
	// end inline asm
	mov.b64 	{%r355, %r356}, %rd143;
	mov.b64 	{%r357, %r358}, %rd142;
	mov.b32 	%f76, %r357;
	mov.b32 	%f77, %r358;
	mov.b32 	%f78, %r355;
	mov.b32 	%f79, %r356;
	add.s64 	%rd147, %rd141, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd145, %rd146}, [%rd147];
	// end inline asm
	mov.b64 	{%r359, %r360}, %rd146;
	mov.b64 	{%r361, %r362}, %rd145;
	mov.b32 	%f80, %r361;
	mov.b32 	%f81, %r362;
	mov.b32 	%f82, %r359;
	mov.b32 	%f83, %r360;
	add.s64 	%rd150, %rd141, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd148, %rd149}, [%rd150];
	// end inline asm
	mov.b64 	{%r363, %r364}, %rd149;
	mov.b64 	{%r365, %r366}, %rd148;
	mov.b32 	%f84, %r365;
	mov.b32 	%f85, %r366;
	mov.b32 	%f86, %r363;
	mov.b32 	%f87, %r364;
	add.s64 	%rd153, %rd141, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd151, %rd152}, [%rd153];
	// end inline asm
	mov.b64 	{%r367, %r368}, %rd152;
	mov.b64 	{%r369, %r370}, %rd151;
	mov.b32 	%f88, %r369;
	mov.b32 	%f89, %r370;
	mov.b32 	%f90, %r367;
	mov.b32 	%f91, %r368;
	cvt.f64.f32 	%fd281, %f72;
	cvt.f64.f32 	%fd282, %f73;
	cvt.f64.f32 	%fd283, %f74;
	cvt.f64.f32 	%fd284, %f75;
	cvt.f64.f32 	%fd285, %f76;
	cvt.f64.f32 	%fd286, %f77;
	cvt.f64.f32 	%fd287, %f78;
	cvt.f64.f32 	%fd288, %f79;
	cvt.f64.f32 	%fd289, %f80;
	cvt.f64.f32 	%fd290, %f81;
	cvt.f64.f32 	%fd291, %f82;
	cvt.f64.f32 	%fd292, %f83;
	cvt.f64.f32 	%fd293, %f84;
	cvt.f64.f32 	%fd294, %f85;
	cvt.f64.f32 	%fd295, %f86;
	cvt.f64.f32 	%fd296, %f87;
	cvt.f64.f32 	%fd297, %f88;
	cvt.f64.f32 	%fd298, %f89;
	cvt.f64.f32 	%fd299, %f90;
	cvt.f64.f32 	%fd300, %f91;
	add.f64 	%fd301, %fd526, %fd281;
	add.f64 	%fd302, %fd301, %fd282;
	add.f64 	%fd303, %fd302, %fd283;
	add.f64 	%fd304, %fd303, %fd284;
	add.f64 	%fd305, %fd304, %fd285;
	add.f64 	%fd306, %fd305, %fd286;
	add.f64 	%fd307, %fd306, %fd287;
	add.f64 	%fd308, %fd307, %fd288;
	add.f64 	%fd309, %fd308, %fd289;
	add.f64 	%fd310, %fd309, %fd290;
	add.f64 	%fd311, %fd310, %fd291;
	add.f64 	%fd312, %fd311, %fd292;
	add.f64 	%fd313, %fd312, %fd293;
	add.f64 	%fd314, %fd313, %fd294;
	add.f64 	%fd315, %fd314, %fd295;
	add.f64 	%fd316, %fd315, %fd296;
	add.f64 	%fd317, %fd316, %fd297;
	add.f64 	%fd318, %fd317, %fd298;
	add.f64 	%fd319, %fd318, %fd299;
	add.f64 	%fd320, %fd319, %fd300;
	add.s32 	%r636, %r631, 5120;
	mul.wide.s32 	%rd170, %r631, 4;
	add.s64 	%rd156, %rd122, %rd170;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd154, %rd155}, [%rd156];
	// end inline asm
	mov.b64 	{%r371, %r372}, %rd155;
	mov.b64 	{%r373, %r374}, %rd154;
	mov.b32 	%f92, %r373;
	mov.b32 	%f93, %r374;
	mov.b32 	%f94, %r371;
	mov.b32 	%f95, %r372;
	add.s64 	%rd159, %rd156, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd157, %rd158}, [%rd159];
	// end inline asm
	mov.b64 	{%r375, %r376}, %rd158;
	mov.b64 	{%r377, %r378}, %rd157;
	mov.b32 	%f96, %r377;
	mov.b32 	%f97, %r378;
	mov.b32 	%f98, %r375;
	mov.b32 	%f99, %r376;
	add.s64 	%rd162, %rd156, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd160, %rd161}, [%rd162];
	// end inline asm
	mov.b64 	{%r379, %r380}, %rd161;
	mov.b64 	{%r381, %r382}, %rd160;
	mov.b32 	%f100, %r381;
	mov.b32 	%f101, %r382;
	mov.b32 	%f102, %r379;
	mov.b32 	%f103, %r380;
	add.s64 	%rd165, %rd156, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd163, %rd164}, [%rd165];
	// end inline asm
	mov.b64 	{%r383, %r384}, %rd164;
	mov.b64 	{%r385, %r386}, %rd163;
	mov.b32 	%f104, %r385;
	mov.b32 	%f105, %r386;
	mov.b32 	%f106, %r383;
	mov.b32 	%f107, %r384;
	add.s64 	%rd168, %rd156, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd166, %rd167}, [%rd168];
	// end inline asm
	mov.b64 	{%r387, %r388}, %rd167;
	mov.b64 	{%r389, %r390}, %rd166;
	mov.b32 	%f108, %r389;
	mov.b32 	%f109, %r390;
	mov.b32 	%f110, %r387;
	mov.b32 	%f111, %r388;
	cvt.f64.f32 	%fd321, %f92;
	cvt.f64.f32 	%fd322, %f93;
	cvt.f64.f32 	%fd323, %f94;
	cvt.f64.f32 	%fd324, %f95;
	cvt.f64.f32 	%fd325, %f96;
	cvt.f64.f32 	%fd326, %f97;
	cvt.f64.f32 	%fd327, %f98;
	cvt.f64.f32 	%fd328, %f99;
	cvt.f64.f32 	%fd329, %f100;
	cvt.f64.f32 	%fd330, %f101;
	cvt.f64.f32 	%fd331, %f102;
	cvt.f64.f32 	%fd332, %f103;
	cvt.f64.f32 	%fd333, %f104;
	cvt.f64.f32 	%fd334, %f105;
	cvt.f64.f32 	%fd335, %f106;
	cvt.f64.f32 	%fd336, %f107;
	cvt.f64.f32 	%fd337, %f108;
	cvt.f64.f32 	%fd338, %f109;
	cvt.f64.f32 	%fd339, %f110;
	cvt.f64.f32 	%fd340, %f111;
	add.f64 	%fd341, %fd320, %fd321;
	add.f64 	%fd342, %fd341, %fd322;
	add.f64 	%fd343, %fd342, %fd323;
	add.f64 	%fd344, %fd343, %fd324;
	add.f64 	%fd345, %fd344, %fd325;
	add.f64 	%fd346, %fd345, %fd326;
	add.f64 	%fd347, %fd346, %fd327;
	add.f64 	%fd348, %fd347, %fd328;
	add.f64 	%fd349, %fd348, %fd329;
	add.f64 	%fd350, %fd349, %fd330;
	add.f64 	%fd351, %fd350, %fd331;
	add.f64 	%fd352, %fd351, %fd332;
	add.f64 	%fd353, %fd352, %fd333;
	add.f64 	%fd354, %fd353, %fd334;
	add.f64 	%fd355, %fd354, %fd335;
	add.f64 	%fd356, %fd355, %fd336;
	add.f64 	%fd357, %fd356, %fd337;
	add.f64 	%fd358, %fd357, %fd338;
	add.f64 	%fd359, %fd358, %fd339;
	add.f64 	%fd526, %fd359, %fd340;
	add.s32 	%r631, %r631, 10240;
	add.s32 	%r633, %r633, -2;
	setp.ne.s32 	%p51, %r633, 0;
	@%p51 bra 	$L__BB4_41;

	cvt.s64.s32 	%rd239, %r636;

$L__BB4_43:
	setp.eq.s32 	%p52, %r34, 0;
	@%p52 bra 	$L__BB4_45;

	shl.b64 	%rd186, %rd239, 2;
	add.s64 	%rd173, %rd122, %rd186;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd171, %rd172}, [%rd173];
	// end inline asm
	mov.b64 	{%r391, %r392}, %rd172;
	mov.b64 	{%r393, %r394}, %rd171;
	mov.b32 	%f112, %r393;
	mov.b32 	%f113, %r394;
	mov.b32 	%f114, %r391;
	mov.b32 	%f115, %r392;
	add.s64 	%rd176, %rd173, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd174, %rd175}, [%rd176];
	// end inline asm
	mov.b64 	{%r395, %r396}, %rd175;
	mov.b64 	{%r397, %r398}, %rd174;
	mov.b32 	%f116, %r397;
	mov.b32 	%f117, %r398;
	mov.b32 	%f118, %r395;
	mov.b32 	%f119, %r396;
	add.s64 	%rd179, %rd173, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd177, %rd178}, [%rd179];
	// end inline asm
	mov.b64 	{%r399, %r400}, %rd178;
	mov.b64 	{%r401, %r402}, %rd177;
	mov.b32 	%f120, %r401;
	mov.b32 	%f121, %r402;
	mov.b32 	%f122, %r399;
	mov.b32 	%f123, %r400;
	add.s64 	%rd182, %rd173, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd180, %rd181}, [%rd182];
	// end inline asm
	mov.b64 	{%r403, %r404}, %rd181;
	mov.b64 	{%r405, %r406}, %rd180;
	mov.b32 	%f124, %r405;
	mov.b32 	%f125, %r406;
	mov.b32 	%f126, %r403;
	mov.b32 	%f127, %r404;
	add.s64 	%rd185, %rd173, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd183, %rd184}, [%rd185];
	// end inline asm
	mov.b64 	{%r407, %r408}, %rd184;
	mov.b64 	{%r409, %r410}, %rd183;
	mov.b32 	%f128, %r409;
	mov.b32 	%f129, %r410;
	mov.b32 	%f130, %r407;
	mov.b32 	%f131, %r408;
	cvt.f64.f32 	%fd360, %f112;
	cvt.f64.f32 	%fd361, %f113;
	cvt.f64.f32 	%fd362, %f114;
	cvt.f64.f32 	%fd363, %f115;
	cvt.f64.f32 	%fd364, %f116;
	cvt.f64.f32 	%fd365, %f117;
	cvt.f64.f32 	%fd366, %f118;
	cvt.f64.f32 	%fd367, %f119;
	cvt.f64.f32 	%fd368, %f120;
	cvt.f64.f32 	%fd369, %f121;
	cvt.f64.f32 	%fd370, %f122;
	cvt.f64.f32 	%fd371, %f123;
	cvt.f64.f32 	%fd372, %f124;
	cvt.f64.f32 	%fd373, %f125;
	cvt.f64.f32 	%fd374, %f126;
	cvt.f64.f32 	%fd375, %f127;
	cvt.f64.f32 	%fd376, %f128;
	cvt.f64.f32 	%fd377, %f129;
	cvt.f64.f32 	%fd378, %f130;
	cvt.f64.f32 	%fd379, %f131;
	add.f64 	%fd380, %fd526, %fd360;
	add.f64 	%fd381, %fd380, %fd361;
	add.f64 	%fd382, %fd381, %fd362;
	add.f64 	%fd383, %fd382, %fd363;
	add.f64 	%fd384, %fd383, %fd364;
	add.f64 	%fd385, %fd384, %fd365;
	add.f64 	%fd386, %fd385, %fd366;
	add.f64 	%fd387, %fd386, %fd367;
	add.f64 	%fd388, %fd387, %fd368;
	add.f64 	%fd389, %fd388, %fd369;
	add.f64 	%fd390, %fd389, %fd370;
	add.f64 	%fd391, %fd390, %fd371;
	add.f64 	%fd392, %fd391, %fd372;
	add.f64 	%fd393, %fd392, %fd373;
	add.f64 	%fd394, %fd393, %fd374;
	add.f64 	%fd395, %fd394, %fd375;
	add.f64 	%fd396, %fd395, %fd376;
	add.f64 	%fd397, %fd396, %fd377;
	add.f64 	%fd398, %fd397, %fd378;
	add.f64 	%fd526, %fd398, %fd379;
	mov.u32 	%r636, %r631;

$L__BB4_45:
	setp.ge.s32 	%p53, %r636, %r74;
	@%p53 bra 	$L__BB4_53;

	sub.s32 	%r45, %r74, %r636;
	setp.ge.s32 	%p54, %r323, %r45;
	@%p54 bra 	$L__BB4_53;

	not.b32 	%r412, %r636;
	add.s32 	%r413, %r412, %r74;
	sub.s32 	%r47, %r413, %r323;
	shr.u32 	%r414, %r47, 8;
	add.s32 	%r415, %r414, 1;
	and.b32  	%r638, %r415, 3;
	setp.eq.s32 	%p55, %r638, 0;
	mov.u32 	%r639, %r323;
	@%p55 bra 	$L__BB4_50;

	mov.u32 	%r639, %tid.x;
	add.s32 	%r416, %r639, %r636;
	mul.wide.s32 	%rd187, %r416, 4;
	add.s64 	%rd240, %rd31, %rd187;

$L__BB4_49:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r417, [%rd240];
	// end inline asm
	mov.b32 	%f132, %r417;
	cvt.f64.f32 	%fd400, %f132;
	add.f64 	%fd526, %fd526, %fd400;
	add.s32 	%r639, %r639, 256;
	add.s64 	%rd240, %rd240, 1024;
	add.s32 	%r638, %r638, -1;
	setp.ne.s32 	%p56, %r638, 0;
	@%p56 bra 	$L__BB4_49;

$L__BB4_50:
	setp.lt.u32 	%p57, %r47, 768;
	@%p57 bra 	$L__BB4_53;

	add.s32 	%r418, %r636, %r639;
	mul.wide.s32 	%rd189, %r418, 4;
	add.s64 	%rd241, %rd31, %rd189;

$L__BB4_52:
	// begin inline asm
	ld.global.nc.u32 %r419, [%rd241];
	// end inline asm
	mov.b32 	%f133, %r419;
	cvt.f64.f32 	%fd401, %f133;
	add.f64 	%fd402, %fd526, %fd401;
	add.s64 	%rd191, %rd241, 1024;
	// begin inline asm
	ld.global.nc.u32 %r420, [%rd191];
	// end inline asm
	mov.b32 	%f134, %r420;
	cvt.f64.f32 	%fd403, %f134;
	add.f64 	%fd404, %fd402, %fd403;
	add.s64 	%rd192, %rd241, 2048;
	// begin inline asm
	ld.global.nc.u32 %r421, [%rd192];
	// end inline asm
	mov.b32 	%f135, %r421;
	cvt.f64.f32 	%fd405, %f135;
	add.f64 	%fd406, %fd404, %fd405;
	add.s64 	%rd193, %rd241, 3072;
	// begin inline asm
	ld.global.nc.u32 %r422, [%rd193];
	// end inline asm
	mov.b32 	%f136, %r422;
	cvt.f64.f32 	%fd407, %f136;
	add.f64 	%fd526, %fd406, %fd407;
	add.s64 	%rd241, %rd241, 4096;
	add.s32 	%r639, %r639, 1024;
	setp.lt.s32 	%p58, %r639, %r45;
	@%p58 bra 	$L__BB4_52;

$L__BB4_53:
	// begin inline asm
	mov.u32 %r423, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r424, %laneid;
	// end inline asm
	mov.b64 	%rd194, %fd526;
	mov.u32 	%r432, 1;
	mov.u32 	%r473, 31;
	mov.u32 	%r474, -1;
	mov.b64 	{%r426, %r431}, %rd194;
	// begin inline asm
	shfl.sync.down.b32 %r425, %r426, %r432, %r473, %r474;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r430, %r431, %r432, %r473, %r474;
	// end inline asm
	mov.b64 	%rd195, {%r425, %r430};
	mov.b64 	%fd408, %rd195;
	setp.gt.s32 	%p59, %r424, 30;
	add.f64 	%fd409, %fd526, %fd408;
	selp.f64 	%fd410, %fd526, %fd409, %p59;
	mov.b64 	%rd196, %fd410;
	mov.u32 	%r442, 2;
	mov.b64 	{%r436, %r441}, %rd196;
	// begin inline asm
	shfl.sync.down.b32 %r435, %r436, %r442, %r473, %r474;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r440, %r441, %r442, %r473, %r474;
	// end inline asm
	mov.b64 	%rd197, {%r435, %r440};
	mov.b64 	%fd411, %rd197;
	setp.gt.s32 	%p60, %r424, 29;
	add.f64 	%fd412, %fd410, %fd411;
	selp.f64 	%fd413, %fd410, %fd412, %p60;
	mov.b64 	%rd198, %fd413;
	mov.u32 	%r452, 4;
	mov.b64 	{%r446, %r451}, %rd198;
	// begin inline asm
	shfl.sync.down.b32 %r445, %r446, %r452, %r473, %r474;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r450, %r451, %r452, %r473, %r474;
	// end inline asm
	mov.b64 	%rd199, {%r445, %r450};
	mov.b64 	%fd414, %rd199;
	setp.gt.s32 	%p61, %r424, 27;
	add.f64 	%fd415, %fd413, %fd414;
	selp.f64 	%fd416, %fd413, %fd415, %p61;
	mov.b64 	%rd200, %fd416;
	mov.u32 	%r462, 8;
	mov.b64 	{%r456, %r461}, %rd200;
	// begin inline asm
	shfl.sync.down.b32 %r455, %r456, %r462, %r473, %r474;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r460, %r461, %r462, %r473, %r474;
	// end inline asm
	mov.b64 	%rd201, {%r455, %r460};
	mov.b64 	%fd417, %rd201;
	setp.gt.s32 	%p62, %r424, 23;
	add.f64 	%fd418, %fd416, %fd417;
	selp.f64 	%fd419, %fd416, %fd418, %p62;
	mov.b64 	%rd202, %fd419;
	mov.u32 	%r472, 16;
	mov.b64 	{%r466, %r471}, %rd202;
	// begin inline asm
	shfl.sync.down.b32 %r465, %r466, %r472, %r473, %r474;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r470, %r471, %r472, %r473, %r474;
	// end inline asm
	mov.b64 	%rd203, {%r465, %r470};
	mov.b64 	%fd420, %rd203;
	setp.gt.s32 	%p63, %r424, 15;
	add.f64 	%fd421, %fd419, %fd420;
	selp.f64 	%fd533, %fd419, %fd421, %p63;
	setp.ne.s32 	%p64, %r423, 0;
	@%p64 bra 	$L__BB4_55;

	shr.s32 	%r476, %r323, 31;
	shr.u32 	%r477, %r476, 27;
	add.s32 	%r478, %r323, %r477;
	shr.s32 	%r479, %r478, 5;
	shl.b32 	%r480, %r479, 3;
	mov.u32 	%r481, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r482, %r481, %r480;
	st.shared.f64 	[%r482+8], %fd533;

$L__BB4_55:
	bar.sync 	0;
	setp.ne.s32 	%p65, %r323, 0;
	@%p65 bra 	$L__BB4_76;

	ld.shared.f64 	%fd422, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd423, %fd533, %fd422;
	ld.shared.f64 	%fd424, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd425, %fd423, %fd424;
	ld.shared.f64 	%fd426, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd427, %fd425, %fd426;
	ld.shared.f64 	%fd428, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd429, %fd427, %fd428;
	ld.shared.f64 	%fd430, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd431, %fd429, %fd430;
	ld.shared.f64 	%fd432, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd433, %fd431, %fd432;
	ld.shared.f64 	%fd434, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd433, %fd434;
	bra.uni 	$L__BB4_76;

$L__BB4_28:
	setp.ne.s32 	%p26, %r202, 0;
	shl.b32 	%r261, %r28, 5;
	add.s32 	%r262, %r261, 32;
	setp.gt.s32 	%p27, %r262, %r74;
	// begin inline asm
	mov.u32 %r210, %laneid;
	// end inline asm
	not.b32 	%r263, %r261;
	mov.u32 	%r260, -1;
	add.s32 	%r264, %r263, %r74;
	selp.b32 	%r259, %r264, 31, %p27;
	mov.u32 	%r218, 1;
	// begin inline asm
	shfl.sync.down.b32 %r211, %r30, %r218, %r259, %r260;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r216, %r31, %r218, %r259, %r260;
	// end inline asm
	mov.b64 	%rd102, {%r211, %r216};
	mov.b64 	%fd183, %rd102;
	setp.lt.s32 	%p28, %r210, %r259;
	add.f64 	%fd184, %fd517, %fd183;
	selp.f64 	%fd185, %fd184, %fd517, %p28;
	mov.b64 	%rd103, %fd185;
	mov.u32 	%r228, 2;
	mov.b64 	{%r222, %r227}, %rd103;
	// begin inline asm
	shfl.sync.down.b32 %r221, %r222, %r228, %r259, %r260;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r226, %r227, %r228, %r259, %r260;
	// end inline asm
	mov.b64 	%rd104, {%r221, %r226};
	mov.b64 	%fd186, %rd104;
	add.s32 	%r265, %r210, 2;
	setp.gt.s32 	%p29, %r265, %r259;
	add.f64 	%fd187, %fd185, %fd186;
	selp.f64 	%fd188, %fd185, %fd187, %p29;
	mov.b64 	%rd105, %fd188;
	mov.u32 	%r238, 4;
	mov.b64 	{%r232, %r237}, %rd105;
	// begin inline asm
	shfl.sync.down.b32 %r231, %r232, %r238, %r259, %r260;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r236, %r237, %r238, %r259, %r260;
	// end inline asm
	mov.b64 	%rd106, {%r231, %r236};
	mov.b64 	%fd189, %rd106;
	add.s32 	%r266, %r210, 4;
	setp.gt.s32 	%p30, %r266, %r259;
	add.f64 	%fd190, %fd188, %fd189;
	selp.f64 	%fd191, %fd188, %fd190, %p30;
	mov.b64 	%rd107, %fd191;
	mov.u32 	%r248, 8;
	mov.b64 	{%r242, %r247}, %rd107;
	// begin inline asm
	shfl.sync.down.b32 %r241, %r242, %r248, %r259, %r260;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r246, %r247, %r248, %r259, %r260;
	// end inline asm
	mov.b64 	%rd108, {%r241, %r246};
	mov.b64 	%fd192, %rd108;
	add.s32 	%r267, %r210, 8;
	setp.gt.s32 	%p31, %r267, %r259;
	add.f64 	%fd193, %fd191, %fd192;
	selp.f64 	%fd194, %fd191, %fd193, %p31;
	mov.b64 	%rd109, %fd194;
	mov.u32 	%r258, 16;
	mov.b64 	{%r252, %r257}, %rd109;
	// begin inline asm
	shfl.sync.down.b32 %r251, %r252, %r258, %r259, %r260;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r256, %r257, %r258, %r259, %r260;
	// end inline asm
	mov.b64 	%rd110, {%r251, %r256};
	mov.b64 	%fd195, %rd110;
	add.s32 	%r268, %r210, 16;
	setp.gt.s32 	%p32, %r268, %r259;
	add.f64 	%fd196, %fd194, %fd195;
	selp.f64 	%fd533, %fd194, %fd196, %p32;
	@%p26 bra 	$L__BB4_30;

	add.s32 	%r616, %r209, 8;
	st.shared.f64 	[%r616], %fd533;

$L__BB4_30:
	bar.sync 	0;
	setp.ne.s32 	%p33, %r203, 0;
	@%p33 bra 	$L__BB4_76;

	setp.gt.s32 	%p34, %r74, 32;
	ld.shared.f64 	%fd197, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd198, %fd533, %fd197;
	selp.f64 	%fd199, %fd198, %fd533, %p34;
	ld.shared.f64 	%fd200, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd201, %fd199, %fd200;
	setp.gt.s32 	%p35, %r74, 64;
	selp.f64 	%fd202, %fd201, %fd199, %p35;
	ld.shared.f64 	%fd203, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd204, %fd202, %fd203;
	setp.gt.s32 	%p36, %r74, 96;
	selp.f64 	%fd205, %fd204, %fd202, %p36;
	ld.shared.f64 	%fd206, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd207, %fd205, %fd206;
	setp.gt.s32 	%p37, %r74, 128;
	selp.f64 	%fd208, %fd207, %fd205, %p37;
	ld.shared.f64 	%fd209, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd210, %fd208, %fd209;
	setp.gt.s32 	%p38, %r74, 160;
	selp.f64 	%fd211, %fd210, %fd208, %p38;
	ld.shared.f64 	%fd212, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd213, %fd211, %fd212;
	setp.gt.s32 	%p39, %r74, 192;
	selp.f64 	%fd533, %fd213, %fd211, %p39;
	setp.lt.s32 	%p40, %r74, 225;
	@%p40 bra 	$L__BB4_76;

	ld.shared.f64 	%fd214, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd533, %fd214;
	bra.uni 	$L__BB4_76;

$L__BB4_67:
	setp.ne.s32 	%p73, %r494, 0;
	shl.b32 	%r553, %r69, 5;
	add.s32 	%r554, %r553, 32;
	setp.gt.s32 	%p74, %r554, %r74;
	// begin inline asm
	mov.u32 %r502, %laneid;
	// end inline asm
	not.b32 	%r555, %r553;
	mov.u32 	%r552, -1;
	add.s32 	%r556, %r555, %r74;
	selp.b32 	%r551, %r556, 31, %p74;
	mov.u32 	%r510, 1;
	// begin inline asm
	shfl.sync.down.b32 %r503, %r71, %r510, %r551, %r552;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r508, %r72, %r510, %r551, %r552;
	// end inline asm
	mov.b64 	%rd214, {%r503, %r508};
	mov.b64 	%fd445, %rd214;
	setp.lt.s32 	%p75, %r502, %r551;
	add.f64 	%fd446, %fd532, %fd445;
	selp.f64 	%fd447, %fd446, %fd532, %p75;
	mov.b64 	%rd215, %fd447;
	mov.u32 	%r520, 2;
	mov.b64 	{%r514, %r519}, %rd215;
	// begin inline asm
	shfl.sync.down.b32 %r513, %r514, %r520, %r551, %r552;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r518, %r519, %r520, %r551, %r552;
	// end inline asm
	mov.b64 	%rd216, {%r513, %r518};
	mov.b64 	%fd448, %rd216;
	add.s32 	%r557, %r502, 2;
	setp.gt.s32 	%p76, %r557, %r551;
	add.f64 	%fd449, %fd447, %fd448;
	selp.f64 	%fd450, %fd447, %fd449, %p76;
	mov.b64 	%rd217, %fd450;
	mov.u32 	%r530, 4;
	mov.b64 	{%r524, %r529}, %rd217;
	// begin inline asm
	shfl.sync.down.b32 %r523, %r524, %r530, %r551, %r552;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r528, %r529, %r530, %r551, %r552;
	// end inline asm
	mov.b64 	%rd218, {%r523, %r528};
	mov.b64 	%fd451, %rd218;
	add.s32 	%r558, %r502, 4;
	setp.gt.s32 	%p77, %r558, %r551;
	add.f64 	%fd452, %fd450, %fd451;
	selp.f64 	%fd453, %fd450, %fd452, %p77;
	mov.b64 	%rd219, %fd453;
	mov.u32 	%r540, 8;
	mov.b64 	{%r534, %r539}, %rd219;
	// begin inline asm
	shfl.sync.down.b32 %r533, %r534, %r540, %r551, %r552;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r538, %r539, %r540, %r551, %r552;
	// end inline asm
	mov.b64 	%rd220, {%r533, %r538};
	mov.b64 	%fd454, %rd220;
	add.s32 	%r559, %r502, 8;
	setp.gt.s32 	%p78, %r559, %r551;
	add.f64 	%fd455, %fd453, %fd454;
	selp.f64 	%fd456, %fd453, %fd455, %p78;
	mov.b64 	%rd221, %fd456;
	mov.u32 	%r550, 16;
	mov.b64 	{%r544, %r549}, %rd221;
	// begin inline asm
	shfl.sync.down.b32 %r543, %r544, %r550, %r551, %r552;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r548, %r549, %r550, %r551, %r552;
	// end inline asm
	mov.b64 	%rd222, {%r543, %r548};
	mov.b64 	%fd457, %rd222;
	add.s32 	%r560, %r502, 16;
	setp.gt.s32 	%p79, %r560, %r551;
	add.f64 	%fd458, %fd456, %fd457;
	selp.f64 	%fd533, %fd456, %fd458, %p79;
	@%p73 bra 	$L__BB4_69;

	add.s32 	%r618, %r501, 8;
	st.shared.f64 	[%r618], %fd533;

$L__BB4_69:
	bar.sync 	0;
	setp.ne.s32 	%p80, %r495, 0;
	@%p80 bra 	$L__BB4_76;

	setp.gt.s32 	%p81, %r74, 32;
	ld.shared.f64 	%fd459, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd460, %fd533, %fd459;
	selp.f64 	%fd461, %fd460, %fd533, %p81;
	ld.shared.f64 	%fd462, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd463, %fd461, %fd462;
	setp.gt.s32 	%p82, %r74, 64;
	selp.f64 	%fd464, %fd463, %fd461, %p82;
	ld.shared.f64 	%fd465, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd466, %fd464, %fd465;
	setp.gt.s32 	%p83, %r74, 96;
	selp.f64 	%fd467, %fd466, %fd464, %p83;
	ld.shared.f64 	%fd468, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd469, %fd467, %fd468;
	setp.gt.s32 	%p84, %r74, 128;
	selp.f64 	%fd470, %fd469, %fd467, %p84;
	ld.shared.f64 	%fd471, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd472, %fd470, %fd471;
	setp.gt.s32 	%p85, %r74, 160;
	selp.f64 	%fd473, %fd472, %fd470, %p85;
	ld.shared.f64 	%fd474, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd475, %fd473, %fd474;
	setp.gt.s32 	%p86, %r74, 192;
	selp.f64 	%fd533, %fd475, %fd473, %p86;
	setp.lt.s32 	%p87, %r74, 225;
	@%p87 bra 	$L__BB4_76;

	ld.shared.f64 	%fd476, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd533, %fd476;

$L__BB4_76:
	mov.u32 	%r614, %tid.x;
	setp.ne.s32 	%p95, %r614, 0;
	@%p95 bra 	$L__BB4_80;

	add.f64 	%fd504, %fd533, %fd59;
	cvta.to.global.u64 	%rd232, %rd32;
	st.global.f64 	[%rd232], %fd504;

$L__BB4_80:
	ret;

}
	// .globl	_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_
.visible .entry _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_(
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_0,
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_1,
	.param .u32 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_2,
	.param .align 4 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_3[40],
	.param .align 1 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_4[1]
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<93>;
	.reg .f32 	%f<103>;
	.reg .b32 	%r<635>;
	.reg .f64 	%fd<447>;
	.reg .b64 	%rd<328>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage[80];

	ld.param.u64 	%rd43, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_0];
	ld.param.u64 	%rd44, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_1];
	ld.param.u32 	%r1, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_3+20];
	ld.param.u32 	%r89, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_3+24];
	mul.lo.s32 	%r2, %r89, 5120;
	mov.u32 	%r90, %ctaid.x;
	mul.lo.s32 	%r3, %r90, 5120;
	and.b64  	%rd45, %rd43, 15;
	setp.eq.s64 	%p1, %rd45, 0;
	@%p1 bra 	$L__BB5_36;

	mad.lo.s32 	%r92, %r90, 5120, 5120;
	setp.gt.s32 	%p2, %r92, %r1;
	@%p2 bra 	$L__BB5_17;
	bra.uni 	$L__BB5_2;

$L__BB5_17:
	sub.s32 	%r37, %r1, %r3;
	mov.u32 	%r622, %tid.x;
	setp.ge.s32 	%p18, %r622, %r37;
	@%p18 bra 	$L__BB5_19;

	mov.u32 	%r219, %tid.x;
	add.s32 	%r220, %r3, %r219;
	mul.wide.s32 	%rd209, %r220, 4;
	add.s64 	%rd208, %rd43, %rd209;
	// begin inline asm
	ld.global.nc.u32 %r218, [%rd208];
	// end inline asm
	mov.b32 	%f46, %r218;
	cvt.f64.f32 	%fd432, %f46;
	add.s32 	%r622, %r219, 256;

$L__BB5_19:
	setp.ge.s32 	%p19, %r622, %r37;
	@%p19 bra 	$L__BB5_26;

	not.b32 	%r221, %r622;
	add.s32 	%r222, %r1, %r221;
	mad.lo.s32 	%r41, %r90, -5120, %r222;
	shr.u32 	%r224, %r41, 8;
	add.s32 	%r225, %r224, 1;
	and.b32  	%r621, %r225, 3;
	setp.eq.s32 	%p20, %r621, 0;
	@%p20 bra 	$L__BB5_23;

	add.s32 	%r226, %r622, %r3;
	mul.wide.s32 	%rd210, %r226, 4;
	add.s64 	%rd322, %rd43, %rd210;

$L__BB5_22:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r227, [%rd322];
	// end inline asm
	mov.b32 	%f47, %r227;
	cvt.f64.f32 	%fd171, %f47;
	add.f64 	%fd432, %fd432, %fd171;
	add.s32 	%r622, %r622, 256;
	add.s64 	%rd322, %rd322, 1024;
	add.s32 	%r621, %r621, -1;
	setp.ne.s32 	%p21, %r621, 0;
	@%p21 bra 	$L__BB5_22;

$L__BB5_23:
	setp.lt.u32 	%p22, %r41, 768;
	@%p22 bra 	$L__BB5_26;

	add.s32 	%r228, %r622, %r3;
	mul.wide.s32 	%rd212, %r228, 4;
	add.s64 	%rd323, %rd43, %rd212;

$L__BB5_25:
	// begin inline asm
	ld.global.nc.u32 %r229, [%rd323];
	// end inline asm
	mov.b32 	%f48, %r229;
	cvt.f64.f32 	%fd172, %f48;
	add.f64 	%fd173, %fd432, %fd172;
	add.s64 	%rd214, %rd323, 1024;
	// begin inline asm
	ld.global.nc.u32 %r230, [%rd214];
	// end inline asm
	mov.b32 	%f49, %r230;
	cvt.f64.f32 	%fd174, %f49;
	add.f64 	%fd175, %fd173, %fd174;
	add.s64 	%rd215, %rd323, 2048;
	// begin inline asm
	ld.global.nc.u32 %r231, [%rd215];
	// end inline asm
	mov.b32 	%f50, %r231;
	cvt.f64.f32 	%fd176, %f50;
	add.f64 	%fd177, %fd175, %fd176;
	add.s64 	%rd216, %rd323, 3072;
	// begin inline asm
	ld.global.nc.u32 %r232, [%rd216];
	// end inline asm
	mov.b32 	%f51, %r232;
	cvt.f64.f32 	%fd178, %f51;
	add.f64 	%fd432, %fd177, %fd178;
	add.s64 	%rd323, %rd323, 4096;
	add.s32 	%r622, %r622, 1024;
	setp.lt.s32 	%p23, %r622, %r37;
	@%p23 bra 	$L__BB5_25;

$L__BB5_26:
	mov.u32 	%r234, %tid.x;
	shr.s32 	%r235, %r234, 31;
	shr.u32 	%r236, %r235, 27;
	add.s32 	%r237, %r234, %r236;
	shr.s32 	%r50, %r237, 5;
	// begin inline asm
	mov.u32 %r233, %laneid;
	// end inline asm
	mov.b64 	%rd217, %fd432;
	mov.b64 	{%r52, %r53}, %rd217;
	shl.b32 	%r238, %r50, 3;
	mov.u32 	%r239, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r240, %r239, %r238;
	setp.gt.s32 	%p24, %r37, 255;
	@%p24 bra 	$L__BB5_32;
	bra.uni 	$L__BB5_27;

$L__BB5_32:
	setp.ne.s32 	%p40, %r233, 0;
	// begin inline asm
	mov.u32 %r301, %laneid;
	// end inline asm
	mov.u32 	%r309, 1;
	mov.u32 	%r350, 31;
	mov.u32 	%r351, -1;
	// begin inline asm
	shfl.sync.down.b32 %r302, %r52, %r309, %r350, %r351;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r307, %r53, %r309, %r350, %r351;
	// end inline asm
	mov.b64 	%rd227, {%r302, %r307};
	mov.b64 	%fd211, %rd227;
	setp.gt.s32 	%p41, %r301, 30;
	add.f64 	%fd212, %fd432, %fd211;
	selp.f64 	%fd213, %fd432, %fd212, %p41;
	mov.b64 	%rd228, %fd213;
	mov.u32 	%r319, 2;
	mov.b64 	{%r313, %r318}, %rd228;
	// begin inline asm
	shfl.sync.down.b32 %r312, %r313, %r319, %r350, %r351;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r317, %r318, %r319, %r350, %r351;
	// end inline asm
	mov.b64 	%rd229, {%r312, %r317};
	mov.b64 	%fd214, %rd229;
	setp.gt.s32 	%p42, %r301, 29;
	add.f64 	%fd215, %fd213, %fd214;
	selp.f64 	%fd216, %fd213, %fd215, %p42;
	mov.b64 	%rd230, %fd216;
	mov.u32 	%r329, 4;
	mov.b64 	{%r323, %r328}, %rd230;
	// begin inline asm
	shfl.sync.down.b32 %r322, %r323, %r329, %r350, %r351;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r327, %r328, %r329, %r350, %r351;
	// end inline asm
	mov.b64 	%rd231, {%r322, %r327};
	mov.b64 	%fd217, %rd231;
	setp.gt.s32 	%p43, %r301, 27;
	add.f64 	%fd218, %fd216, %fd217;
	selp.f64 	%fd219, %fd216, %fd218, %p43;
	mov.b64 	%rd232, %fd219;
	mov.u32 	%r339, 8;
	mov.b64 	{%r333, %r338}, %rd232;
	// begin inline asm
	shfl.sync.down.b32 %r332, %r333, %r339, %r350, %r351;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r337, %r338, %r339, %r350, %r351;
	// end inline asm
	mov.b64 	%rd233, {%r332, %r337};
	mov.b64 	%fd220, %rd233;
	setp.gt.s32 	%p44, %r301, 23;
	add.f64 	%fd221, %fd219, %fd220;
	selp.f64 	%fd222, %fd219, %fd221, %p44;
	mov.b64 	%rd234, %fd222;
	mov.u32 	%r349, 16;
	mov.b64 	{%r343, %r348}, %rd234;
	// begin inline asm
	shfl.sync.down.b32 %r342, %r343, %r349, %r350, %r351;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r347, %r348, %r349, %r350, %r351;
	// end inline asm
	mov.b64 	%rd235, {%r342, %r347};
	mov.b64 	%fd223, %rd235;
	setp.gt.s32 	%p45, %r301, 15;
	add.f64 	%fd224, %fd222, %fd223;
	selp.f64 	%fd446, %fd222, %fd224, %p45;
	@%p40 bra 	$L__BB5_34;

	add.s32 	%r610, %r240, 8;
	st.shared.f64 	[%r610], %fd446;

$L__BB5_34:
	bar.sync 	0;
	setp.ne.s32 	%p46, %r234, 0;
	@%p46 bra 	$L__BB5_71;

	ld.shared.f64 	%fd225, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd226, %fd446, %fd225;
	ld.shared.f64 	%fd227, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd228, %fd226, %fd227;
	ld.shared.f64 	%fd229, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd230, %fd228, %fd229;
	ld.shared.f64 	%fd231, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd232, %fd230, %fd231;
	ld.shared.f64 	%fd233, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd234, %fd232, %fd233;
	ld.shared.f64 	%fd235, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd236, %fd234, %fd235;
	ld.shared.f64 	%fd237, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd236, %fd237;
	bra.uni 	$L__BB5_71;

$L__BB5_36:
	mad.lo.s32 	%r354, %r90, 5120, 5120;
	setp.gt.s32 	%p47, %r354, %r1;
	@%p47 bra 	$L__BB5_52;
	bra.uni 	$L__BB5_37;

$L__BB5_52:
	sub.s32 	%r71, %r1, %r3;
	mov.u32 	%r633, %tid.x;
	setp.ge.s32 	%p63, %r633, %r71;
	@%p63 bra 	$L__BB5_54;

	mov.u32 	%r473, %tid.x;
	add.s32 	%r474, %r3, %r473;
	mul.wide.s32 	%rd290, %r474, 4;
	add.s64 	%rd289, %rd43, %rd290;
	// begin inline asm
	ld.global.nc.u32 %r472, [%rd289];
	// end inline asm
	mov.b32 	%f97, %r472;
	cvt.f64.f32 	%fd445, %f97;
	add.s32 	%r633, %r473, 256;

$L__BB5_54:
	setp.ge.s32 	%p64, %r633, %r71;
	@%p64 bra 	$L__BB5_61;

	not.b32 	%r475, %r633;
	add.s32 	%r476, %r1, %r475;
	mad.lo.s32 	%r75, %r90, -5120, %r476;
	shr.u32 	%r478, %r75, 8;
	add.s32 	%r479, %r478, 1;
	and.b32  	%r632, %r479, 3;
	setp.eq.s32 	%p65, %r632, 0;
	@%p65 bra 	$L__BB5_58;

	add.s32 	%r480, %r633, %r3;
	mul.wide.s32 	%rd291, %r480, 4;
	add.s64 	%rd326, %rd43, %rd291;

$L__BB5_57:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r481, [%rd326];
	// end inline asm
	mov.b32 	%f98, %r481;
	cvt.f64.f32 	%fd353, %f98;
	add.f64 	%fd445, %fd445, %fd353;
	add.s32 	%r633, %r633, 256;
	add.s64 	%rd326, %rd326, 1024;
	add.s32 	%r632, %r632, -1;
	setp.ne.s32 	%p66, %r632, 0;
	@%p66 bra 	$L__BB5_57;

$L__BB5_58:
	setp.lt.u32 	%p67, %r75, 768;
	@%p67 bra 	$L__BB5_61;

	add.s32 	%r482, %r633, %r3;
	mul.wide.s32 	%rd293, %r482, 4;
	add.s64 	%rd327, %rd43, %rd293;

$L__BB5_60:
	// begin inline asm
	ld.global.nc.u32 %r483, [%rd327];
	// end inline asm
	mov.b32 	%f99, %r483;
	cvt.f64.f32 	%fd354, %f99;
	add.f64 	%fd355, %fd445, %fd354;
	add.s64 	%rd295, %rd327, 1024;
	// begin inline asm
	ld.global.nc.u32 %r484, [%rd295];
	// end inline asm
	mov.b32 	%f100, %r484;
	cvt.f64.f32 	%fd356, %f100;
	add.f64 	%fd357, %fd355, %fd356;
	add.s64 	%rd296, %rd327, 2048;
	// begin inline asm
	ld.global.nc.u32 %r485, [%rd296];
	// end inline asm
	mov.b32 	%f101, %r485;
	cvt.f64.f32 	%fd358, %f101;
	add.f64 	%fd359, %fd357, %fd358;
	add.s64 	%rd297, %rd327, 3072;
	// begin inline asm
	ld.global.nc.u32 %r486, [%rd297];
	// end inline asm
	mov.b32 	%f102, %r486;
	cvt.f64.f32 	%fd360, %f102;
	add.f64 	%fd445, %fd359, %fd360;
	add.s64 	%rd327, %rd327, 4096;
	add.s32 	%r633, %r633, 1024;
	setp.lt.s32 	%p68, %r633, %r71;
	@%p68 bra 	$L__BB5_60;

$L__BB5_61:
	mov.u32 	%r488, %tid.x;
	shr.s32 	%r489, %r488, 31;
	shr.u32 	%r490, %r489, 27;
	add.s32 	%r491, %r488, %r490;
	shr.s32 	%r84, %r491, 5;
	// begin inline asm
	mov.u32 %r487, %laneid;
	// end inline asm
	mov.b64 	%rd298, %fd445;
	mov.b64 	{%r86, %r87}, %rd298;
	shl.b32 	%r492, %r84, 3;
	mov.u32 	%r493, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r494, %r493, %r492;
	setp.gt.s32 	%p69, %r71, 255;
	@%p69 bra 	$L__BB5_67;
	bra.uni 	$L__BB5_62;

$L__BB5_67:
	setp.ne.s32 	%p85, %r487, 0;
	// begin inline asm
	mov.u32 %r555, %laneid;
	// end inline asm
	mov.u32 	%r563, 1;
	mov.u32 	%r604, 31;
	mov.u32 	%r605, -1;
	// begin inline asm
	shfl.sync.down.b32 %r556, %r86, %r563, %r604, %r605;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r561, %r87, %r563, %r604, %r605;
	// end inline asm
	mov.b64 	%rd308, {%r556, %r561};
	mov.b64 	%fd393, %rd308;
	setp.gt.s32 	%p86, %r555, 30;
	add.f64 	%fd394, %fd445, %fd393;
	selp.f64 	%fd395, %fd445, %fd394, %p86;
	mov.b64 	%rd309, %fd395;
	mov.u32 	%r573, 2;
	mov.b64 	{%r567, %r572}, %rd309;
	// begin inline asm
	shfl.sync.down.b32 %r566, %r567, %r573, %r604, %r605;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r571, %r572, %r573, %r604, %r605;
	// end inline asm
	mov.b64 	%rd310, {%r566, %r571};
	mov.b64 	%fd396, %rd310;
	setp.gt.s32 	%p87, %r555, 29;
	add.f64 	%fd397, %fd395, %fd396;
	selp.f64 	%fd398, %fd395, %fd397, %p87;
	mov.b64 	%rd311, %fd398;
	mov.u32 	%r583, 4;
	mov.b64 	{%r577, %r582}, %rd311;
	// begin inline asm
	shfl.sync.down.b32 %r576, %r577, %r583, %r604, %r605;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r581, %r582, %r583, %r604, %r605;
	// end inline asm
	mov.b64 	%rd312, {%r576, %r581};
	mov.b64 	%fd399, %rd312;
	setp.gt.s32 	%p88, %r555, 27;
	add.f64 	%fd400, %fd398, %fd399;
	selp.f64 	%fd401, %fd398, %fd400, %p88;
	mov.b64 	%rd313, %fd401;
	mov.u32 	%r593, 8;
	mov.b64 	{%r587, %r592}, %rd313;
	// begin inline asm
	shfl.sync.down.b32 %r586, %r587, %r593, %r604, %r605;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r591, %r592, %r593, %r604, %r605;
	// end inline asm
	mov.b64 	%rd314, {%r586, %r591};
	mov.b64 	%fd402, %rd314;
	setp.gt.s32 	%p89, %r555, 23;
	add.f64 	%fd403, %fd401, %fd402;
	selp.f64 	%fd404, %fd401, %fd403, %p89;
	mov.b64 	%rd315, %fd404;
	mov.u32 	%r603, 16;
	mov.b64 	{%r597, %r602}, %rd315;
	// begin inline asm
	shfl.sync.down.b32 %r596, %r597, %r603, %r604, %r605;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r601, %r602, %r603, %r604, %r605;
	// end inline asm
	mov.b64 	%rd316, {%r596, %r601};
	mov.b64 	%fd405, %rd316;
	setp.gt.s32 	%p90, %r555, 15;
	add.f64 	%fd406, %fd404, %fd405;
	selp.f64 	%fd446, %fd404, %fd406, %p90;
	@%p85 bra 	$L__BB5_69;

	add.s32 	%r612, %r494, 8;
	st.shared.f64 	[%r612], %fd446;

$L__BB5_69:
	bar.sync 	0;
	setp.ne.s32 	%p91, %r488, 0;
	@%p91 bra 	$L__BB5_71;

	ld.shared.f64 	%fd407, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd408, %fd446, %fd407;
	ld.shared.f64 	%fd409, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd410, %fd408, %fd409;
	ld.shared.f64 	%fd411, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd412, %fd410, %fd411;
	ld.shared.f64 	%fd413, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd414, %fd412, %fd413;
	ld.shared.f64 	%fd415, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd416, %fd414, %fd415;
	ld.shared.f64 	%fd417, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd418, %fd416, %fd417;
	ld.shared.f64 	%fd419, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd418, %fd419;
	bra.uni 	$L__BB5_71;

$L__BB5_2:
	cvt.s64.s32 	%rd66, %r3;
	mov.u32 	%r113, %tid.x;
	cvt.s64.s32 	%rd67, %r113;
	add.s64 	%rd68, %rd67, %rd66;
	shl.b64 	%rd69, %rd68, 2;
	add.s64 	%rd46, %rd43, %rd69;
	// begin inline asm
	ld.global.nc.u32 %r93, [%rd46];
	// end inline asm
	mov.b32 	%f1, %r93;
	cvt.f64.f32 	%fd56, %f1;
	add.s32 	%r114, %r113, 256;
	cvt.s64.s32 	%rd70, %r114;
	add.s64 	%rd71, %rd70, %rd66;
	shl.b64 	%rd72, %rd71, 2;
	add.s64 	%rd47, %rd43, %rd72;
	// begin inline asm
	ld.global.nc.u32 %r94, [%rd47];
	// end inline asm
	mov.b32 	%f2, %r94;
	cvt.f64.f32 	%fd57, %f2;
	add.s32 	%r4, %r113, 512;
	cvt.s64.s32 	%rd73, %r4;
	add.s64 	%rd74, %rd73, %rd66;
	shl.b64 	%rd75, %rd74, 2;
	add.s64 	%rd48, %rd43, %rd75;
	// begin inline asm
	ld.global.nc.u32 %r95, [%rd48];
	// end inline asm
	mov.b32 	%f3, %r95;
	cvt.f64.f32 	%fd58, %f3;
	add.s32 	%r5, %r113, 768;
	cvt.s64.s32 	%rd76, %r5;
	add.s64 	%rd77, %rd76, %rd66;
	shl.b64 	%rd78, %rd77, 2;
	add.s64 	%rd49, %rd43, %rd78;
	// begin inline asm
	ld.global.nc.u32 %r96, [%rd49];
	// end inline asm
	mov.b32 	%f4, %r96;
	cvt.f64.f32 	%fd59, %f4;
	add.s32 	%r6, %r113, 1024;
	cvt.s64.s32 	%rd79, %r6;
	add.s64 	%rd80, %rd79, %rd66;
	shl.b64 	%rd81, %rd80, 2;
	add.s64 	%rd50, %rd43, %rd81;
	// begin inline asm
	ld.global.nc.u32 %r97, [%rd50];
	// end inline asm
	mov.b32 	%f5, %r97;
	cvt.f64.f32 	%fd60, %f5;
	add.s32 	%r7, %r113, 1280;
	cvt.s64.s32 	%rd82, %r7;
	add.s64 	%rd83, %rd82, %rd66;
	shl.b64 	%rd84, %rd83, 2;
	add.s64 	%rd51, %rd43, %rd84;
	// begin inline asm
	ld.global.nc.u32 %r98, [%rd51];
	// end inline asm
	mov.b32 	%f6, %r98;
	cvt.f64.f32 	%fd61, %f6;
	add.s32 	%r8, %r113, 1536;
	cvt.s64.s32 	%rd85, %r8;
	add.s64 	%rd86, %rd85, %rd66;
	shl.b64 	%rd87, %rd86, 2;
	add.s64 	%rd52, %rd43, %rd87;
	// begin inline asm
	ld.global.nc.u32 %r99, [%rd52];
	// end inline asm
	mov.b32 	%f7, %r99;
	cvt.f64.f32 	%fd62, %f7;
	add.s32 	%r9, %r113, 1792;
	cvt.s64.s32 	%rd88, %r9;
	add.s64 	%rd89, %rd88, %rd66;
	shl.b64 	%rd90, %rd89, 2;
	add.s64 	%rd53, %rd43, %rd90;
	// begin inline asm
	ld.global.nc.u32 %r100, [%rd53];
	// end inline asm
	mov.b32 	%f8, %r100;
	cvt.f64.f32 	%fd63, %f8;
	add.s32 	%r10, %r113, 2048;
	cvt.s64.s32 	%rd91, %r10;
	add.s64 	%rd92, %rd91, %rd66;
	shl.b64 	%rd93, %rd92, 2;
	add.s64 	%rd54, %rd43, %rd93;
	// begin inline asm
	ld.global.nc.u32 %r101, [%rd54];
	// end inline asm
	mov.b32 	%f9, %r101;
	cvt.f64.f32 	%fd64, %f9;
	add.s32 	%r11, %r113, 2304;
	cvt.s64.s32 	%rd94, %r11;
	add.s64 	%rd95, %rd94, %rd66;
	shl.b64 	%rd96, %rd95, 2;
	add.s64 	%rd55, %rd43, %rd96;
	// begin inline asm
	ld.global.nc.u32 %r102, [%rd55];
	// end inline asm
	mov.b32 	%f10, %r102;
	cvt.f64.f32 	%fd65, %f10;
	add.s32 	%r12, %r113, 2560;
	cvt.s64.s32 	%rd97, %r12;
	add.s64 	%rd98, %rd97, %rd66;
	shl.b64 	%rd99, %rd98, 2;
	add.s64 	%rd56, %rd43, %rd99;
	// begin inline asm
	ld.global.nc.u32 %r103, [%rd56];
	// end inline asm
	mov.b32 	%f11, %r103;
	cvt.f64.f32 	%fd66, %f11;
	add.s32 	%r13, %r113, 2816;
	cvt.s64.s32 	%rd100, %r13;
	add.s64 	%rd101, %rd100, %rd66;
	shl.b64 	%rd102, %rd101, 2;
	add.s64 	%rd57, %rd43, %rd102;
	// begin inline asm
	ld.global.nc.u32 %r104, [%rd57];
	// end inline asm
	mov.b32 	%f12, %r104;
	cvt.f64.f32 	%fd67, %f12;
	add.s32 	%r14, %r113, 3072;
	cvt.s64.s32 	%rd103, %r14;
	add.s64 	%rd104, %rd103, %rd66;
	shl.b64 	%rd105, %rd104, 2;
	add.s64 	%rd58, %rd43, %rd105;
	// begin inline asm
	ld.global.nc.u32 %r105, [%rd58];
	// end inline asm
	mov.b32 	%f13, %r105;
	cvt.f64.f32 	%fd68, %f13;
	add.s32 	%r15, %r113, 3328;
	cvt.s64.s32 	%rd106, %r15;
	add.s64 	%rd107, %rd106, %rd66;
	shl.b64 	%rd108, %rd107, 2;
	add.s64 	%rd59, %rd43, %rd108;
	// begin inline asm
	ld.global.nc.u32 %r106, [%rd59];
	// end inline asm
	mov.b32 	%f14, %r106;
	cvt.f64.f32 	%fd69, %f14;
	add.s32 	%r16, %r113, 3584;
	cvt.s64.s32 	%rd109, %r16;
	add.s64 	%rd110, %rd109, %rd66;
	shl.b64 	%rd111, %rd110, 2;
	add.s64 	%rd60, %rd43, %rd111;
	// begin inline asm
	ld.global.nc.u32 %r107, [%rd60];
	// end inline asm
	mov.b32 	%f15, %r107;
	cvt.f64.f32 	%fd70, %f15;
	add.s32 	%r17, %r113, 3840;
	cvt.s64.s32 	%rd112, %r17;
	add.s64 	%rd113, %rd112, %rd66;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd61, %rd43, %rd114;
	// begin inline asm
	ld.global.nc.u32 %r108, [%rd61];
	// end inline asm
	mov.b32 	%f16, %r108;
	cvt.f64.f32 	%fd71, %f16;
	add.s32 	%r18, %r113, 4096;
	cvt.s64.s32 	%rd115, %r18;
	add.s64 	%rd116, %rd115, %rd66;
	shl.b64 	%rd117, %rd116, 2;
	add.s64 	%rd62, %rd43, %rd117;
	// begin inline asm
	ld.global.nc.u32 %r109, [%rd62];
	// end inline asm
	mov.b32 	%f17, %r109;
	cvt.f64.f32 	%fd72, %f17;
	add.s32 	%r19, %r113, 4352;
	cvt.s64.s32 	%rd118, %r19;
	add.s64 	%rd119, %rd118, %rd66;
	shl.b64 	%rd120, %rd119, 2;
	add.s64 	%rd63, %rd43, %rd120;
	// begin inline asm
	ld.global.nc.u32 %r110, [%rd63];
	// end inline asm
	mov.b32 	%f18, %r110;
	cvt.f64.f32 	%fd73, %f18;
	add.s32 	%r115, %r113, 4608;
	cvt.s64.s32 	%rd121, %r115;
	add.s64 	%rd122, %rd121, %rd66;
	shl.b64 	%rd123, %rd122, 2;
	add.s64 	%rd64, %rd43, %rd123;
	// begin inline asm
	ld.global.nc.u32 %r111, [%rd64];
	// end inline asm
	mov.b32 	%f19, %r111;
	cvt.f64.f32 	%fd74, %f19;
	add.s32 	%r116, %r113, 4864;
	cvt.s64.s32 	%rd124, %r116;
	add.s64 	%rd125, %rd124, %rd66;
	shl.b64 	%rd126, %rd125, 2;
	add.s64 	%rd65, %rd43, %rd126;
	// begin inline asm
	ld.global.nc.u32 %r112, [%rd65];
	// end inline asm
	mov.b32 	%f20, %r112;
	cvt.f64.f32 	%fd75, %f20;
	add.f64 	%fd76, %fd56, %fd57;
	add.f64 	%fd77, %fd76, %fd58;
	add.f64 	%fd78, %fd77, %fd59;
	add.f64 	%fd79, %fd78, %fd60;
	add.f64 	%fd80, %fd79, %fd61;
	add.f64 	%fd81, %fd80, %fd62;
	add.f64 	%fd82, %fd81, %fd63;
	add.f64 	%fd83, %fd82, %fd64;
	add.f64 	%fd84, %fd83, %fd65;
	add.f64 	%fd85, %fd84, %fd66;
	add.f64 	%fd86, %fd85, %fd67;
	add.f64 	%fd87, %fd86, %fd68;
	add.f64 	%fd88, %fd87, %fd69;
	add.f64 	%fd89, %fd88, %fd70;
	add.f64 	%fd90, %fd89, %fd71;
	add.f64 	%fd91, %fd90, %fd72;
	add.f64 	%fd92, %fd91, %fd73;
	add.f64 	%fd93, %fd92, %fd74;
	add.f64 	%fd426, %fd93, %fd75;
	add.s32 	%r614, %r3, %r2;
	add.s32 	%r117, %r614, 5120;
	setp.gt.s32 	%p3, %r117, %r1;
	@%p3 bra 	$L__BB5_5;

	mad.lo.s32 	%r614, %r90, 5120, %r2;

$L__BB5_4:
	cvt.s64.s32 	%rd147, %r614;
	add.s64 	%rd148, %rd67, %rd147;
	shl.b64 	%rd149, %rd148, 2;
	add.s64 	%rd127, %rd43, %rd149;
	// begin inline asm
	ld.global.nc.u32 %r120, [%rd127];
	// end inline asm
	mov.b32 	%f21, %r120;
	cvt.f64.f32 	%fd94, %f21;
	add.s64 	%rd151, %rd70, %rd147;
	shl.b64 	%rd152, %rd151, 2;
	add.s64 	%rd128, %rd43, %rd152;
	// begin inline asm
	ld.global.nc.u32 %r121, [%rd128];
	// end inline asm
	mov.b32 	%f22, %r121;
	cvt.f64.f32 	%fd95, %f22;
	add.s64 	%rd153, %rd73, %rd147;
	shl.b64 	%rd154, %rd153, 2;
	add.s64 	%rd129, %rd43, %rd154;
	// begin inline asm
	ld.global.nc.u32 %r122, [%rd129];
	// end inline asm
	mov.b32 	%f23, %r122;
	cvt.f64.f32 	%fd96, %f23;
	add.s64 	%rd155, %rd76, %rd147;
	shl.b64 	%rd156, %rd155, 2;
	add.s64 	%rd130, %rd43, %rd156;
	// begin inline asm
	ld.global.nc.u32 %r123, [%rd130];
	// end inline asm
	mov.b32 	%f24, %r123;
	cvt.f64.f32 	%fd97, %f24;
	add.s64 	%rd157, %rd79, %rd147;
	shl.b64 	%rd158, %rd157, 2;
	add.s64 	%rd131, %rd43, %rd158;
	// begin inline asm
	ld.global.nc.u32 %r124, [%rd131];
	// end inline asm
	mov.b32 	%f25, %r124;
	cvt.f64.f32 	%fd98, %f25;
	add.s64 	%rd159, %rd82, %rd147;
	shl.b64 	%rd160, %rd159, 2;
	add.s64 	%rd132, %rd43, %rd160;
	// begin inline asm
	ld.global.nc.u32 %r125, [%rd132];
	// end inline asm
	mov.b32 	%f26, %r125;
	cvt.f64.f32 	%fd99, %f26;
	add.s64 	%rd161, %rd85, %rd147;
	shl.b64 	%rd162, %rd161, 2;
	add.s64 	%rd133, %rd43, %rd162;
	// begin inline asm
	ld.global.nc.u32 %r126, [%rd133];
	// end inline asm
	mov.b32 	%f27, %r126;
	cvt.f64.f32 	%fd100, %f27;
	add.s64 	%rd163, %rd88, %rd147;
	shl.b64 	%rd164, %rd163, 2;
	add.s64 	%rd134, %rd43, %rd164;
	// begin inline asm
	ld.global.nc.u32 %r127, [%rd134];
	// end inline asm
	mov.b32 	%f28, %r127;
	cvt.f64.f32 	%fd101, %f28;
	add.s64 	%rd165, %rd91, %rd147;
	shl.b64 	%rd166, %rd165, 2;
	add.s64 	%rd135, %rd43, %rd166;
	// begin inline asm
	ld.global.nc.u32 %r128, [%rd135];
	// end inline asm
	mov.b32 	%f29, %r128;
	cvt.f64.f32 	%fd102, %f29;
	add.s64 	%rd167, %rd94, %rd147;
	shl.b64 	%rd168, %rd167, 2;
	add.s64 	%rd136, %rd43, %rd168;
	// begin inline asm
	ld.global.nc.u32 %r129, [%rd136];
	// end inline asm
	mov.b32 	%f30, %r129;
	cvt.f64.f32 	%fd103, %f30;
	add.s64 	%rd169, %rd97, %rd147;
	shl.b64 	%rd170, %rd169, 2;
	add.s64 	%rd137, %rd43, %rd170;
	// begin inline asm
	ld.global.nc.u32 %r130, [%rd137];
	// end inline asm
	mov.b32 	%f31, %r130;
	cvt.f64.f32 	%fd104, %f31;
	add.s64 	%rd171, %rd100, %rd147;
	shl.b64 	%rd172, %rd171, 2;
	add.s64 	%rd138, %rd43, %rd172;
	// begin inline asm
	ld.global.nc.u32 %r131, [%rd138];
	// end inline asm
	mov.b32 	%f32, %r131;
	cvt.f64.f32 	%fd105, %f32;
	add.s64 	%rd173, %rd103, %rd147;
	shl.b64 	%rd174, %rd173, 2;
	add.s64 	%rd139, %rd43, %rd174;
	// begin inline asm
	ld.global.nc.u32 %r132, [%rd139];
	// end inline asm
	mov.b32 	%f33, %r132;
	cvt.f64.f32 	%fd106, %f33;
	add.s64 	%rd175, %rd106, %rd147;
	shl.b64 	%rd176, %rd175, 2;
	add.s64 	%rd140, %rd43, %rd176;
	// begin inline asm
	ld.global.nc.u32 %r133, [%rd140];
	// end inline asm
	mov.b32 	%f34, %r133;
	cvt.f64.f32 	%fd107, %f34;
	add.s64 	%rd177, %rd109, %rd147;
	shl.b64 	%rd178, %rd177, 2;
	add.s64 	%rd141, %rd43, %rd178;
	// begin inline asm
	ld.global.nc.u32 %r134, [%rd141];
	// end inline asm
	mov.b32 	%f35, %r134;
	cvt.f64.f32 	%fd108, %f35;
	add.s64 	%rd179, %rd112, %rd147;
	shl.b64 	%rd180, %rd179, 2;
	add.s64 	%rd142, %rd43, %rd180;
	// begin inline asm
	ld.global.nc.u32 %r135, [%rd142];
	// end inline asm
	mov.b32 	%f36, %r135;
	cvt.f64.f32 	%fd109, %f36;
	add.s64 	%rd181, %rd115, %rd147;
	shl.b64 	%rd182, %rd181, 2;
	add.s64 	%rd143, %rd43, %rd182;
	// begin inline asm
	ld.global.nc.u32 %r136, [%rd143];
	// end inline asm
	mov.b32 	%f37, %r136;
	cvt.f64.f32 	%fd110, %f37;
	add.s64 	%rd183, %rd118, %rd147;
	shl.b64 	%rd184, %rd183, 2;
	add.s64 	%rd144, %rd43, %rd184;
	// begin inline asm
	ld.global.nc.u32 %r137, [%rd144];
	// end inline asm
	mov.b32 	%f38, %r137;
	cvt.f64.f32 	%fd111, %f38;
	add.s64 	%rd186, %rd121, %rd147;
	shl.b64 	%rd187, %rd186, 2;
	add.s64 	%rd145, %rd43, %rd187;
	// begin inline asm
	ld.global.nc.u32 %r138, [%rd145];
	// end inline asm
	mov.b32 	%f39, %r138;
	cvt.f64.f32 	%fd112, %f39;
	add.s64 	%rd189, %rd124, %rd147;
	shl.b64 	%rd190, %rd189, 2;
	add.s64 	%rd146, %rd43, %rd190;
	// begin inline asm
	ld.global.nc.u32 %r139, [%rd146];
	// end inline asm
	mov.b32 	%f40, %r139;
	cvt.f64.f32 	%fd113, %f40;
	add.f64 	%fd114, %fd426, %fd94;
	add.f64 	%fd115, %fd114, %fd95;
	add.f64 	%fd116, %fd115, %fd96;
	add.f64 	%fd117, %fd116, %fd97;
	add.f64 	%fd118, %fd117, %fd98;
	add.f64 	%fd119, %fd118, %fd99;
	add.f64 	%fd120, %fd119, %fd100;
	add.f64 	%fd121, %fd120, %fd101;
	add.f64 	%fd122, %fd121, %fd102;
	add.f64 	%fd123, %fd122, %fd103;
	add.f64 	%fd124, %fd123, %fd104;
	add.f64 	%fd125, %fd124, %fd105;
	add.f64 	%fd126, %fd125, %fd106;
	add.f64 	%fd127, %fd126, %fd107;
	add.f64 	%fd128, %fd127, %fd108;
	add.f64 	%fd129, %fd128, %fd109;
	add.f64 	%fd130, %fd129, %fd110;
	add.f64 	%fd131, %fd130, %fd111;
	add.f64 	%fd132, %fd131, %fd112;
	add.f64 	%fd426, %fd132, %fd113;
	add.s32 	%r614, %r614, %r2;
	add.s32 	%r144, %r614, 5120;
	setp.le.s32 	%p4, %r144, %r1;
	@%p4 bra 	$L__BB5_4;

$L__BB5_5:
	setp.le.s32 	%p5, %r1, %r614;
	@%p5 bra 	$L__BB5_13;

	sub.s32 	%r25, %r1, %r614;
	setp.ge.s32 	%p6, %r113, %r25;
	@%p6 bra 	$L__BB5_13;

	not.b32 	%r146, %r113;
	add.s32 	%r147, %r1, %r146;
	sub.s32 	%r27, %r147, %r614;
	shr.u32 	%r148, %r27, 8;
	add.s32 	%r149, %r148, 1;
	and.b32  	%r616, %r149, 3;
	setp.eq.s32 	%p7, %r616, 0;
	mov.u32 	%r617, %r113;
	@%p7 bra 	$L__BB5_10;

	mov.u32 	%r617, %tid.x;
	add.s32 	%r150, %r617, %r614;
	mul.wide.s32 	%rd191, %r150, 4;
	add.s64 	%rd320, %rd43, %rd191;

$L__BB5_9:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r151, [%rd320];
	// end inline asm
	mov.b32 	%f41, %r151;
	cvt.f64.f32 	%fd134, %f41;
	add.f64 	%fd426, %fd426, %fd134;
	add.s32 	%r617, %r617, 256;
	add.s64 	%rd320, %rd320, 1024;
	add.s32 	%r616, %r616, -1;
	setp.ne.s32 	%p8, %r616, 0;
	@%p8 bra 	$L__BB5_9;

$L__BB5_10:
	setp.lt.u32 	%p9, %r27, 768;
	@%p9 bra 	$L__BB5_13;

	add.s32 	%r152, %r617, %r614;
	mul.wide.s32 	%rd193, %r152, 4;
	add.s64 	%rd321, %rd43, %rd193;

$L__BB5_12:
	// begin inline asm
	ld.global.nc.u32 %r153, [%rd321];
	// end inline asm
	mov.b32 	%f42, %r153;
	cvt.f64.f32 	%fd135, %f42;
	add.f64 	%fd136, %fd426, %fd135;
	add.s64 	%rd195, %rd321, 1024;
	// begin inline asm
	ld.global.nc.u32 %r154, [%rd195];
	// end inline asm
	mov.b32 	%f43, %r154;
	cvt.f64.f32 	%fd137, %f43;
	add.f64 	%fd138, %fd136, %fd137;
	add.s64 	%rd196, %rd321, 2048;
	// begin inline asm
	ld.global.nc.u32 %r155, [%rd196];
	// end inline asm
	mov.b32 	%f44, %r155;
	cvt.f64.f32 	%fd139, %f44;
	add.f64 	%fd140, %fd138, %fd139;
	add.s64 	%rd197, %rd321, 3072;
	// begin inline asm
	ld.global.nc.u32 %r156, [%rd197];
	// end inline asm
	mov.b32 	%f45, %r156;
	cvt.f64.f32 	%fd141, %f45;
	add.f64 	%fd426, %fd140, %fd141;
	add.s64 	%rd321, %rd321, 4096;
	add.s32 	%r617, %r617, 1024;
	setp.lt.s32 	%p10, %r617, %r25;
	@%p10 bra 	$L__BB5_12;

$L__BB5_13:
	// begin inline asm
	mov.u32 %r157, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r158, %laneid;
	// end inline asm
	mov.b64 	%rd198, %fd426;
	mov.u32 	%r166, 1;
	mov.u32 	%r207, 31;
	mov.u32 	%r208, -1;
	mov.b64 	{%r160, %r165}, %rd198;
	// begin inline asm
	shfl.sync.down.b32 %r159, %r160, %r166, %r207, %r208;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r164, %r165, %r166, %r207, %r208;
	// end inline asm
	mov.b64 	%rd199, {%r159, %r164};
	mov.b64 	%fd142, %rd199;
	setp.gt.s32 	%p11, %r158, 30;
	add.f64 	%fd143, %fd426, %fd142;
	selp.f64 	%fd144, %fd426, %fd143, %p11;
	mov.b64 	%rd200, %fd144;
	mov.u32 	%r176, 2;
	mov.b64 	{%r170, %r175}, %rd200;
	// begin inline asm
	shfl.sync.down.b32 %r169, %r170, %r176, %r207, %r208;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r174, %r175, %r176, %r207, %r208;
	// end inline asm
	mov.b64 	%rd201, {%r169, %r174};
	mov.b64 	%fd145, %rd201;
	setp.gt.s32 	%p12, %r158, 29;
	add.f64 	%fd146, %fd144, %fd145;
	selp.f64 	%fd147, %fd144, %fd146, %p12;
	mov.b64 	%rd202, %fd147;
	mov.u32 	%r186, 4;
	mov.b64 	{%r180, %r185}, %rd202;
	// begin inline asm
	shfl.sync.down.b32 %r179, %r180, %r186, %r207, %r208;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r184, %r185, %r186, %r207, %r208;
	// end inline asm
	mov.b64 	%rd203, {%r179, %r184};
	mov.b64 	%fd148, %rd203;
	setp.gt.s32 	%p13, %r158, 27;
	add.f64 	%fd149, %fd147, %fd148;
	selp.f64 	%fd150, %fd147, %fd149, %p13;
	mov.b64 	%rd204, %fd150;
	mov.u32 	%r196, 8;
	mov.b64 	{%r190, %r195}, %rd204;
	// begin inline asm
	shfl.sync.down.b32 %r189, %r190, %r196, %r207, %r208;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r194, %r195, %r196, %r207, %r208;
	// end inline asm
	mov.b64 	%rd205, {%r189, %r194};
	mov.b64 	%fd151, %rd205;
	setp.gt.s32 	%p14, %r158, 23;
	add.f64 	%fd152, %fd150, %fd151;
	selp.f64 	%fd153, %fd150, %fd152, %p14;
	mov.b64 	%rd206, %fd153;
	mov.u32 	%r206, 16;
	mov.b64 	{%r200, %r205}, %rd206;
	// begin inline asm
	shfl.sync.down.b32 %r199, %r200, %r206, %r207, %r208;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r204, %r205, %r206, %r207, %r208;
	// end inline asm
	mov.b64 	%rd207, {%r199, %r204};
	mov.b64 	%fd154, %rd207;
	setp.gt.s32 	%p15, %r158, 15;
	add.f64 	%fd155, %fd153, %fd154;
	selp.f64 	%fd446, %fd153, %fd155, %p15;
	setp.ne.s32 	%p16, %r157, 0;
	@%p16 bra 	$L__BB5_15;

	shr.s32 	%r210, %r113, 31;
	shr.u32 	%r211, %r210, 27;
	add.s32 	%r212, %r113, %r211;
	shr.s32 	%r213, %r212, 5;
	shl.b32 	%r214, %r213, 3;
	mov.u32 	%r215, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r216, %r215, %r214;
	st.shared.f64 	[%r216+8], %fd446;

$L__BB5_15:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r113, 0;
	@%p17 bra 	$L__BB5_71;

	ld.shared.f64 	%fd156, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd157, %fd446, %fd156;
	ld.shared.f64 	%fd158, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd159, %fd157, %fd158;
	ld.shared.f64 	%fd160, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd161, %fd159, %fd160;
	ld.shared.f64 	%fd162, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd163, %fd161, %fd162;
	ld.shared.f64 	%fd164, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd165, %fd163, %fd164;
	ld.shared.f64 	%fd166, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd167, %fd165, %fd166;
	ld.shared.f64 	%fd168, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd167, %fd168;
	bra.uni 	$L__BB5_71;

$L__BB5_37:
	cvt.s64.s32 	%rd251, %r3;
	mov.u32 	%r355, %tid.x;
	shl.b32 	%r356, %r355, 2;
	cvt.u64.u32 	%rd30, %r356;
	add.s64 	%rd252, %rd251, %rd30;
	shl.b64 	%rd253, %rd252, 2;
	add.s64 	%rd238, %rd43, %rd253;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd236, %rd237}, [%rd238];
	// end inline asm
	mov.b64 	{%r357, %r358}, %rd237;
	mov.b64 	{%r359, %r360}, %rd236;
	mov.b32 	%f52, %r359;
	mov.b32 	%f53, %r360;
	mov.b32 	%f54, %r357;
	mov.b32 	%f55, %r358;
	add.s64 	%rd241, %rd238, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd239, %rd240}, [%rd241];
	// end inline asm
	mov.b64 	{%r361, %r362}, %rd240;
	mov.b64 	{%r363, %r364}, %rd239;
	mov.b32 	%f56, %r363;
	mov.b32 	%f57, %r364;
	mov.b32 	%f58, %r361;
	mov.b32 	%f59, %r362;
	add.s64 	%rd244, %rd238, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd242, %rd243}, [%rd244];
	// end inline asm
	mov.b64 	{%r365, %r366}, %rd243;
	mov.b64 	{%r367, %r368}, %rd242;
	mov.b32 	%f60, %r367;
	mov.b32 	%f61, %r368;
	mov.b32 	%f62, %r365;
	mov.b32 	%f63, %r366;
	add.s64 	%rd247, %rd238, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd245, %rd246}, [%rd247];
	// end inline asm
	mov.b64 	{%r369, %r370}, %rd246;
	mov.b64 	{%r371, %r372}, %rd245;
	mov.b32 	%f64, %r371;
	mov.b32 	%f65, %r372;
	mov.b32 	%f66, %r369;
	mov.b32 	%f67, %r370;
	add.s64 	%rd250, %rd238, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd248, %rd249}, [%rd250];
	// end inline asm
	mov.b64 	{%r373, %r374}, %rd249;
	mov.b64 	{%r375, %r376}, %rd248;
	mov.b32 	%f68, %r375;
	mov.b32 	%f69, %r376;
	mov.b32 	%f70, %r373;
	mov.b32 	%f71, %r374;
	cvt.f64.f32 	%fd238, %f52;
	cvt.f64.f32 	%fd239, %f53;
	cvt.f64.f32 	%fd240, %f54;
	cvt.f64.f32 	%fd241, %f55;
	cvt.f64.f32 	%fd242, %f56;
	cvt.f64.f32 	%fd243, %f57;
	cvt.f64.f32 	%fd244, %f58;
	cvt.f64.f32 	%fd245, %f59;
	cvt.f64.f32 	%fd246, %f60;
	cvt.f64.f32 	%fd247, %f61;
	cvt.f64.f32 	%fd248, %f62;
	cvt.f64.f32 	%fd249, %f63;
	cvt.f64.f32 	%fd250, %f64;
	cvt.f64.f32 	%fd251, %f65;
	cvt.f64.f32 	%fd252, %f66;
	cvt.f64.f32 	%fd253, %f67;
	cvt.f64.f32 	%fd254, %f68;
	cvt.f64.f32 	%fd255, %f69;
	cvt.f64.f32 	%fd256, %f70;
	cvt.f64.f32 	%fd257, %f71;
	add.f64 	%fd258, %fd238, %fd239;
	add.f64 	%fd259, %fd258, %fd240;
	add.f64 	%fd260, %fd259, %fd241;
	add.f64 	%fd261, %fd260, %fd242;
	add.f64 	%fd262, %fd261, %fd243;
	add.f64 	%fd263, %fd262, %fd244;
	add.f64 	%fd264, %fd263, %fd245;
	add.f64 	%fd265, %fd264, %fd246;
	add.f64 	%fd266, %fd265, %fd247;
	add.f64 	%fd267, %fd266, %fd248;
	add.f64 	%fd268, %fd267, %fd249;
	add.f64 	%fd269, %fd268, %fd250;
	add.f64 	%fd270, %fd269, %fd251;
	add.f64 	%fd271, %fd270, %fd252;
	add.f64 	%fd272, %fd271, %fd253;
	add.f64 	%fd273, %fd272, %fd254;
	add.f64 	%fd274, %fd273, %fd255;
	add.f64 	%fd275, %fd274, %fd256;
	add.f64 	%fd439, %fd275, %fd257;
	add.s32 	%r625, %r3, %r2;
	add.s32 	%r377, %r625, 5120;
	setp.gt.s32 	%p48, %r377, %r1;
	@%p48 bra 	$L__BB5_40;

$L__BB5_39:
	cvt.s64.s32 	%rd269, %r625;
	add.s64 	%rd270, %rd269, %rd30;
	shl.b64 	%rd271, %rd270, 2;
	add.s64 	%rd256, %rd43, %rd271;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd254, %rd255}, [%rd256];
	// end inline asm
	mov.b64 	{%r378, %r379}, %rd255;
	mov.b64 	{%r380, %r381}, %rd254;
	mov.b32 	%f72, %r380;
	mov.b32 	%f73, %r381;
	mov.b32 	%f74, %r378;
	mov.b32 	%f75, %r379;
	add.s64 	%rd259, %rd256, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd257, %rd258}, [%rd259];
	// end inline asm
	mov.b64 	{%r382, %r383}, %rd258;
	mov.b64 	{%r384, %r385}, %rd257;
	mov.b32 	%f76, %r384;
	mov.b32 	%f77, %r385;
	mov.b32 	%f78, %r382;
	mov.b32 	%f79, %r383;
	add.s64 	%rd262, %rd256, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd260, %rd261}, [%rd262];
	// end inline asm
	mov.b64 	{%r386, %r387}, %rd261;
	mov.b64 	{%r388, %r389}, %rd260;
	mov.b32 	%f80, %r388;
	mov.b32 	%f81, %r389;
	mov.b32 	%f82, %r386;
	mov.b32 	%f83, %r387;
	add.s64 	%rd265, %rd256, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd263, %rd264}, [%rd265];
	// end inline asm
	mov.b64 	{%r390, %r391}, %rd264;
	mov.b64 	{%r392, %r393}, %rd263;
	mov.b32 	%f84, %r392;
	mov.b32 	%f85, %r393;
	mov.b32 	%f86, %r390;
	mov.b32 	%f87, %r391;
	add.s64 	%rd268, %rd256, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd266, %rd267}, [%rd268];
	// end inline asm
	mov.b64 	{%r394, %r395}, %rd267;
	mov.b64 	{%r396, %r397}, %rd266;
	mov.b32 	%f88, %r396;
	mov.b32 	%f89, %r397;
	mov.b32 	%f90, %r394;
	mov.b32 	%f91, %r395;
	cvt.f64.f32 	%fd276, %f72;
	cvt.f64.f32 	%fd277, %f73;
	cvt.f64.f32 	%fd278, %f74;
	cvt.f64.f32 	%fd279, %f75;
	cvt.f64.f32 	%fd280, %f76;
	cvt.f64.f32 	%fd281, %f77;
	cvt.f64.f32 	%fd282, %f78;
	cvt.f64.f32 	%fd283, %f79;
	cvt.f64.f32 	%fd284, %f80;
	cvt.f64.f32 	%fd285, %f81;
	cvt.f64.f32 	%fd286, %f82;
	cvt.f64.f32 	%fd287, %f83;
	cvt.f64.f32 	%fd288, %f84;
	cvt.f64.f32 	%fd289, %f85;
	cvt.f64.f32 	%fd290, %f86;
	cvt.f64.f32 	%fd291, %f87;
	cvt.f64.f32 	%fd292, %f88;
	cvt.f64.f32 	%fd293, %f89;
	cvt.f64.f32 	%fd294, %f90;
	cvt.f64.f32 	%fd295, %f91;
	add.f64 	%fd296, %fd439, %fd276;
	add.f64 	%fd297, %fd296, %fd277;
	add.f64 	%fd298, %fd297, %fd278;
	add.f64 	%fd299, %fd298, %fd279;
	add.f64 	%fd300, %fd299, %fd280;
	add.f64 	%fd301, %fd300, %fd281;
	add.f64 	%fd302, %fd301, %fd282;
	add.f64 	%fd303, %fd302, %fd283;
	add.f64 	%fd304, %fd303, %fd284;
	add.f64 	%fd305, %fd304, %fd285;
	add.f64 	%fd306, %fd305, %fd286;
	add.f64 	%fd307, %fd306, %fd287;
	add.f64 	%fd308, %fd307, %fd288;
	add.f64 	%fd309, %fd308, %fd289;
	add.f64 	%fd310, %fd309, %fd290;
	add.f64 	%fd311, %fd310, %fd291;
	add.f64 	%fd312, %fd311, %fd292;
	add.f64 	%fd313, %fd312, %fd293;
	add.f64 	%fd314, %fd313, %fd294;
	add.f64 	%fd439, %fd314, %fd295;
	add.s32 	%r625, %r625, %r2;
	add.s32 	%r398, %r625, 5120;
	setp.le.s32 	%p49, %r398, %r1;
	@%p49 bra 	$L__BB5_39;

$L__BB5_40:
	setp.le.s32 	%p50, %r1, %r625;
	@%p50 bra 	$L__BB5_48;

	sub.s32 	%r59, %r1, %r625;
	setp.ge.s32 	%p51, %r355, %r59;
	@%p51 bra 	$L__BB5_48;

	not.b32 	%r400, %r355;
	add.s32 	%r401, %r1, %r400;
	sub.s32 	%r61, %r401, %r625;
	shr.u32 	%r402, %r61, 8;
	add.s32 	%r403, %r402, 1;
	and.b32  	%r627, %r403, 3;
	setp.eq.s32 	%p52, %r627, 0;
	mov.u32 	%r628, %r355;
	@%p52 bra 	$L__BB5_45;

	mov.u32 	%r628, %tid.x;
	add.s32 	%r404, %r628, %r625;
	mul.wide.s32 	%rd272, %r404, 4;
	add.s64 	%rd324, %rd43, %rd272;

$L__BB5_44:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r405, [%rd324];
	// end inline asm
	mov.b32 	%f92, %r405;
	cvt.f64.f32 	%fd316, %f92;
	add.f64 	%fd439, %fd439, %fd316;
	add.s32 	%r628, %r628, 256;
	add.s64 	%rd324, %rd324, 1024;
	add.s32 	%r627, %r627, -1;
	setp.ne.s32 	%p53, %r627, 0;
	@%p53 bra 	$L__BB5_44;

$L__BB5_45:
	setp.lt.u32 	%p54, %r61, 768;
	@%p54 bra 	$L__BB5_48;

	add.s32 	%r406, %r628, %r625;
	mul.wide.s32 	%rd274, %r406, 4;
	add.s64 	%rd325, %rd43, %rd274;

$L__BB5_47:
	// begin inline asm
	ld.global.nc.u32 %r407, [%rd325];
	// end inline asm
	mov.b32 	%f93, %r407;
	cvt.f64.f32 	%fd317, %f93;
	add.f64 	%fd318, %fd439, %fd317;
	add.s64 	%rd276, %rd325, 1024;
	// begin inline asm
	ld.global.nc.u32 %r408, [%rd276];
	// end inline asm
	mov.b32 	%f94, %r408;
	cvt.f64.f32 	%fd319, %f94;
	add.f64 	%fd320, %fd318, %fd319;
	add.s64 	%rd277, %rd325, 2048;
	// begin inline asm
	ld.global.nc.u32 %r409, [%rd277];
	// end inline asm
	mov.b32 	%f95, %r409;
	cvt.f64.f32 	%fd321, %f95;
	add.f64 	%fd322, %fd320, %fd321;
	add.s64 	%rd278, %rd325, 3072;
	// begin inline asm
	ld.global.nc.u32 %r410, [%rd278];
	// end inline asm
	mov.b32 	%f96, %r410;
	cvt.f64.f32 	%fd323, %f96;
	add.f64 	%fd439, %fd322, %fd323;
	add.s64 	%rd325, %rd325, 4096;
	add.s32 	%r628, %r628, 1024;
	setp.lt.s32 	%p55, %r628, %r59;
	@%p55 bra 	$L__BB5_47;

$L__BB5_48:
	// begin inline asm
	mov.u32 %r411, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r412, %laneid;
	// end inline asm
	mov.b64 	%rd279, %fd439;
	mov.u32 	%r420, 1;
	mov.u32 	%r461, 31;
	mov.u32 	%r462, -1;
	mov.b64 	{%r414, %r419}, %rd279;
	// begin inline asm
	shfl.sync.down.b32 %r413, %r414, %r420, %r461, %r462;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r418, %r419, %r420, %r461, %r462;
	// end inline asm
	mov.b64 	%rd280, {%r413, %r418};
	mov.b64 	%fd324, %rd280;
	setp.gt.s32 	%p56, %r412, 30;
	add.f64 	%fd325, %fd439, %fd324;
	selp.f64 	%fd326, %fd439, %fd325, %p56;
	mov.b64 	%rd281, %fd326;
	mov.u32 	%r430, 2;
	mov.b64 	{%r424, %r429}, %rd281;
	// begin inline asm
	shfl.sync.down.b32 %r423, %r424, %r430, %r461, %r462;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r428, %r429, %r430, %r461, %r462;
	// end inline asm
	mov.b64 	%rd282, {%r423, %r428};
	mov.b64 	%fd327, %rd282;
	setp.gt.s32 	%p57, %r412, 29;
	add.f64 	%fd328, %fd326, %fd327;
	selp.f64 	%fd329, %fd326, %fd328, %p57;
	mov.b64 	%rd283, %fd329;
	mov.u32 	%r440, 4;
	mov.b64 	{%r434, %r439}, %rd283;
	// begin inline asm
	shfl.sync.down.b32 %r433, %r434, %r440, %r461, %r462;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r438, %r439, %r440, %r461, %r462;
	// end inline asm
	mov.b64 	%rd284, {%r433, %r438};
	mov.b64 	%fd330, %rd284;
	setp.gt.s32 	%p58, %r412, 27;
	add.f64 	%fd331, %fd329, %fd330;
	selp.f64 	%fd332, %fd329, %fd331, %p58;
	mov.b64 	%rd285, %fd332;
	mov.u32 	%r450, 8;
	mov.b64 	{%r444, %r449}, %rd285;
	// begin inline asm
	shfl.sync.down.b32 %r443, %r444, %r450, %r461, %r462;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r448, %r449, %r450, %r461, %r462;
	// end inline asm
	mov.b64 	%rd286, {%r443, %r448};
	mov.b64 	%fd333, %rd286;
	setp.gt.s32 	%p59, %r412, 23;
	add.f64 	%fd334, %fd332, %fd333;
	selp.f64 	%fd335, %fd332, %fd334, %p59;
	mov.b64 	%rd287, %fd335;
	mov.u32 	%r460, 16;
	mov.b64 	{%r454, %r459}, %rd287;
	// begin inline asm
	shfl.sync.down.b32 %r453, %r454, %r460, %r461, %r462;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r458, %r459, %r460, %r461, %r462;
	// end inline asm
	mov.b64 	%rd288, {%r453, %r458};
	mov.b64 	%fd336, %rd288;
	setp.gt.s32 	%p60, %r412, 15;
	add.f64 	%fd337, %fd335, %fd336;
	selp.f64 	%fd446, %fd335, %fd337, %p60;
	setp.ne.s32 	%p61, %r411, 0;
	@%p61 bra 	$L__BB5_50;

	shr.s32 	%r464, %r355, 31;
	shr.u32 	%r465, %r464, 27;
	add.s32 	%r466, %r355, %r465;
	shr.s32 	%r467, %r466, 5;
	shl.b32 	%r468, %r467, 3;
	mov.u32 	%r469, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r470, %r469, %r468;
	st.shared.f64 	[%r470+8], %fd446;

$L__BB5_50:
	bar.sync 	0;
	setp.ne.s32 	%p62, %r355, 0;
	@%p62 bra 	$L__BB5_71;

	ld.shared.f64 	%fd338, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd339, %fd446, %fd338;
	ld.shared.f64 	%fd340, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd341, %fd339, %fd340;
	ld.shared.f64 	%fd342, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd343, %fd341, %fd342;
	ld.shared.f64 	%fd344, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd345, %fd343, %fd344;
	ld.shared.f64 	%fd346, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd347, %fd345, %fd346;
	ld.shared.f64 	%fd348, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd349, %fd347, %fd348;
	ld.shared.f64 	%fd350, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd349, %fd350;
	bra.uni 	$L__BB5_71;

$L__BB5_27:
	setp.ne.s32 	%p25, %r233, 0;
	shl.b32 	%r292, %r50, 5;
	add.s32 	%r293, %r292, 32;
	setp.gt.s32 	%p26, %r293, %r37;
	// begin inline asm
	mov.u32 %r241, %laneid;
	// end inline asm
	not.b32 	%r294, %r292;
	mov.u32 	%r291, -1;
	add.s32 	%r295, %r37, %r294;
	selp.b32 	%r290, %r295, 31, %p26;
	mov.u32 	%r249, 1;
	// begin inline asm
	shfl.sync.down.b32 %r242, %r52, %r249, %r290, %r291;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r247, %r53, %r249, %r290, %r291;
	// end inline asm
	mov.b64 	%rd218, {%r242, %r247};
	mov.b64 	%fd179, %rd218;
	setp.lt.s32 	%p27, %r241, %r290;
	add.f64 	%fd180, %fd432, %fd179;
	selp.f64 	%fd181, %fd180, %fd432, %p27;
	mov.b64 	%rd219, %fd181;
	mov.u32 	%r259, 2;
	mov.b64 	{%r253, %r258}, %rd219;
	// begin inline asm
	shfl.sync.down.b32 %r252, %r253, %r259, %r290, %r291;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r257, %r258, %r259, %r290, %r291;
	// end inline asm
	mov.b64 	%rd220, {%r252, %r257};
	mov.b64 	%fd182, %rd220;
	add.s32 	%r296, %r241, 2;
	setp.gt.s32 	%p28, %r296, %r290;
	add.f64 	%fd183, %fd181, %fd182;
	selp.f64 	%fd184, %fd181, %fd183, %p28;
	mov.b64 	%rd221, %fd184;
	mov.u32 	%r269, 4;
	mov.b64 	{%r263, %r268}, %rd221;
	// begin inline asm
	shfl.sync.down.b32 %r262, %r263, %r269, %r290, %r291;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r267, %r268, %r269, %r290, %r291;
	// end inline asm
	mov.b64 	%rd222, {%r262, %r267};
	mov.b64 	%fd185, %rd222;
	add.s32 	%r297, %r241, 4;
	setp.gt.s32 	%p29, %r297, %r290;
	add.f64 	%fd186, %fd184, %fd185;
	selp.f64 	%fd187, %fd184, %fd186, %p29;
	mov.b64 	%rd223, %fd187;
	mov.u32 	%r279, 8;
	mov.b64 	{%r273, %r278}, %rd223;
	// begin inline asm
	shfl.sync.down.b32 %r272, %r273, %r279, %r290, %r291;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r277, %r278, %r279, %r290, %r291;
	// end inline asm
	mov.b64 	%rd224, {%r272, %r277};
	mov.b64 	%fd188, %rd224;
	add.s32 	%r298, %r241, 8;
	setp.gt.s32 	%p30, %r298, %r290;
	add.f64 	%fd189, %fd187, %fd188;
	selp.f64 	%fd190, %fd187, %fd189, %p30;
	mov.b64 	%rd225, %fd190;
	mov.u32 	%r289, 16;
	mov.b64 	{%r283, %r288}, %rd225;
	// begin inline asm
	shfl.sync.down.b32 %r282, %r283, %r289, %r290, %r291;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r287, %r288, %r289, %r290, %r291;
	// end inline asm
	mov.b64 	%rd226, {%r282, %r287};
	mov.b64 	%fd191, %rd226;
	add.s32 	%r299, %r241, 16;
	setp.gt.s32 	%p31, %r299, %r290;
	add.f64 	%fd192, %fd190, %fd191;
	selp.f64 	%fd446, %fd190, %fd192, %p31;
	@%p25 bra 	$L__BB5_29;

	add.s32 	%r609, %r240, 8;
	st.shared.f64 	[%r609], %fd446;

$L__BB5_29:
	bar.sync 	0;
	setp.ne.s32 	%p32, %r234, 0;
	@%p32 bra 	$L__BB5_71;

	setp.gt.s32 	%p33, %r37, 32;
	ld.shared.f64 	%fd193, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd194, %fd446, %fd193;
	selp.f64 	%fd195, %fd194, %fd446, %p33;
	ld.shared.f64 	%fd196, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd197, %fd195, %fd196;
	setp.gt.s32 	%p34, %r37, 64;
	selp.f64 	%fd198, %fd197, %fd195, %p34;
	ld.shared.f64 	%fd199, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd200, %fd198, %fd199;
	setp.gt.s32 	%p35, %r37, 96;
	selp.f64 	%fd201, %fd200, %fd198, %p35;
	ld.shared.f64 	%fd202, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd203, %fd201, %fd202;
	setp.gt.s32 	%p36, %r37, 128;
	selp.f64 	%fd204, %fd203, %fd201, %p36;
	ld.shared.f64 	%fd205, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd206, %fd204, %fd205;
	setp.gt.s32 	%p37, %r37, 160;
	selp.f64 	%fd207, %fd206, %fd204, %p37;
	ld.shared.f64 	%fd208, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd209, %fd207, %fd208;
	setp.gt.s32 	%p38, %r37, 192;
	selp.f64 	%fd446, %fd209, %fd207, %p38;
	setp.lt.s32 	%p39, %r37, 225;
	@%p39 bra 	$L__BB5_71;

	ld.shared.f64 	%fd210, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd446, %fd210;
	bra.uni 	$L__BB5_71;

$L__BB5_62:
	setp.ne.s32 	%p70, %r487, 0;
	shl.b32 	%r546, %r84, 5;
	add.s32 	%r547, %r546, 32;
	setp.gt.s32 	%p71, %r547, %r71;
	// begin inline asm
	mov.u32 %r495, %laneid;
	// end inline asm
	not.b32 	%r548, %r546;
	mov.u32 	%r545, -1;
	add.s32 	%r549, %r71, %r548;
	selp.b32 	%r544, %r549, 31, %p71;
	mov.u32 	%r503, 1;
	// begin inline asm
	shfl.sync.down.b32 %r496, %r86, %r503, %r544, %r545;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r501, %r87, %r503, %r544, %r545;
	// end inline asm
	mov.b64 	%rd299, {%r496, %r501};
	mov.b64 	%fd361, %rd299;
	setp.lt.s32 	%p72, %r495, %r544;
	add.f64 	%fd362, %fd445, %fd361;
	selp.f64 	%fd363, %fd362, %fd445, %p72;
	mov.b64 	%rd300, %fd363;
	mov.u32 	%r513, 2;
	mov.b64 	{%r507, %r512}, %rd300;
	// begin inline asm
	shfl.sync.down.b32 %r506, %r507, %r513, %r544, %r545;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r511, %r512, %r513, %r544, %r545;
	// end inline asm
	mov.b64 	%rd301, {%r506, %r511};
	mov.b64 	%fd364, %rd301;
	add.s32 	%r550, %r495, 2;
	setp.gt.s32 	%p73, %r550, %r544;
	add.f64 	%fd365, %fd363, %fd364;
	selp.f64 	%fd366, %fd363, %fd365, %p73;
	mov.b64 	%rd302, %fd366;
	mov.u32 	%r523, 4;
	mov.b64 	{%r517, %r522}, %rd302;
	// begin inline asm
	shfl.sync.down.b32 %r516, %r517, %r523, %r544, %r545;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r521, %r522, %r523, %r544, %r545;
	// end inline asm
	mov.b64 	%rd303, {%r516, %r521};
	mov.b64 	%fd367, %rd303;
	add.s32 	%r551, %r495, 4;
	setp.gt.s32 	%p74, %r551, %r544;
	add.f64 	%fd368, %fd366, %fd367;
	selp.f64 	%fd369, %fd366, %fd368, %p74;
	mov.b64 	%rd304, %fd369;
	mov.u32 	%r533, 8;
	mov.b64 	{%r527, %r532}, %rd304;
	// begin inline asm
	shfl.sync.down.b32 %r526, %r527, %r533, %r544, %r545;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r531, %r532, %r533, %r544, %r545;
	// end inline asm
	mov.b64 	%rd305, {%r526, %r531};
	mov.b64 	%fd370, %rd305;
	add.s32 	%r552, %r495, 8;
	setp.gt.s32 	%p75, %r552, %r544;
	add.f64 	%fd371, %fd369, %fd370;
	selp.f64 	%fd372, %fd369, %fd371, %p75;
	mov.b64 	%rd306, %fd372;
	mov.u32 	%r543, 16;
	mov.b64 	{%r537, %r542}, %rd306;
	// begin inline asm
	shfl.sync.down.b32 %r536, %r537, %r543, %r544, %r545;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r541, %r542, %r543, %r544, %r545;
	// end inline asm
	mov.b64 	%rd307, {%r536, %r541};
	mov.b64 	%fd373, %rd307;
	add.s32 	%r553, %r495, 16;
	setp.gt.s32 	%p76, %r553, %r544;
	add.f64 	%fd374, %fd372, %fd373;
	selp.f64 	%fd446, %fd372, %fd374, %p76;
	@%p70 bra 	$L__BB5_64;

	add.s32 	%r611, %r494, 8;
	st.shared.f64 	[%r611], %fd446;

$L__BB5_64:
	bar.sync 	0;
	setp.ne.s32 	%p77, %r488, 0;
	@%p77 bra 	$L__BB5_71;

	setp.gt.s32 	%p78, %r71, 32;
	ld.shared.f64 	%fd375, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd376, %fd446, %fd375;
	selp.f64 	%fd377, %fd376, %fd446, %p78;
	ld.shared.f64 	%fd378, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd379, %fd377, %fd378;
	setp.gt.s32 	%p79, %r71, 64;
	selp.f64 	%fd380, %fd379, %fd377, %p79;
	ld.shared.f64 	%fd381, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd382, %fd380, %fd381;
	setp.gt.s32 	%p80, %r71, 96;
	selp.f64 	%fd383, %fd382, %fd380, %p80;
	ld.shared.f64 	%fd384, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd385, %fd383, %fd384;
	setp.gt.s32 	%p81, %r71, 128;
	selp.f64 	%fd386, %fd385, %fd383, %p81;
	ld.shared.f64 	%fd387, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd388, %fd386, %fd387;
	setp.gt.s32 	%p82, %r71, 160;
	selp.f64 	%fd389, %fd388, %fd386, %p82;
	ld.shared.f64 	%fd390, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd391, %fd389, %fd390;
	setp.gt.s32 	%p83, %r71, 192;
	selp.f64 	%fd446, %fd391, %fd389, %p83;
	setp.lt.s32 	%p84, %r71, 225;
	@%p84 bra 	$L__BB5_71;

	ld.shared.f64 	%fd392, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPfPdiS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd446, %fd392;

$L__BB5_71:
	mov.u32 	%r607, %tid.x;
	setp.ne.s32 	%p92, %r607, 0;
	@%p92 bra 	$L__BB5_73;

	cvta.to.global.u64 	%rd317, %rd44;
	mul.wide.u32 	%rd318, %r90, 8;
	add.s64 	%rd319, %rd317, %rd318;
	st.global.f64 	[%rd319], %fd446;

$L__BB5_73:
	ret;

}
	// .globl	_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_
.visible .entry _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_(
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_0,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_1,
	.param .u32 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_2,
	.param .align 1 .b8 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_3[1],
	.param .f64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_4
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<98>;
	.reg .b32 	%r<516>;
	.reg .f64 	%fd<616>;
	.reg .b64 	%rd<422>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage[80];

	ld.param.u64 	%rd29, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_0];
	ld.param.u64 	%rd30, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_1];
	ld.param.u32 	%r81, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_2];
	ld.param.f64 	%fd60, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_4];
	setp.eq.s32 	%p1, %r81, 0;
	@%p1 bra 	$L__BB6_78;

	and.b64  	%rd31, %rd29, 31;
	setp.eq.s64 	%p2, %rd31, 0;
	@%p2 bra 	$L__BB6_37;

	setp.lt.s32 	%p3, %r81, 5120;
	@%p3 bra 	$L__BB6_18;
	bra.uni 	$L__BB6_3;

$L__BB6_18:
	mov.u32 	%r495, %tid.x;
	setp.ge.s32 	%p19, %r495, %r81;
	@%p19 bra 	$L__BB6_20;

	mov.u32 	%r154, %tid.x;
	mul.wide.s32 	%rd137, %r154, 8;
	add.s64 	%rd136, %rd29, %rd137;
	// begin inline asm
	ld.global.nc.u64 %rd135, [%rd136];
	// end inline asm
	mov.b64 	%fd598, %rd135;
	add.s32 	%r495, %r154, 256;

$L__BB6_20:
	setp.ge.s32 	%p20, %r495, %r81;
	@%p20 bra 	$L__BB6_27;

	not.b32 	%r155, %r495;
	add.s32 	%r19, %r155, %r81;
	shr.u32 	%r156, %r19, 8;
	add.s32 	%r157, %r156, 1;
	and.b32  	%r494, %r157, 3;
	setp.eq.s32 	%p21, %r494, 0;
	@%p21 bra 	$L__BB6_24;

	mul.wide.s32 	%rd138, %r495, 8;
	add.s64 	%rd416, %rd29, %rd138;

$L__BB6_23:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd139, [%rd416];
	// end inline asm
	mov.b64 	%fd176, %rd139;
	add.f64 	%fd598, %fd598, %fd176;
	add.s32 	%r495, %r495, 256;
	add.s64 	%rd416, %rd416, 2048;
	add.s32 	%r494, %r494, -1;
	setp.ne.s32 	%p22, %r494, 0;
	@%p22 bra 	$L__BB6_23;

$L__BB6_24:
	setp.lt.u32 	%p23, %r19, 768;
	@%p23 bra 	$L__BB6_27;

	mul.wide.s32 	%rd141, %r495, 8;
	add.s64 	%rd417, %rd29, %rd141;

$L__BB6_26:
	// begin inline asm
	ld.global.nc.u64 %rd142, [%rd417];
	// end inline asm
	mov.b64 	%fd177, %rd142;
	add.f64 	%fd178, %fd598, %fd177;
	add.s64 	%rd145, %rd417, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd144, [%rd145];
	// end inline asm
	mov.b64 	%fd179, %rd144;
	add.f64 	%fd180, %fd178, %fd179;
	add.s64 	%rd147, %rd417, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd146, [%rd147];
	// end inline asm
	mov.b64 	%fd181, %rd146;
	add.f64 	%fd182, %fd180, %fd181;
	add.s64 	%rd149, %rd417, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd148, [%rd149];
	// end inline asm
	mov.b64 	%fd183, %rd148;
	add.f64 	%fd598, %fd182, %fd183;
	add.s64 	%rd417, %rd417, 8192;
	add.s32 	%r495, %r495, 1024;
	setp.lt.s32 	%p24, %r495, %r81;
	@%p24 bra 	$L__BB6_26;

$L__BB6_27:
	mov.u32 	%r159, %tid.x;
	shr.s32 	%r160, %r159, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r159, %r161;
	shr.s32 	%r28, %r162, 5;
	// begin inline asm
	mov.u32 %r158, %laneid;
	// end inline asm
	mov.b64 	%rd150, %fd598;
	mov.b64 	{%r30, %r31}, %rd150;
	shl.b32 	%r163, %r28, 3;
	mov.u32 	%r164, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r165, %r164, %r163;
	setp.gt.s32 	%p25, %r81, 255;
	@%p25 bra 	$L__BB6_33;
	bra.uni 	$L__BB6_28;

$L__BB6_33:
	setp.ne.s32 	%p41, %r158, 0;
	// begin inline asm
	mov.u32 %r226, %laneid;
	// end inline asm
	mov.u32 	%r234, 1;
	mov.u32 	%r275, 31;
	mov.u32 	%r276, -1;
	// begin inline asm
	shfl.sync.down.b32 %r227, %r30, %r234, %r275, %r276;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r232, %r31, %r234, %r275, %r276;
	// end inline asm
	mov.b64 	%rd160, {%r227, %r232};
	mov.b64 	%fd216, %rd160;
	setp.gt.s32 	%p42, %r226, 30;
	add.f64 	%fd217, %fd598, %fd216;
	selp.f64 	%fd218, %fd598, %fd217, %p42;
	mov.b64 	%rd161, %fd218;
	mov.u32 	%r244, 2;
	mov.b64 	{%r238, %r243}, %rd161;
	// begin inline asm
	shfl.sync.down.b32 %r237, %r238, %r244, %r275, %r276;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r242, %r243, %r244, %r275, %r276;
	// end inline asm
	mov.b64 	%rd162, {%r237, %r242};
	mov.b64 	%fd219, %rd162;
	setp.gt.s32 	%p43, %r226, 29;
	add.f64 	%fd220, %fd218, %fd219;
	selp.f64 	%fd221, %fd218, %fd220, %p43;
	mov.b64 	%rd163, %fd221;
	mov.u32 	%r254, 4;
	mov.b64 	{%r248, %r253}, %rd163;
	// begin inline asm
	shfl.sync.down.b32 %r247, %r248, %r254, %r275, %r276;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r252, %r253, %r254, %r275, %r276;
	// end inline asm
	mov.b64 	%rd164, {%r247, %r252};
	mov.b64 	%fd222, %rd164;
	setp.gt.s32 	%p44, %r226, 27;
	add.f64 	%fd223, %fd221, %fd222;
	selp.f64 	%fd224, %fd221, %fd223, %p44;
	mov.b64 	%rd165, %fd224;
	mov.u32 	%r264, 8;
	mov.b64 	{%r258, %r263}, %rd165;
	// begin inline asm
	shfl.sync.down.b32 %r257, %r258, %r264, %r275, %r276;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r262, %r263, %r264, %r275, %r276;
	// end inline asm
	mov.b64 	%rd166, {%r257, %r262};
	mov.b64 	%fd225, %rd166;
	setp.gt.s32 	%p45, %r226, 23;
	add.f64 	%fd226, %fd224, %fd225;
	selp.f64 	%fd227, %fd224, %fd226, %p45;
	mov.b64 	%rd167, %fd227;
	mov.u32 	%r274, 16;
	mov.b64 	{%r268, %r273}, %rd167;
	// begin inline asm
	shfl.sync.down.b32 %r267, %r268, %r274, %r275, %r276;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r272, %r273, %r274, %r275, %r276;
	// end inline asm
	mov.b64 	%rd168, {%r267, %r272};
	mov.b64 	%fd228, %rd168;
	setp.gt.s32 	%p46, %r226, 15;
	add.f64 	%fd229, %fd227, %fd228;
	selp.f64 	%fd615, %fd227, %fd229, %p46;
	@%p41 bra 	$L__BB6_35;

	add.s32 	%r483, %r165, 8;
	st.shared.f64 	[%r483], %fd615;

$L__BB6_35:
	bar.sync 	0;
	setp.ne.s32 	%p47, %r159, 0;
	@%p47 bra 	$L__BB6_76;

	ld.shared.f64 	%fd230, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd231, %fd615, %fd230;
	ld.shared.f64 	%fd232, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd233, %fd231, %fd232;
	ld.shared.f64 	%fd234, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd235, %fd233, %fd234;
	ld.shared.f64 	%fd236, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd237, %fd235, %fd236;
	ld.shared.f64 	%fd238, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd239, %fd237, %fd238;
	ld.shared.f64 	%fd240, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd241, %fd239, %fd240;
	ld.shared.f64 	%fd242, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd241, %fd242;
	bra.uni 	$L__BB6_76;

$L__BB6_78:
	mov.u32 	%r481, %tid.x;
	setp.ne.s32 	%p97, %r481, 0;
	@%p97 bra 	$L__BB6_80;

	cvta.to.global.u64 	%rd412, %rd30;
	st.global.f64 	[%rd412], %fd60;
	bra.uni 	$L__BB6_80;

$L__BB6_37:
	setp.lt.s32 	%p48, %r81, 5120;
	@%p48 bra 	$L__BB6_57;
	bra.uni 	$L__BB6_38;

$L__BB6_57:
	mov.u32 	%r514, %tid.x;
	setp.ge.s32 	%p67, %r514, %r81;
	@%p67 bra 	$L__BB6_59;

	mov.u32 	%r356, %tid.x;
	mul.wide.s32 	%rd379, %r356, 8;
	add.s64 	%rd378, %rd29, %rd379;
	// begin inline asm
	ld.global.nc.u64 %rd377, [%rd378];
	// end inline asm
	mov.b64 	%fd614, %rd377;
	add.s32 	%r514, %r356, 256;

$L__BB6_59:
	setp.ge.s32 	%p68, %r514, %r81;
	@%p68 bra 	$L__BB6_66;

	not.b32 	%r357, %r514;
	add.s32 	%r67, %r357, %r81;
	shr.u32 	%r358, %r67, 8;
	add.s32 	%r359, %r358, 1;
	and.b32  	%r513, %r359, 3;
	setp.eq.s32 	%p69, %r513, 0;
	@%p69 bra 	$L__BB6_63;

	mul.wide.s32 	%rd380, %r514, 8;
	add.s64 	%rd420, %rd29, %rd380;

$L__BB6_62:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd381, [%rd420];
	// end inline asm
	mov.b64 	%fd518, %rd381;
	add.f64 	%fd614, %fd614, %fd518;
	add.s32 	%r514, %r514, 256;
	add.s64 	%rd420, %rd420, 2048;
	add.s32 	%r513, %r513, -1;
	setp.ne.s32 	%p70, %r513, 0;
	@%p70 bra 	$L__BB6_62;

$L__BB6_63:
	setp.lt.u32 	%p71, %r67, 768;
	@%p71 bra 	$L__BB6_66;

	mul.wide.s32 	%rd383, %r514, 8;
	add.s64 	%rd421, %rd29, %rd383;

$L__BB6_65:
	// begin inline asm
	ld.global.nc.u64 %rd384, [%rd421];
	// end inline asm
	mov.b64 	%fd519, %rd384;
	add.f64 	%fd520, %fd614, %fd519;
	add.s64 	%rd387, %rd421, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd386, [%rd387];
	// end inline asm
	mov.b64 	%fd521, %rd386;
	add.f64 	%fd522, %fd520, %fd521;
	add.s64 	%rd389, %rd421, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd388, [%rd389];
	// end inline asm
	mov.b64 	%fd523, %rd388;
	add.f64 	%fd524, %fd522, %fd523;
	add.s64 	%rd391, %rd421, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd390, [%rd391];
	// end inline asm
	mov.b64 	%fd525, %rd390;
	add.f64 	%fd614, %fd524, %fd525;
	add.s64 	%rd421, %rd421, 8192;
	add.s32 	%r514, %r514, 1024;
	setp.lt.s32 	%p72, %r514, %r81;
	@%p72 bra 	$L__BB6_65;

$L__BB6_66:
	mov.u32 	%r361, %tid.x;
	shr.s32 	%r362, %r361, 31;
	shr.u32 	%r363, %r362, 27;
	add.s32 	%r364, %r361, %r363;
	shr.s32 	%r76, %r364, 5;
	// begin inline asm
	mov.u32 %r360, %laneid;
	// end inline asm
	mov.b64 	%rd392, %fd614;
	mov.b64 	{%r78, %r79}, %rd392;
	shl.b32 	%r365, %r76, 3;
	mov.u32 	%r366, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r367, %r366, %r365;
	setp.gt.s32 	%p73, %r81, 255;
	@%p73 bra 	$L__BB6_72;
	bra.uni 	$L__BB6_67;

$L__BB6_72:
	setp.ne.s32 	%p89, %r360, 0;
	// begin inline asm
	mov.u32 %r428, %laneid;
	// end inline asm
	mov.u32 	%r436, 1;
	mov.u32 	%r477, 31;
	mov.u32 	%r478, -1;
	// begin inline asm
	shfl.sync.down.b32 %r429, %r78, %r436, %r477, %r478;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r434, %r79, %r436, %r477, %r478;
	// end inline asm
	mov.b64 	%rd402, {%r429, %r434};
	mov.b64 	%fd558, %rd402;
	setp.gt.s32 	%p90, %r428, 30;
	add.f64 	%fd559, %fd614, %fd558;
	selp.f64 	%fd560, %fd614, %fd559, %p90;
	mov.b64 	%rd403, %fd560;
	mov.u32 	%r446, 2;
	mov.b64 	{%r440, %r445}, %rd403;
	// begin inline asm
	shfl.sync.down.b32 %r439, %r440, %r446, %r477, %r478;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r444, %r445, %r446, %r477, %r478;
	// end inline asm
	mov.b64 	%rd404, {%r439, %r444};
	mov.b64 	%fd561, %rd404;
	setp.gt.s32 	%p91, %r428, 29;
	add.f64 	%fd562, %fd560, %fd561;
	selp.f64 	%fd563, %fd560, %fd562, %p91;
	mov.b64 	%rd405, %fd563;
	mov.u32 	%r456, 4;
	mov.b64 	{%r450, %r455}, %rd405;
	// begin inline asm
	shfl.sync.down.b32 %r449, %r450, %r456, %r477, %r478;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r454, %r455, %r456, %r477, %r478;
	// end inline asm
	mov.b64 	%rd406, {%r449, %r454};
	mov.b64 	%fd564, %rd406;
	setp.gt.s32 	%p92, %r428, 27;
	add.f64 	%fd565, %fd563, %fd564;
	selp.f64 	%fd566, %fd563, %fd565, %p92;
	mov.b64 	%rd407, %fd566;
	mov.u32 	%r466, 8;
	mov.b64 	{%r460, %r465}, %rd407;
	// begin inline asm
	shfl.sync.down.b32 %r459, %r460, %r466, %r477, %r478;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r464, %r465, %r466, %r477, %r478;
	// end inline asm
	mov.b64 	%rd408, {%r459, %r464};
	mov.b64 	%fd567, %rd408;
	setp.gt.s32 	%p93, %r428, 23;
	add.f64 	%fd568, %fd566, %fd567;
	selp.f64 	%fd569, %fd566, %fd568, %p93;
	mov.b64 	%rd409, %fd569;
	mov.u32 	%r476, 16;
	mov.b64 	{%r470, %r475}, %rd409;
	// begin inline asm
	shfl.sync.down.b32 %r469, %r470, %r476, %r477, %r478;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r474, %r475, %r476, %r477, %r478;
	// end inline asm
	mov.b64 	%rd410, {%r469, %r474};
	mov.b64 	%fd570, %rd410;
	setp.gt.s32 	%p94, %r428, 15;
	add.f64 	%fd571, %fd569, %fd570;
	selp.f64 	%fd615, %fd569, %fd571, %p94;
	@%p89 bra 	$L__BB6_74;

	add.s32 	%r485, %r367, 8;
	st.shared.f64 	[%r485], %fd615;

$L__BB6_74:
	bar.sync 	0;
	setp.ne.s32 	%p95, %r361, 0;
	@%p95 bra 	$L__BB6_76;

	ld.shared.f64 	%fd572, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd573, %fd615, %fd572;
	ld.shared.f64 	%fd574, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd575, %fd573, %fd574;
	ld.shared.f64 	%fd576, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd577, %fd575, %fd576;
	ld.shared.f64 	%fd578, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd579, %fd577, %fd578;
	ld.shared.f64 	%fd580, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd581, %fd579, %fd580;
	ld.shared.f64 	%fd582, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd583, %fd581, %fd582;
	ld.shared.f64 	%fd584, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd583, %fd584;
	bra.uni 	$L__BB6_76;

$L__BB6_3:
	mov.u32 	%r83, %tid.x;
	mul.wide.s32 	%rd72, %r83, 8;
	add.s64 	%rd413, %rd29, %rd72;
	// begin inline asm
	ld.global.nc.u64 %rd32, [%rd413];
	// end inline asm
	add.s64 	%rd35, %rd413, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd34, [%rd35];
	// end inline asm
	add.s64 	%rd37, %rd413, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd36, [%rd37];
	// end inline asm
	add.s64 	%rd39, %rd413, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd38, [%rd39];
	// end inline asm
	add.s64 	%rd41, %rd413, 8192;
	// begin inline asm
	ld.global.nc.u64 %rd40, [%rd41];
	// end inline asm
	add.s64 	%rd43, %rd413, 10240;
	// begin inline asm
	ld.global.nc.u64 %rd42, [%rd43];
	// end inline asm
	add.s64 	%rd45, %rd413, 12288;
	// begin inline asm
	ld.global.nc.u64 %rd44, [%rd45];
	// end inline asm
	add.s64 	%rd47, %rd413, 14336;
	// begin inline asm
	ld.global.nc.u64 %rd46, [%rd47];
	// end inline asm
	add.s64 	%rd49, %rd413, 16384;
	// begin inline asm
	ld.global.nc.u64 %rd48, [%rd49];
	// end inline asm
	add.s64 	%rd51, %rd413, 18432;
	// begin inline asm
	ld.global.nc.u64 %rd50, [%rd51];
	// end inline asm
	add.s64 	%rd53, %rd413, 20480;
	// begin inline asm
	ld.global.nc.u64 %rd52, [%rd53];
	// end inline asm
	add.s64 	%rd55, %rd413, 22528;
	// begin inline asm
	ld.global.nc.u64 %rd54, [%rd55];
	// end inline asm
	add.s64 	%rd57, %rd413, 24576;
	// begin inline asm
	ld.global.nc.u64 %rd56, [%rd57];
	// end inline asm
	add.s64 	%rd59, %rd413, 26624;
	// begin inline asm
	ld.global.nc.u64 %rd58, [%rd59];
	// end inline asm
	add.s64 	%rd61, %rd413, 28672;
	// begin inline asm
	ld.global.nc.u64 %rd60, [%rd61];
	// end inline asm
	add.s64 	%rd63, %rd413, 30720;
	// begin inline asm
	ld.global.nc.u64 %rd62, [%rd63];
	// end inline asm
	add.s64 	%rd65, %rd413, 32768;
	// begin inline asm
	ld.global.nc.u64 %rd64, [%rd65];
	// end inline asm
	add.s64 	%rd67, %rd413, 34816;
	// begin inline asm
	ld.global.nc.u64 %rd66, [%rd67];
	// end inline asm
	add.s64 	%rd69, %rd413, 36864;
	// begin inline asm
	ld.global.nc.u64 %rd68, [%rd69];
	// end inline asm
	add.s64 	%rd71, %rd413, 38912;
	// begin inline asm
	ld.global.nc.u64 %rd70, [%rd71];
	// end inline asm
	mov.b64 	%fd61, %rd32;
	mov.b64 	%fd62, %rd34;
	add.f64 	%fd63, %fd61, %fd62;
	mov.b64 	%fd64, %rd36;
	add.f64 	%fd65, %fd63, %fd64;
	mov.b64 	%fd66, %rd38;
	add.f64 	%fd67, %fd65, %fd66;
	mov.b64 	%fd68, %rd40;
	add.f64 	%fd69, %fd67, %fd68;
	mov.b64 	%fd70, %rd42;
	add.f64 	%fd71, %fd69, %fd70;
	mov.b64 	%fd72, %rd44;
	add.f64 	%fd73, %fd71, %fd72;
	mov.b64 	%fd74, %rd46;
	add.f64 	%fd75, %fd73, %fd74;
	mov.b64 	%fd76, %rd48;
	add.f64 	%fd77, %fd75, %fd76;
	mov.b64 	%fd78, %rd50;
	add.f64 	%fd79, %fd77, %fd78;
	mov.b64 	%fd80, %rd52;
	add.f64 	%fd81, %fd79, %fd80;
	mov.b64 	%fd82, %rd54;
	add.f64 	%fd83, %fd81, %fd82;
	mov.b64 	%fd84, %rd56;
	add.f64 	%fd85, %fd83, %fd84;
	mov.b64 	%fd86, %rd58;
	add.f64 	%fd87, %fd85, %fd86;
	mov.b64 	%fd88, %rd60;
	add.f64 	%fd89, %fd87, %fd88;
	mov.b64 	%fd90, %rd62;
	add.f64 	%fd91, %fd89, %fd90;
	mov.b64 	%fd92, %rd64;
	add.f64 	%fd93, %fd91, %fd92;
	mov.b64 	%fd94, %rd66;
	add.f64 	%fd95, %fd93, %fd94;
	mov.b64 	%fd96, %rd68;
	add.f64 	%fd97, %fd95, %fd96;
	mov.b64 	%fd98, %rd70;
	add.f64 	%fd592, %fd97, %fd98;
	setp.lt.s32 	%p4, %r81, 10240;
	mov.u32 	%r487, 5120;
	@%p4 bra 	$L__BB6_6;

	mov.u32 	%r486, %r487;

$L__BB6_5:
	add.s64 	%rd74, %rd413, 40960;
	// begin inline asm
	ld.global.nc.u64 %rd73, [%rd74];
	// end inline asm
	add.s64 	%rd76, %rd413, 43008;
	// begin inline asm
	ld.global.nc.u64 %rd75, [%rd76];
	// end inline asm
	add.s64 	%rd78, %rd413, 45056;
	// begin inline asm
	ld.global.nc.u64 %rd77, [%rd78];
	// end inline asm
	add.s64 	%rd80, %rd413, 47104;
	// begin inline asm
	ld.global.nc.u64 %rd79, [%rd80];
	// end inline asm
	add.s64 	%rd82, %rd413, 49152;
	// begin inline asm
	ld.global.nc.u64 %rd81, [%rd82];
	// end inline asm
	add.s64 	%rd84, %rd413, 51200;
	// begin inline asm
	ld.global.nc.u64 %rd83, [%rd84];
	// end inline asm
	add.s64 	%rd86, %rd413, 53248;
	// begin inline asm
	ld.global.nc.u64 %rd85, [%rd86];
	// end inline asm
	add.s64 	%rd88, %rd413, 55296;
	// begin inline asm
	ld.global.nc.u64 %rd87, [%rd88];
	// end inline asm
	add.s64 	%rd90, %rd413, 57344;
	// begin inline asm
	ld.global.nc.u64 %rd89, [%rd90];
	// end inline asm
	add.s64 	%rd92, %rd413, 59392;
	// begin inline asm
	ld.global.nc.u64 %rd91, [%rd92];
	// end inline asm
	add.s64 	%rd94, %rd413, 61440;
	// begin inline asm
	ld.global.nc.u64 %rd93, [%rd94];
	// end inline asm
	add.s64 	%rd96, %rd413, 63488;
	// begin inline asm
	ld.global.nc.u64 %rd95, [%rd96];
	// end inline asm
	add.s64 	%rd98, %rd413, 65536;
	// begin inline asm
	ld.global.nc.u64 %rd97, [%rd98];
	// end inline asm
	add.s64 	%rd100, %rd413, 67584;
	// begin inline asm
	ld.global.nc.u64 %rd99, [%rd100];
	// end inline asm
	add.s64 	%rd102, %rd413, 69632;
	// begin inline asm
	ld.global.nc.u64 %rd101, [%rd102];
	// end inline asm
	add.s64 	%rd104, %rd413, 71680;
	// begin inline asm
	ld.global.nc.u64 %rd103, [%rd104];
	// end inline asm
	add.s64 	%rd106, %rd413, 73728;
	// begin inline asm
	ld.global.nc.u64 %rd105, [%rd106];
	// end inline asm
	add.s64 	%rd108, %rd413, 75776;
	// begin inline asm
	ld.global.nc.u64 %rd107, [%rd108];
	// end inline asm
	add.s64 	%rd110, %rd413, 77824;
	// begin inline asm
	ld.global.nc.u64 %rd109, [%rd110];
	// end inline asm
	add.s64 	%rd112, %rd413, 79872;
	// begin inline asm
	ld.global.nc.u64 %rd111, [%rd112];
	// end inline asm
	mov.b64 	%fd99, %rd73;
	add.f64 	%fd100, %fd592, %fd99;
	mov.b64 	%fd101, %rd75;
	add.f64 	%fd102, %fd100, %fd101;
	mov.b64 	%fd103, %rd77;
	add.f64 	%fd104, %fd102, %fd103;
	mov.b64 	%fd105, %rd79;
	add.f64 	%fd106, %fd104, %fd105;
	mov.b64 	%fd107, %rd81;
	add.f64 	%fd108, %fd106, %fd107;
	mov.b64 	%fd109, %rd83;
	add.f64 	%fd110, %fd108, %fd109;
	mov.b64 	%fd111, %rd85;
	add.f64 	%fd112, %fd110, %fd111;
	mov.b64 	%fd113, %rd87;
	add.f64 	%fd114, %fd112, %fd113;
	mov.b64 	%fd115, %rd89;
	add.f64 	%fd116, %fd114, %fd115;
	mov.b64 	%fd117, %rd91;
	add.f64 	%fd118, %fd116, %fd117;
	mov.b64 	%fd119, %rd93;
	add.f64 	%fd120, %fd118, %fd119;
	mov.b64 	%fd121, %rd95;
	add.f64 	%fd122, %fd120, %fd121;
	mov.b64 	%fd123, %rd97;
	add.f64 	%fd124, %fd122, %fd123;
	mov.b64 	%fd125, %rd99;
	add.f64 	%fd126, %fd124, %fd125;
	mov.b64 	%fd127, %rd101;
	add.f64 	%fd128, %fd126, %fd127;
	mov.b64 	%fd129, %rd103;
	add.f64 	%fd130, %fd128, %fd129;
	mov.b64 	%fd131, %rd105;
	add.f64 	%fd132, %fd130, %fd131;
	mov.b64 	%fd133, %rd107;
	add.f64 	%fd134, %fd132, %fd133;
	mov.b64 	%fd135, %rd109;
	add.f64 	%fd136, %fd134, %fd135;
	mov.b64 	%fd137, %rd111;
	add.f64 	%fd592, %fd136, %fd137;
	add.s32 	%r487, %r486, 5120;
	add.s32 	%r85, %r486, 10240;
	setp.le.s32 	%p5, %r85, %r81;
	mov.u64 	%rd413, %rd74;
	mov.u32 	%r486, %r487;
	@%p5 bra 	$L__BB6_5;

$L__BB6_6:
	setp.ge.s32 	%p6, %r487, %r81;
	@%p6 bra 	$L__BB6_14;

	sub.s32 	%r4, %r81, %r487;
	setp.ge.s32 	%p7, %r83, %r4;
	@%p7 bra 	$L__BB6_14;

	not.b32 	%r87, %r487;
	add.s32 	%r88, %r87, %r81;
	sub.s32 	%r6, %r88, %r83;
	shr.u32 	%r89, %r6, 8;
	add.s32 	%r90, %r89, 1;
	and.b32  	%r489, %r90, 3;
	setp.eq.s32 	%p8, %r489, 0;
	mov.u32 	%r490, %r83;
	@%p8 bra 	$L__BB6_11;

	mov.u32 	%r490, %tid.x;
	add.s32 	%r91, %r490, %r487;
	mul.wide.s32 	%rd113, %r91, 8;
	add.s64 	%rd414, %rd29, %rd113;

$L__BB6_10:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd114, [%rd414];
	// end inline asm
	mov.b64 	%fd139, %rd114;
	add.f64 	%fd592, %fd592, %fd139;
	add.s32 	%r490, %r490, 256;
	add.s64 	%rd414, %rd414, 2048;
	add.s32 	%r489, %r489, -1;
	setp.ne.s32 	%p9, %r489, 0;
	@%p9 bra 	$L__BB6_10;

$L__BB6_11:
	setp.lt.u32 	%p10, %r6, 768;
	@%p10 bra 	$L__BB6_14;

	add.s32 	%r92, %r490, %r487;
	mul.wide.s32 	%rd116, %r92, 8;
	add.s64 	%rd415, %rd29, %rd116;

$L__BB6_13:
	// begin inline asm
	ld.global.nc.u64 %rd117, [%rd415];
	// end inline asm
	mov.b64 	%fd140, %rd117;
	add.f64 	%fd141, %fd592, %fd140;
	add.s64 	%rd120, %rd415, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd119, [%rd120];
	// end inline asm
	mov.b64 	%fd142, %rd119;
	add.f64 	%fd143, %fd141, %fd142;
	add.s64 	%rd122, %rd415, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd121, [%rd122];
	// end inline asm
	mov.b64 	%fd144, %rd121;
	add.f64 	%fd145, %fd143, %fd144;
	add.s64 	%rd124, %rd415, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd123, [%rd124];
	// end inline asm
	mov.b64 	%fd146, %rd123;
	add.f64 	%fd592, %fd145, %fd146;
	add.s64 	%rd415, %rd415, 8192;
	add.s32 	%r490, %r490, 1024;
	setp.lt.s32 	%p11, %r490, %r4;
	@%p11 bra 	$L__BB6_13;

$L__BB6_14:
	// begin inline asm
	mov.u32 %r93, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r94, %laneid;
	// end inline asm
	mov.b64 	%rd125, %fd592;
	mov.u32 	%r102, 1;
	mov.u32 	%r143, 31;
	mov.u32 	%r144, -1;
	mov.b64 	{%r96, %r101}, %rd125;
	// begin inline asm
	shfl.sync.down.b32 %r95, %r96, %r102, %r143, %r144;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r100, %r101, %r102, %r143, %r144;
	// end inline asm
	mov.b64 	%rd126, {%r95, %r100};
	mov.b64 	%fd147, %rd126;
	setp.gt.s32 	%p12, %r94, 30;
	add.f64 	%fd148, %fd592, %fd147;
	selp.f64 	%fd149, %fd592, %fd148, %p12;
	mov.b64 	%rd127, %fd149;
	mov.u32 	%r112, 2;
	mov.b64 	{%r106, %r111}, %rd127;
	// begin inline asm
	shfl.sync.down.b32 %r105, %r106, %r112, %r143, %r144;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r110, %r111, %r112, %r143, %r144;
	// end inline asm
	mov.b64 	%rd128, {%r105, %r110};
	mov.b64 	%fd150, %rd128;
	setp.gt.s32 	%p13, %r94, 29;
	add.f64 	%fd151, %fd149, %fd150;
	selp.f64 	%fd152, %fd149, %fd151, %p13;
	mov.b64 	%rd129, %fd152;
	mov.u32 	%r122, 4;
	mov.b64 	{%r116, %r121}, %rd129;
	// begin inline asm
	shfl.sync.down.b32 %r115, %r116, %r122, %r143, %r144;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r120, %r121, %r122, %r143, %r144;
	// end inline asm
	mov.b64 	%rd130, {%r115, %r120};
	mov.b64 	%fd153, %rd130;
	setp.gt.s32 	%p14, %r94, 27;
	add.f64 	%fd154, %fd152, %fd153;
	selp.f64 	%fd155, %fd152, %fd154, %p14;
	mov.b64 	%rd131, %fd155;
	mov.u32 	%r132, 8;
	mov.b64 	{%r126, %r131}, %rd131;
	// begin inline asm
	shfl.sync.down.b32 %r125, %r126, %r132, %r143, %r144;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r130, %r131, %r132, %r143, %r144;
	// end inline asm
	mov.b64 	%rd132, {%r125, %r130};
	mov.b64 	%fd156, %rd132;
	setp.gt.s32 	%p15, %r94, 23;
	add.f64 	%fd157, %fd155, %fd156;
	selp.f64 	%fd158, %fd155, %fd157, %p15;
	mov.b64 	%rd133, %fd158;
	mov.u32 	%r142, 16;
	mov.b64 	{%r136, %r141}, %rd133;
	// begin inline asm
	shfl.sync.down.b32 %r135, %r136, %r142, %r143, %r144;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r140, %r141, %r142, %r143, %r144;
	// end inline asm
	mov.b64 	%rd134, {%r135, %r140};
	mov.b64 	%fd159, %rd134;
	setp.gt.s32 	%p16, %r94, 15;
	add.f64 	%fd160, %fd158, %fd159;
	selp.f64 	%fd615, %fd158, %fd160, %p16;
	setp.ne.s32 	%p17, %r93, 0;
	@%p17 bra 	$L__BB6_16;

	shr.s32 	%r146, %r83, 31;
	shr.u32 	%r147, %r146, 27;
	add.s32 	%r148, %r83, %r147;
	shr.s32 	%r149, %r148, 5;
	shl.b32 	%r150, %r149, 3;
	mov.u32 	%r151, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r152, %r151, %r150;
	st.shared.f64 	[%r152+8], %fd615;

$L__BB6_16:
	bar.sync 	0;
	setp.ne.s32 	%p18, %r83, 0;
	@%p18 bra 	$L__BB6_76;

	ld.shared.f64 	%fd161, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd162, %fd615, %fd161;
	ld.shared.f64 	%fd163, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd164, %fd162, %fd163;
	ld.shared.f64 	%fd165, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd166, %fd164, %fd165;
	ld.shared.f64 	%fd167, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd168, %fd166, %fd167;
	ld.shared.f64 	%fd169, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd170, %fd168, %fd169;
	ld.shared.f64 	%fd171, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd172, %fd170, %fd171;
	ld.shared.f64 	%fd173, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd172, %fd173;
	bra.uni 	$L__BB6_76;

$L__BB6_38:
	mov.u32 	%r279, %tid.x;
	shl.b32 	%r280, %r279, 2;
	mul.wide.u32 	%rd199, %r280, 8;
	add.s64 	%rd171, %rd29, %rd199;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd169, %rd170}, [%rd171];
	// end inline asm
	add.s64 	%rd174, %rd171, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd172, %rd173}, [%rd174];
	// end inline asm
	add.s64 	%rd177, %rd171, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd175, %rd176}, [%rd177];
	// end inline asm
	add.s64 	%rd180, %rd171, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd178, %rd179}, [%rd180];
	// end inline asm
	add.s64 	%rd183, %rd171, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd181, %rd182}, [%rd183];
	// end inline asm
	add.s64 	%rd186, %rd171, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd184, %rd185}, [%rd186];
	// end inline asm
	add.s64 	%rd189, %rd171, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd187, %rd188}, [%rd189];
	// end inline asm
	add.s64 	%rd192, %rd171, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd190, %rd191}, [%rd192];
	// end inline asm
	add.s64 	%rd195, %rd171, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd193, %rd194}, [%rd195];
	// end inline asm
	add.s64 	%rd198, %rd171, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd196, %rd197}, [%rd198];
	// end inline asm
	mov.b64 	%fd243, %rd169;
	mov.b64 	%fd244, %rd170;
	mov.b64 	%fd245, %rd172;
	mov.b64 	%fd246, %rd173;
	mov.b64 	%fd247, %rd175;
	mov.b64 	%fd248, %rd176;
	mov.b64 	%fd249, %rd178;
	mov.b64 	%fd250, %rd179;
	mov.b64 	%fd251, %rd181;
	mov.b64 	%fd252, %rd182;
	mov.b64 	%fd253, %rd184;
	mov.b64 	%fd254, %rd185;
	mov.b64 	%fd255, %rd187;
	mov.b64 	%fd256, %rd188;
	mov.b64 	%fd257, %rd190;
	mov.b64 	%fd258, %rd191;
	mov.b64 	%fd259, %rd193;
	mov.b64 	%fd260, %rd194;
	mov.b64 	%fd261, %rd196;
	mov.b64 	%fd262, %rd197;
	add.f64 	%fd263, %fd243, %fd244;
	add.f64 	%fd264, %fd263, %fd245;
	add.f64 	%fd265, %fd264, %fd246;
	add.f64 	%fd266, %fd265, %fd247;
	add.f64 	%fd267, %fd266, %fd248;
	add.f64 	%fd268, %fd267, %fd249;
	add.f64 	%fd269, %fd268, %fd250;
	add.f64 	%fd270, %fd269, %fd251;
	add.f64 	%fd271, %fd270, %fd252;
	add.f64 	%fd272, %fd271, %fd253;
	add.f64 	%fd273, %fd272, %fd254;
	add.f64 	%fd274, %fd273, %fd255;
	add.f64 	%fd275, %fd274, %fd256;
	add.f64 	%fd276, %fd275, %fd257;
	add.f64 	%fd277, %fd276, %fd258;
	add.f64 	%fd278, %fd277, %fd259;
	add.f64 	%fd279, %fd278, %fd260;
	add.f64 	%fd280, %fd279, %fd261;
	add.f64 	%fd608, %fd280, %fd262;
	setp.lt.s32 	%p49, %r81, 10240;
	mov.u32 	%r506, 5120;
	@%p49 bra 	$L__BB6_45;

	add.s32 	%r284, %r81, -10240;
	mul.wide.u32 	%rd200, %r284, -858993459;
	shr.u64 	%rd201, %rd200, 44;
	cvt.u32.u64 	%r285, %rd201;
	mov.u32 	%r500, 5120;
	add.s32 	%r33, %r285, 1;
	and.b32  	%r505, %r33, 3;
	setp.lt.u32 	%p50, %r284, 15360;
	mov.u32 	%r501, 10240;
	@%p50 bra 	$L__BB6_42;

	sub.s32 	%r499, %r33, %r505;

$L__BB6_41:
	mul.wide.s32 	%rd322, %r500, 8;
	add.s64 	%rd204, %rd171, %rd322;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd202, %rd203}, [%rd204];
	// end inline asm
	add.s64 	%rd207, %rd204, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd205, %rd206}, [%rd207];
	// end inline asm
	add.s64 	%rd210, %rd204, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd208, %rd209}, [%rd210];
	// end inline asm
	add.s64 	%rd213, %rd204, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd211, %rd212}, [%rd213];
	// end inline asm
	add.s64 	%rd216, %rd204, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd214, %rd215}, [%rd216];
	// end inline asm
	add.s64 	%rd219, %rd204, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd217, %rd218}, [%rd219];
	// end inline asm
	add.s64 	%rd222, %rd204, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd220, %rd221}, [%rd222];
	// end inline asm
	add.s64 	%rd225, %rd204, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd223, %rd224}, [%rd225];
	// end inline asm
	add.s64 	%rd228, %rd204, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd226, %rd227}, [%rd228];
	// end inline asm
	add.s64 	%rd231, %rd204, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd229, %rd230}, [%rd231];
	// end inline asm
	mov.b64 	%fd282, %rd202;
	mov.b64 	%fd283, %rd203;
	mov.b64 	%fd284, %rd205;
	mov.b64 	%fd285, %rd206;
	mov.b64 	%fd286, %rd208;
	mov.b64 	%fd287, %rd209;
	mov.b64 	%fd288, %rd211;
	mov.b64 	%fd289, %rd212;
	mov.b64 	%fd290, %rd214;
	mov.b64 	%fd291, %rd215;
	mov.b64 	%fd292, %rd217;
	mov.b64 	%fd293, %rd218;
	mov.b64 	%fd294, %rd220;
	mov.b64 	%fd295, %rd221;
	mov.b64 	%fd296, %rd223;
	mov.b64 	%fd297, %rd224;
	mov.b64 	%fd298, %rd226;
	mov.b64 	%fd299, %rd227;
	mov.b64 	%fd300, %rd229;
	mov.b64 	%fd301, %rd230;
	add.f64 	%fd302, %fd608, %fd282;
	add.f64 	%fd303, %fd302, %fd283;
	add.f64 	%fd304, %fd303, %fd284;
	add.f64 	%fd305, %fd304, %fd285;
	add.f64 	%fd306, %fd305, %fd286;
	add.f64 	%fd307, %fd306, %fd287;
	add.f64 	%fd308, %fd307, %fd288;
	add.f64 	%fd309, %fd308, %fd289;
	add.f64 	%fd310, %fd309, %fd290;
	add.f64 	%fd311, %fd310, %fd291;
	add.f64 	%fd312, %fd311, %fd292;
	add.f64 	%fd313, %fd312, %fd293;
	add.f64 	%fd314, %fd313, %fd294;
	add.f64 	%fd315, %fd314, %fd295;
	add.f64 	%fd316, %fd315, %fd296;
	add.f64 	%fd317, %fd316, %fd297;
	add.f64 	%fd318, %fd317, %fd298;
	add.f64 	%fd319, %fd318, %fd299;
	add.f64 	%fd320, %fd319, %fd300;
	add.f64 	%fd321, %fd320, %fd301;
	mul.wide.s32 	%rd323, %r501, 8;
	add.s64 	%rd234, %rd171, %rd323;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd232, %rd233}, [%rd234];
	// end inline asm
	add.s64 	%rd237, %rd234, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd235, %rd236}, [%rd237];
	// end inline asm
	add.s64 	%rd240, %rd234, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd238, %rd239}, [%rd240];
	// end inline asm
	add.s64 	%rd243, %rd234, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd241, %rd242}, [%rd243];
	// end inline asm
	add.s64 	%rd246, %rd234, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd244, %rd245}, [%rd246];
	// end inline asm
	add.s64 	%rd249, %rd234, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd247, %rd248}, [%rd249];
	// end inline asm
	add.s64 	%rd252, %rd234, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd250, %rd251}, [%rd252];
	// end inline asm
	add.s64 	%rd255, %rd234, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd253, %rd254}, [%rd255];
	// end inline asm
	add.s64 	%rd258, %rd234, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd256, %rd257}, [%rd258];
	// end inline asm
	add.s64 	%rd261, %rd234, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd259, %rd260}, [%rd261];
	// end inline asm
	mov.b64 	%fd322, %rd232;
	mov.b64 	%fd323, %rd233;
	mov.b64 	%fd324, %rd235;
	mov.b64 	%fd325, %rd236;
	mov.b64 	%fd326, %rd238;
	mov.b64 	%fd327, %rd239;
	mov.b64 	%fd328, %rd241;
	mov.b64 	%fd329, %rd242;
	mov.b64 	%fd330, %rd244;
	mov.b64 	%fd331, %rd245;
	mov.b64 	%fd332, %rd247;
	mov.b64 	%fd333, %rd248;
	mov.b64 	%fd334, %rd250;
	mov.b64 	%fd335, %rd251;
	mov.b64 	%fd336, %rd253;
	mov.b64 	%fd337, %rd254;
	mov.b64 	%fd338, %rd256;
	mov.b64 	%fd339, %rd257;
	mov.b64 	%fd340, %rd259;
	mov.b64 	%fd341, %rd260;
	add.f64 	%fd342, %fd321, %fd322;
	add.f64 	%fd343, %fd342, %fd323;
	add.f64 	%fd344, %fd343, %fd324;
	add.f64 	%fd345, %fd344, %fd325;
	add.f64 	%fd346, %fd345, %fd326;
	add.f64 	%fd347, %fd346, %fd327;
	add.f64 	%fd348, %fd347, %fd328;
	add.f64 	%fd349, %fd348, %fd329;
	add.f64 	%fd350, %fd349, %fd330;
	add.f64 	%fd351, %fd350, %fd331;
	add.f64 	%fd352, %fd351, %fd332;
	add.f64 	%fd353, %fd352, %fd333;
	add.f64 	%fd354, %fd353, %fd334;
	add.f64 	%fd355, %fd354, %fd335;
	add.f64 	%fd356, %fd355, %fd336;
	add.f64 	%fd357, %fd356, %fd337;
	add.f64 	%fd358, %fd357, %fd338;
	add.f64 	%fd359, %fd358, %fd339;
	add.f64 	%fd360, %fd359, %fd340;
	add.f64 	%fd361, %fd360, %fd341;
	add.s64 	%rd264, %rd234, 40960;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd262, %rd263}, [%rd264];
	// end inline asm
	add.s64 	%rd267, %rd234, 40976;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd265, %rd266}, [%rd267];
	// end inline asm
	add.s64 	%rd270, %rd234, 49152;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd268, %rd269}, [%rd270];
	// end inline asm
	add.s64 	%rd273, %rd234, 49168;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd271, %rd272}, [%rd273];
	// end inline asm
	add.s64 	%rd276, %rd234, 57344;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd274, %rd275}, [%rd276];
	// end inline asm
	add.s64 	%rd279, %rd234, 57360;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd277, %rd278}, [%rd279];
	// end inline asm
	add.s64 	%rd282, %rd234, 65536;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd280, %rd281}, [%rd282];
	// end inline asm
	add.s64 	%rd285, %rd234, 65552;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd283, %rd284}, [%rd285];
	// end inline asm
	add.s64 	%rd288, %rd234, 73728;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd286, %rd287}, [%rd288];
	// end inline asm
	add.s64 	%rd291, %rd234, 73744;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd289, %rd290}, [%rd291];
	// end inline asm
	mov.b64 	%fd362, %rd262;
	mov.b64 	%fd363, %rd263;
	mov.b64 	%fd364, %rd265;
	mov.b64 	%fd365, %rd266;
	mov.b64 	%fd366, %rd268;
	mov.b64 	%fd367, %rd269;
	mov.b64 	%fd368, %rd271;
	mov.b64 	%fd369, %rd272;
	mov.b64 	%fd370, %rd274;
	mov.b64 	%fd371, %rd275;
	mov.b64 	%fd372, %rd277;
	mov.b64 	%fd373, %rd278;
	mov.b64 	%fd374, %rd280;
	mov.b64 	%fd375, %rd281;
	mov.b64 	%fd376, %rd283;
	mov.b64 	%fd377, %rd284;
	mov.b64 	%fd378, %rd286;
	mov.b64 	%fd379, %rd287;
	mov.b64 	%fd380, %rd289;
	mov.b64 	%fd381, %rd290;
	add.f64 	%fd382, %fd361, %fd362;
	add.f64 	%fd383, %fd382, %fd363;
	add.f64 	%fd384, %fd383, %fd364;
	add.f64 	%fd385, %fd384, %fd365;
	add.f64 	%fd386, %fd385, %fd366;
	add.f64 	%fd387, %fd386, %fd367;
	add.f64 	%fd388, %fd387, %fd368;
	add.f64 	%fd389, %fd388, %fd369;
	add.f64 	%fd390, %fd389, %fd370;
	add.f64 	%fd391, %fd390, %fd371;
	add.f64 	%fd392, %fd391, %fd372;
	add.f64 	%fd393, %fd392, %fd373;
	add.f64 	%fd394, %fd393, %fd374;
	add.f64 	%fd395, %fd394, %fd375;
	add.f64 	%fd396, %fd395, %fd376;
	add.f64 	%fd397, %fd396, %fd377;
	add.f64 	%fd398, %fd397, %fd378;
	add.f64 	%fd399, %fd398, %fd379;
	add.f64 	%fd400, %fd399, %fd380;
	add.f64 	%fd401, %fd400, %fd381;
	add.s32 	%r500, %r501, 15360;
	add.s64 	%rd294, %rd234, 81920;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd292, %rd293}, [%rd294];
	// end inline asm
	add.s64 	%rd297, %rd234, 81936;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd295, %rd296}, [%rd297];
	// end inline asm
	add.s64 	%rd300, %rd234, 90112;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd298, %rd299}, [%rd300];
	// end inline asm
	add.s64 	%rd303, %rd234, 90128;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd301, %rd302}, [%rd303];
	// end inline asm
	add.s64 	%rd306, %rd234, 98304;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd304, %rd305}, [%rd306];
	// end inline asm
	add.s64 	%rd309, %rd234, 98320;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd307, %rd308}, [%rd309];
	// end inline asm
	add.s64 	%rd312, %rd234, 106496;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd310, %rd311}, [%rd312];
	// end inline asm
	add.s64 	%rd315, %rd234, 106512;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd313, %rd314}, [%rd315];
	// end inline asm
	add.s64 	%rd318, %rd234, 114688;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd316, %rd317}, [%rd318];
	// end inline asm
	add.s64 	%rd321, %rd234, 114704;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd319, %rd320}, [%rd321];
	// end inline asm
	mov.b64 	%fd402, %rd292;
	mov.b64 	%fd403, %rd293;
	mov.b64 	%fd404, %rd295;
	mov.b64 	%fd405, %rd296;
	mov.b64 	%fd406, %rd298;
	mov.b64 	%fd407, %rd299;
	mov.b64 	%fd408, %rd301;
	mov.b64 	%fd409, %rd302;
	mov.b64 	%fd410, %rd304;
	mov.b64 	%fd411, %rd305;
	mov.b64 	%fd412, %rd307;
	mov.b64 	%fd413, %rd308;
	mov.b64 	%fd414, %rd310;
	mov.b64 	%fd415, %rd311;
	mov.b64 	%fd416, %rd313;
	mov.b64 	%fd417, %rd314;
	mov.b64 	%fd418, %rd316;
	mov.b64 	%fd419, %rd317;
	mov.b64 	%fd420, %rd319;
	mov.b64 	%fd421, %rd320;
	add.f64 	%fd422, %fd401, %fd402;
	add.f64 	%fd423, %fd422, %fd403;
	add.f64 	%fd424, %fd423, %fd404;
	add.f64 	%fd425, %fd424, %fd405;
	add.f64 	%fd426, %fd425, %fd406;
	add.f64 	%fd427, %fd426, %fd407;
	add.f64 	%fd428, %fd427, %fd408;
	add.f64 	%fd429, %fd428, %fd409;
	add.f64 	%fd430, %fd429, %fd410;
	add.f64 	%fd431, %fd430, %fd411;
	add.f64 	%fd432, %fd431, %fd412;
	add.f64 	%fd433, %fd432, %fd413;
	add.f64 	%fd434, %fd433, %fd414;
	add.f64 	%fd435, %fd434, %fd415;
	add.f64 	%fd436, %fd435, %fd416;
	add.f64 	%fd437, %fd436, %fd417;
	add.f64 	%fd438, %fd437, %fd418;
	add.f64 	%fd439, %fd438, %fd419;
	add.f64 	%fd440, %fd439, %fd420;
	add.f64 	%fd608, %fd440, %fd421;
	add.s32 	%r501, %r501, 20480;
	add.s32 	%r499, %r499, -4;
	setp.ne.s32 	%p51, %r499, 0;
	@%p51 bra 	$L__BB6_41;

$L__BB6_42:
	setp.eq.s32 	%p52, %r505, 0;
	mov.u32 	%r506, %r500;
	@%p52 bra 	$L__BB6_45;

	add.s32 	%r506, %r501, -5120;

$L__BB6_44:
	.pragma "nounroll";
	add.s32 	%r506, %r506, 5120;
	mul.wide.s32 	%rd354, %r500, 8;
	add.s64 	%rd326, %rd171, %rd354;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd324, %rd325}, [%rd326];
	// end inline asm
	add.s64 	%rd329, %rd326, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd327, %rd328}, [%rd329];
	// end inline asm
	add.s64 	%rd332, %rd326, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd330, %rd331}, [%rd332];
	// end inline asm
	add.s64 	%rd335, %rd326, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd333, %rd334}, [%rd335];
	// end inline asm
	add.s64 	%rd338, %rd326, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd336, %rd337}, [%rd338];
	// end inline asm
	add.s64 	%rd341, %rd326, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd339, %rd340}, [%rd341];
	// end inline asm
	add.s64 	%rd344, %rd326, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd342, %rd343}, [%rd344];
	// end inline asm
	add.s64 	%rd347, %rd326, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd345, %rd346}, [%rd347];
	// end inline asm
	add.s64 	%rd350, %rd326, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd348, %rd349}, [%rd350];
	// end inline asm
	add.s64 	%rd353, %rd326, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd351, %rd352}, [%rd353];
	// end inline asm
	mov.b64 	%fd441, %rd324;
	mov.b64 	%fd442, %rd325;
	mov.b64 	%fd443, %rd327;
	mov.b64 	%fd444, %rd328;
	mov.b64 	%fd445, %rd330;
	mov.b64 	%fd446, %rd331;
	mov.b64 	%fd447, %rd333;
	mov.b64 	%fd448, %rd334;
	mov.b64 	%fd449, %rd336;
	mov.b64 	%fd450, %rd337;
	mov.b64 	%fd451, %rd339;
	mov.b64 	%fd452, %rd340;
	mov.b64 	%fd453, %rd342;
	mov.b64 	%fd454, %rd343;
	mov.b64 	%fd455, %rd345;
	mov.b64 	%fd456, %rd346;
	mov.b64 	%fd457, %rd348;
	mov.b64 	%fd458, %rd349;
	mov.b64 	%fd459, %rd351;
	mov.b64 	%fd460, %rd352;
	add.f64 	%fd461, %fd608, %fd441;
	add.f64 	%fd462, %fd461, %fd442;
	add.f64 	%fd463, %fd462, %fd443;
	add.f64 	%fd464, %fd463, %fd444;
	add.f64 	%fd465, %fd464, %fd445;
	add.f64 	%fd466, %fd465, %fd446;
	add.f64 	%fd467, %fd466, %fd447;
	add.f64 	%fd468, %fd467, %fd448;
	add.f64 	%fd469, %fd468, %fd449;
	add.f64 	%fd470, %fd469, %fd450;
	add.f64 	%fd471, %fd470, %fd451;
	add.f64 	%fd472, %fd471, %fd452;
	add.f64 	%fd473, %fd472, %fd453;
	add.f64 	%fd474, %fd473, %fd454;
	add.f64 	%fd475, %fd474, %fd455;
	add.f64 	%fd476, %fd475, %fd456;
	add.f64 	%fd477, %fd476, %fd457;
	add.f64 	%fd478, %fd477, %fd458;
	add.f64 	%fd479, %fd478, %fd459;
	add.f64 	%fd608, %fd479, %fd460;
	add.s32 	%r505, %r505, -1;
	setp.ne.s32 	%p53, %r505, 0;
	mov.u32 	%r500, %r506;
	@%p53 bra 	$L__BB6_44;

$L__BB6_45:
	setp.ge.s32 	%p54, %r506, %r81;
	@%p54 bra 	$L__BB6_53;

	sub.s32 	%r52, %r81, %r506;
	setp.ge.s32 	%p55, %r279, %r52;
	@%p55 bra 	$L__BB6_53;

	not.b32 	%r289, %r506;
	add.s32 	%r290, %r289, %r81;
	sub.s32 	%r54, %r290, %r279;
	shr.u32 	%r291, %r54, 8;
	add.s32 	%r292, %r291, 1;
	and.b32  	%r508, %r292, 3;
	setp.eq.s32 	%p56, %r508, 0;
	mov.u32 	%r509, %r279;
	@%p56 bra 	$L__BB6_50;

	mov.u32 	%r509, %tid.x;
	add.s32 	%r293, %r509, %r506;
	mul.wide.s32 	%rd355, %r293, 8;
	add.s64 	%rd418, %rd29, %rd355;

$L__BB6_49:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd356, [%rd418];
	// end inline asm
	mov.b64 	%fd481, %rd356;
	add.f64 	%fd608, %fd608, %fd481;
	add.s32 	%r509, %r509, 256;
	add.s64 	%rd418, %rd418, 2048;
	add.s32 	%r508, %r508, -1;
	setp.ne.s32 	%p57, %r508, 0;
	@%p57 bra 	$L__BB6_49;

$L__BB6_50:
	setp.lt.u32 	%p58, %r54, 768;
	@%p58 bra 	$L__BB6_53;

	add.s32 	%r294, %r506, %r509;
	mul.wide.s32 	%rd358, %r294, 8;
	add.s64 	%rd419, %rd29, %rd358;

$L__BB6_52:
	// begin inline asm
	ld.global.nc.u64 %rd359, [%rd419];
	// end inline asm
	mov.b64 	%fd482, %rd359;
	add.f64 	%fd483, %fd608, %fd482;
	add.s64 	%rd362, %rd419, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd361, [%rd362];
	// end inline asm
	mov.b64 	%fd484, %rd361;
	add.f64 	%fd485, %fd483, %fd484;
	add.s64 	%rd364, %rd419, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd363, [%rd364];
	// end inline asm
	mov.b64 	%fd486, %rd363;
	add.f64 	%fd487, %fd485, %fd486;
	add.s64 	%rd366, %rd419, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd365, [%rd366];
	// end inline asm
	mov.b64 	%fd488, %rd365;
	add.f64 	%fd608, %fd487, %fd488;
	add.s64 	%rd419, %rd419, 8192;
	add.s32 	%r509, %r509, 1024;
	setp.lt.s32 	%p59, %r509, %r52;
	@%p59 bra 	$L__BB6_52;

$L__BB6_53:
	// begin inline asm
	mov.u32 %r295, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r296, %laneid;
	// end inline asm
	mov.b64 	%rd367, %fd608;
	mov.u32 	%r304, 1;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, -1;
	mov.b64 	{%r298, %r303}, %rd367;
	// begin inline asm
	shfl.sync.down.b32 %r297, %r298, %r304, %r345, %r346;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r302, %r303, %r304, %r345, %r346;
	// end inline asm
	mov.b64 	%rd368, {%r297, %r302};
	mov.b64 	%fd489, %rd368;
	setp.gt.s32 	%p60, %r296, 30;
	add.f64 	%fd490, %fd608, %fd489;
	selp.f64 	%fd491, %fd608, %fd490, %p60;
	mov.b64 	%rd369, %fd491;
	mov.u32 	%r314, 2;
	mov.b64 	{%r308, %r313}, %rd369;
	// begin inline asm
	shfl.sync.down.b32 %r307, %r308, %r314, %r345, %r346;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r312, %r313, %r314, %r345, %r346;
	// end inline asm
	mov.b64 	%rd370, {%r307, %r312};
	mov.b64 	%fd492, %rd370;
	setp.gt.s32 	%p61, %r296, 29;
	add.f64 	%fd493, %fd491, %fd492;
	selp.f64 	%fd494, %fd491, %fd493, %p61;
	mov.b64 	%rd371, %fd494;
	mov.u32 	%r324, 4;
	mov.b64 	{%r318, %r323}, %rd371;
	// begin inline asm
	shfl.sync.down.b32 %r317, %r318, %r324, %r345, %r346;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r322, %r323, %r324, %r345, %r346;
	// end inline asm
	mov.b64 	%rd372, {%r317, %r322};
	mov.b64 	%fd495, %rd372;
	setp.gt.s32 	%p62, %r296, 27;
	add.f64 	%fd496, %fd494, %fd495;
	selp.f64 	%fd497, %fd494, %fd496, %p62;
	mov.b64 	%rd373, %fd497;
	mov.u32 	%r334, 8;
	mov.b64 	{%r328, %r333}, %rd373;
	// begin inline asm
	shfl.sync.down.b32 %r327, %r328, %r334, %r345, %r346;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r332, %r333, %r334, %r345, %r346;
	// end inline asm
	mov.b64 	%rd374, {%r327, %r332};
	mov.b64 	%fd498, %rd374;
	setp.gt.s32 	%p63, %r296, 23;
	add.f64 	%fd499, %fd497, %fd498;
	selp.f64 	%fd500, %fd497, %fd499, %p63;
	mov.b64 	%rd375, %fd500;
	mov.u32 	%r344, 16;
	mov.b64 	{%r338, %r343}, %rd375;
	// begin inline asm
	shfl.sync.down.b32 %r337, %r338, %r344, %r345, %r346;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r342, %r343, %r344, %r345, %r346;
	// end inline asm
	mov.b64 	%rd376, {%r337, %r342};
	mov.b64 	%fd501, %rd376;
	setp.gt.s32 	%p64, %r296, 15;
	add.f64 	%fd502, %fd500, %fd501;
	selp.f64 	%fd615, %fd500, %fd502, %p64;
	setp.ne.s32 	%p65, %r295, 0;
	@%p65 bra 	$L__BB6_55;

	shr.s32 	%r348, %r279, 31;
	shr.u32 	%r349, %r348, 27;
	add.s32 	%r350, %r279, %r349;
	shr.s32 	%r351, %r350, 5;
	shl.b32 	%r352, %r351, 3;
	mov.u32 	%r353, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r354, %r353, %r352;
	st.shared.f64 	[%r354+8], %fd615;

$L__BB6_55:
	bar.sync 	0;
	setp.ne.s32 	%p66, %r279, 0;
	@%p66 bra 	$L__BB6_76;

	ld.shared.f64 	%fd503, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd504, %fd615, %fd503;
	ld.shared.f64 	%fd505, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd506, %fd504, %fd505;
	ld.shared.f64 	%fd507, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd508, %fd506, %fd507;
	ld.shared.f64 	%fd509, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd510, %fd508, %fd509;
	ld.shared.f64 	%fd511, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd512, %fd510, %fd511;
	ld.shared.f64 	%fd513, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd514, %fd512, %fd513;
	ld.shared.f64 	%fd515, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd514, %fd515;
	bra.uni 	$L__BB6_76;

$L__BB6_28:
	setp.ne.s32 	%p26, %r158, 0;
	shl.b32 	%r217, %r28, 5;
	add.s32 	%r218, %r217, 32;
	setp.gt.s32 	%p27, %r218, %r81;
	// begin inline asm
	mov.u32 %r166, %laneid;
	// end inline asm
	not.b32 	%r219, %r217;
	mov.u32 	%r216, -1;
	add.s32 	%r220, %r219, %r81;
	selp.b32 	%r215, %r220, 31, %p27;
	mov.u32 	%r174, 1;
	// begin inline asm
	shfl.sync.down.b32 %r167, %r30, %r174, %r215, %r216;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r172, %r31, %r174, %r215, %r216;
	// end inline asm
	mov.b64 	%rd151, {%r167, %r172};
	mov.b64 	%fd184, %rd151;
	setp.lt.s32 	%p28, %r166, %r215;
	add.f64 	%fd185, %fd598, %fd184;
	selp.f64 	%fd186, %fd185, %fd598, %p28;
	mov.b64 	%rd152, %fd186;
	mov.u32 	%r184, 2;
	mov.b64 	{%r178, %r183}, %rd152;
	// begin inline asm
	shfl.sync.down.b32 %r177, %r178, %r184, %r215, %r216;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r182, %r183, %r184, %r215, %r216;
	// end inline asm
	mov.b64 	%rd153, {%r177, %r182};
	mov.b64 	%fd187, %rd153;
	add.s32 	%r221, %r166, 2;
	setp.gt.s32 	%p29, %r221, %r215;
	add.f64 	%fd188, %fd186, %fd187;
	selp.f64 	%fd189, %fd186, %fd188, %p29;
	mov.b64 	%rd154, %fd189;
	mov.u32 	%r194, 4;
	mov.b64 	{%r188, %r193}, %rd154;
	// begin inline asm
	shfl.sync.down.b32 %r187, %r188, %r194, %r215, %r216;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r192, %r193, %r194, %r215, %r216;
	// end inline asm
	mov.b64 	%rd155, {%r187, %r192};
	mov.b64 	%fd190, %rd155;
	add.s32 	%r222, %r166, 4;
	setp.gt.s32 	%p30, %r222, %r215;
	add.f64 	%fd191, %fd189, %fd190;
	selp.f64 	%fd192, %fd189, %fd191, %p30;
	mov.b64 	%rd156, %fd192;
	mov.u32 	%r204, 8;
	mov.b64 	{%r198, %r203}, %rd156;
	// begin inline asm
	shfl.sync.down.b32 %r197, %r198, %r204, %r215, %r216;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r202, %r203, %r204, %r215, %r216;
	// end inline asm
	mov.b64 	%rd157, {%r197, %r202};
	mov.b64 	%fd193, %rd157;
	add.s32 	%r223, %r166, 8;
	setp.gt.s32 	%p31, %r223, %r215;
	add.f64 	%fd194, %fd192, %fd193;
	selp.f64 	%fd195, %fd192, %fd194, %p31;
	mov.b64 	%rd158, %fd195;
	mov.u32 	%r214, 16;
	mov.b64 	{%r208, %r213}, %rd158;
	// begin inline asm
	shfl.sync.down.b32 %r207, %r208, %r214, %r215, %r216;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r212, %r213, %r214, %r215, %r216;
	// end inline asm
	mov.b64 	%rd159, {%r207, %r212};
	mov.b64 	%fd196, %rd159;
	add.s32 	%r224, %r166, 16;
	setp.gt.s32 	%p32, %r224, %r215;
	add.f64 	%fd197, %fd195, %fd196;
	selp.f64 	%fd615, %fd195, %fd197, %p32;
	@%p26 bra 	$L__BB6_30;

	add.s32 	%r482, %r165, 8;
	st.shared.f64 	[%r482], %fd615;

$L__BB6_30:
	bar.sync 	0;
	setp.ne.s32 	%p33, %r159, 0;
	@%p33 bra 	$L__BB6_76;

	setp.gt.s32 	%p34, %r81, 32;
	ld.shared.f64 	%fd198, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd199, %fd615, %fd198;
	selp.f64 	%fd200, %fd199, %fd615, %p34;
	ld.shared.f64 	%fd201, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd202, %fd200, %fd201;
	setp.gt.s32 	%p35, %r81, 64;
	selp.f64 	%fd203, %fd202, %fd200, %p35;
	ld.shared.f64 	%fd204, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd205, %fd203, %fd204;
	setp.gt.s32 	%p36, %r81, 96;
	selp.f64 	%fd206, %fd205, %fd203, %p36;
	ld.shared.f64 	%fd207, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd208, %fd206, %fd207;
	setp.gt.s32 	%p37, %r81, 128;
	selp.f64 	%fd209, %fd208, %fd206, %p37;
	ld.shared.f64 	%fd210, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd211, %fd209, %fd210;
	setp.gt.s32 	%p38, %r81, 160;
	selp.f64 	%fd212, %fd211, %fd209, %p38;
	ld.shared.f64 	%fd213, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd214, %fd212, %fd213;
	setp.gt.s32 	%p39, %r81, 192;
	selp.f64 	%fd615, %fd214, %fd212, %p39;
	setp.lt.s32 	%p40, %r81, 225;
	@%p40 bra 	$L__BB6_76;

	ld.shared.f64 	%fd215, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd615, %fd215;
	bra.uni 	$L__BB6_76;

$L__BB6_67:
	setp.ne.s32 	%p74, %r360, 0;
	shl.b32 	%r419, %r76, 5;
	add.s32 	%r420, %r419, 32;
	setp.gt.s32 	%p75, %r420, %r81;
	// begin inline asm
	mov.u32 %r368, %laneid;
	// end inline asm
	not.b32 	%r421, %r419;
	mov.u32 	%r418, -1;
	add.s32 	%r422, %r421, %r81;
	selp.b32 	%r417, %r422, 31, %p75;
	mov.u32 	%r376, 1;
	// begin inline asm
	shfl.sync.down.b32 %r369, %r78, %r376, %r417, %r418;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r374, %r79, %r376, %r417, %r418;
	// end inline asm
	mov.b64 	%rd393, {%r369, %r374};
	mov.b64 	%fd526, %rd393;
	setp.lt.s32 	%p76, %r368, %r417;
	add.f64 	%fd527, %fd614, %fd526;
	selp.f64 	%fd528, %fd527, %fd614, %p76;
	mov.b64 	%rd394, %fd528;
	mov.u32 	%r386, 2;
	mov.b64 	{%r380, %r385}, %rd394;
	// begin inline asm
	shfl.sync.down.b32 %r379, %r380, %r386, %r417, %r418;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r384, %r385, %r386, %r417, %r418;
	// end inline asm
	mov.b64 	%rd395, {%r379, %r384};
	mov.b64 	%fd529, %rd395;
	add.s32 	%r423, %r368, 2;
	setp.gt.s32 	%p77, %r423, %r417;
	add.f64 	%fd530, %fd528, %fd529;
	selp.f64 	%fd531, %fd528, %fd530, %p77;
	mov.b64 	%rd396, %fd531;
	mov.u32 	%r396, 4;
	mov.b64 	{%r390, %r395}, %rd396;
	// begin inline asm
	shfl.sync.down.b32 %r389, %r390, %r396, %r417, %r418;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r394, %r395, %r396, %r417, %r418;
	// end inline asm
	mov.b64 	%rd397, {%r389, %r394};
	mov.b64 	%fd532, %rd397;
	add.s32 	%r424, %r368, 4;
	setp.gt.s32 	%p78, %r424, %r417;
	add.f64 	%fd533, %fd531, %fd532;
	selp.f64 	%fd534, %fd531, %fd533, %p78;
	mov.b64 	%rd398, %fd534;
	mov.u32 	%r406, 8;
	mov.b64 	{%r400, %r405}, %rd398;
	// begin inline asm
	shfl.sync.down.b32 %r399, %r400, %r406, %r417, %r418;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r404, %r405, %r406, %r417, %r418;
	// end inline asm
	mov.b64 	%rd399, {%r399, %r404};
	mov.b64 	%fd535, %rd399;
	add.s32 	%r425, %r368, 8;
	setp.gt.s32 	%p79, %r425, %r417;
	add.f64 	%fd536, %fd534, %fd535;
	selp.f64 	%fd537, %fd534, %fd536, %p79;
	mov.b64 	%rd400, %fd537;
	mov.u32 	%r416, 16;
	mov.b64 	{%r410, %r415}, %rd400;
	// begin inline asm
	shfl.sync.down.b32 %r409, %r410, %r416, %r417, %r418;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r414, %r415, %r416, %r417, %r418;
	// end inline asm
	mov.b64 	%rd401, {%r409, %r414};
	mov.b64 	%fd538, %rd401;
	add.s32 	%r426, %r368, 16;
	setp.gt.s32 	%p80, %r426, %r417;
	add.f64 	%fd539, %fd537, %fd538;
	selp.f64 	%fd615, %fd537, %fd539, %p80;
	@%p74 bra 	$L__BB6_69;

	add.s32 	%r484, %r367, 8;
	st.shared.f64 	[%r484], %fd615;

$L__BB6_69:
	bar.sync 	0;
	setp.ne.s32 	%p81, %r361, 0;
	@%p81 bra 	$L__BB6_76;

	setp.gt.s32 	%p82, %r81, 32;
	ld.shared.f64 	%fd540, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd541, %fd615, %fd540;
	selp.f64 	%fd542, %fd541, %fd615, %p82;
	ld.shared.f64 	%fd543, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd544, %fd542, %fd543;
	setp.gt.s32 	%p83, %r81, 64;
	selp.f64 	%fd545, %fd544, %fd542, %p83;
	ld.shared.f64 	%fd546, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd547, %fd545, %fd546;
	setp.gt.s32 	%p84, %r81, 96;
	selp.f64 	%fd548, %fd547, %fd545, %p84;
	ld.shared.f64 	%fd549, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd550, %fd548, %fd549;
	setp.gt.s32 	%p85, %r81, 128;
	selp.f64 	%fd551, %fd550, %fd548, %p85;
	ld.shared.f64 	%fd552, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd553, %fd551, %fd552;
	setp.gt.s32 	%p86, %r81, 160;
	selp.f64 	%fd554, %fd553, %fd551, %p86;
	ld.shared.f64 	%fd555, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd556, %fd554, %fd555;
	setp.gt.s32 	%p87, %r81, 192;
	selp.f64 	%fd615, %fd556, %fd554, %p87;
	setp.lt.s32 	%p88, %r81, 225;
	@%p88 bra 	$L__BB6_76;

	ld.shared.f64 	%fd557, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd615, %fd557;

$L__BB6_76:
	mov.u32 	%r480, %tid.x;
	setp.ne.s32 	%p96, %r480, 0;
	@%p96 bra 	$L__BB6_80;

	add.f64 	%fd585, %fd615, %fd60;
	cvta.to.global.u64 	%rd411, %rd30;
	st.global.f64 	[%rd411], %fd585;

$L__BB6_80:
	ret;

}
	// .globl	_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_
.visible .entry _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_(
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_0,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_1,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_2,
	.param .align 1 .b8 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_3[1],
	.param .f64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_4
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<97>;
	.reg .f32 	%f<143>;
	.reg .b32 	%r<614>;
	.reg .f64 	%fd<534>;
	.reg .b64 	%rd<289>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage[80];

	ld.param.u64 	%rd48, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_0];
	ld.param.u64 	%rd49, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_1];
	ld.param.u64 	%rd50, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_2];
	ld.param.f64 	%fd59, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4__param_4];
	setp.eq.s64 	%p1, %rd50, 0;
	@%p1 bra 	$L__BB7_77;

	and.b64  	%rd51, %rd48, 15;
	setp.eq.s64 	%p2, %rd51, 0;
	@%p2 bra 	$L__BB7_37;

	setp.lt.s64 	%p3, %rd50, 5120;
	@%p3 bra 	$L__BB7_18;
	bra.uni 	$L__BB7_3;

$L__BB7_18:
	cvt.u32.u64 	%r13, %rd50;
	mov.u32 	%r603, %tid.x;
	setp.ge.s32 	%p19, %r603, %r13;
	@%p19 bra 	$L__BB7_20;

	mov.u32 	%r176, %tid.x;
	mul.wide.s32 	%rd118, %r176, 4;
	add.s64 	%rd117, %rd48, %rd118;
	// begin inline asm
	ld.global.nc.u32 %r175, [%rd117];
	// end inline asm
	mov.b32 	%f46, %r175;
	cvt.f64.f32 	%fd517, %f46;
	add.s32 	%r603, %r176, 256;

$L__BB7_20:
	setp.ge.s32 	%p20, %r603, %r13;
	@%p20 bra 	$L__BB7_27;

	not.b32 	%r177, %r603;
	add.s32 	%r17, %r177, %r13;
	shr.u32 	%r178, %r17, 8;
	add.s32 	%r179, %r178, 1;
	and.b32  	%r602, %r179, 3;
	setp.eq.s32 	%p21, %r602, 0;
	@%p21 bra 	$L__BB7_24;

	mul.wide.s32 	%rd119, %r603, 4;
	add.s64 	%rd276, %rd48, %rd119;

$L__BB7_23:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r180, [%rd276];
	// end inline asm
	mov.b32 	%f47, %r180;
	cvt.f64.f32 	%fd175, %f47;
	add.f64 	%fd517, %fd517, %fd175;
	add.s32 	%r603, %r603, 256;
	add.s64 	%rd276, %rd276, 1024;
	add.s32 	%r602, %r602, -1;
	setp.ne.s32 	%p22, %r602, 0;
	@%p22 bra 	$L__BB7_23;

$L__BB7_24:
	setp.lt.u32 	%p23, %r17, 768;
	@%p23 bra 	$L__BB7_27;

	mul.wide.s32 	%rd121, %r603, 4;
	add.s64 	%rd277, %rd48, %rd121;

$L__BB7_26:
	// begin inline asm
	ld.global.nc.u32 %r181, [%rd277];
	// end inline asm
	mov.b32 	%f48, %r181;
	cvt.f64.f32 	%fd176, %f48;
	add.f64 	%fd177, %fd517, %fd176;
	add.s64 	%rd123, %rd277, 1024;
	// begin inline asm
	ld.global.nc.u32 %r182, [%rd123];
	// end inline asm
	mov.b32 	%f49, %r182;
	cvt.f64.f32 	%fd178, %f49;
	add.f64 	%fd179, %fd177, %fd178;
	add.s64 	%rd124, %rd277, 2048;
	// begin inline asm
	ld.global.nc.u32 %r183, [%rd124];
	// end inline asm
	mov.b32 	%f50, %r183;
	cvt.f64.f32 	%fd180, %f50;
	add.f64 	%fd181, %fd179, %fd180;
	add.s64 	%rd125, %rd277, 3072;
	// begin inline asm
	ld.global.nc.u32 %r184, [%rd125];
	// end inline asm
	mov.b32 	%f51, %r184;
	cvt.f64.f32 	%fd182, %f51;
	add.f64 	%fd517, %fd181, %fd182;
	add.s64 	%rd277, %rd277, 4096;
	add.s32 	%r603, %r603, 1024;
	setp.lt.s32 	%p24, %r603, %r13;
	@%p24 bra 	$L__BB7_26;

$L__BB7_27:
	mov.u32 	%r186, %tid.x;
	shr.s32 	%r187, %r186, 31;
	shr.u32 	%r188, %r187, 27;
	add.s32 	%r189, %r186, %r188;
	shr.s32 	%r26, %r189, 5;
	// begin inline asm
	mov.u32 %r185, %laneid;
	// end inline asm
	mov.b64 	%rd126, %fd517;
	mov.b64 	{%r28, %r29}, %rd126;
	shl.b32 	%r190, %r26, 3;
	mov.u32 	%r191, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r192, %r191, %r190;
	setp.gt.s32 	%p25, %r13, 255;
	@%p25 bra 	$L__BB7_33;
	bra.uni 	$L__BB7_28;

$L__BB7_33:
	setp.ne.s32 	%p41, %r185, 0;
	// begin inline asm
	mov.u32 %r253, %laneid;
	// end inline asm
	mov.u32 	%r261, 1;
	mov.u32 	%r302, 31;
	mov.u32 	%r303, -1;
	// begin inline asm
	shfl.sync.down.b32 %r254, %r28, %r261, %r302, %r303;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r259, %r29, %r261, %r302, %r303;
	// end inline asm
	mov.b64 	%rd136, {%r254, %r259};
	mov.b64 	%fd215, %rd136;
	setp.gt.s32 	%p42, %r253, 30;
	add.f64 	%fd216, %fd517, %fd215;
	selp.f64 	%fd217, %fd517, %fd216, %p42;
	mov.b64 	%rd137, %fd217;
	mov.u32 	%r271, 2;
	mov.b64 	{%r265, %r270}, %rd137;
	// begin inline asm
	shfl.sync.down.b32 %r264, %r265, %r271, %r302, %r303;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r269, %r270, %r271, %r302, %r303;
	// end inline asm
	mov.b64 	%rd138, {%r264, %r269};
	mov.b64 	%fd218, %rd138;
	setp.gt.s32 	%p43, %r253, 29;
	add.f64 	%fd219, %fd217, %fd218;
	selp.f64 	%fd220, %fd217, %fd219, %p43;
	mov.b64 	%rd139, %fd220;
	mov.u32 	%r281, 4;
	mov.b64 	{%r275, %r280}, %rd139;
	// begin inline asm
	shfl.sync.down.b32 %r274, %r275, %r281, %r302, %r303;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r279, %r280, %r281, %r302, %r303;
	// end inline asm
	mov.b64 	%rd140, {%r274, %r279};
	mov.b64 	%fd221, %rd140;
	setp.gt.s32 	%p44, %r253, 27;
	add.f64 	%fd222, %fd220, %fd221;
	selp.f64 	%fd223, %fd220, %fd222, %p44;
	mov.b64 	%rd141, %fd223;
	mov.u32 	%r291, 8;
	mov.b64 	{%r285, %r290}, %rd141;
	// begin inline asm
	shfl.sync.down.b32 %r284, %r285, %r291, %r302, %r303;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r289, %r290, %r291, %r302, %r303;
	// end inline asm
	mov.b64 	%rd142, {%r284, %r289};
	mov.b64 	%fd224, %rd142;
	setp.gt.s32 	%p45, %r253, 23;
	add.f64 	%fd225, %fd223, %fd224;
	selp.f64 	%fd226, %fd223, %fd225, %p45;
	mov.b64 	%rd143, %fd226;
	mov.u32 	%r301, 16;
	mov.b64 	{%r295, %r300}, %rd143;
	// begin inline asm
	shfl.sync.down.b32 %r294, %r295, %r301, %r302, %r303;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r299, %r300, %r301, %r302, %r303;
	// end inline asm
	mov.b64 	%rd144, {%r294, %r299};
	mov.b64 	%fd227, %rd144;
	setp.gt.s32 	%p46, %r253, 15;
	add.f64 	%fd228, %fd226, %fd227;
	selp.f64 	%fd533, %fd226, %fd228, %p46;
	@%p41 bra 	$L__BB7_35;

	add.s32 	%r593, %r192, 8;
	st.shared.f64 	[%r593], %fd533;

$L__BB7_35:
	bar.sync 	0;
	setp.ne.s32 	%p47, %r186, 0;
	@%p47 bra 	$L__BB7_75;

	ld.shared.f64 	%fd229, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd230, %fd533, %fd229;
	ld.shared.f64 	%fd231, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd232, %fd230, %fd231;
	ld.shared.f64 	%fd233, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd234, %fd232, %fd233;
	ld.shared.f64 	%fd235, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd236, %fd234, %fd235;
	ld.shared.f64 	%fd237, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd238, %fd236, %fd237;
	ld.shared.f64 	%fd239, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd240, %fd238, %fd239;
	ld.shared.f64 	%fd241, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd240, %fd241;
	bra.uni 	$L__BB7_75;

$L__BB7_77:
	mov.u32 	%r591, %tid.x;
	setp.ne.s32 	%p96, %r591, 0;
	@%p96 bra 	$L__BB7_79;

	cvta.to.global.u64 	%rd269, %rd49;
	st.global.f64 	[%rd269], %fd59;
	bra.uni 	$L__BB7_79;

$L__BB7_37:
	setp.lt.s64 	%p48, %rd50, 5120;
	@%p48 bra 	$L__BB7_56;
	bra.uni 	$L__BB7_38;

$L__BB7_56:
	cvt.u32.u64 	%r43, %rd50;
	mov.u32 	%r612, %tid.x;
	setp.ge.s32 	%p66, %r612, %r43;
	@%p66 bra 	$L__BB7_58;

	mov.u32 	%r461, %tid.x;
	mul.wide.s32 	%rd241, %r461, 4;
	add.s64 	%rd240, %rd48, %rd241;
	// begin inline asm
	ld.global.nc.u32 %r460, [%rd240];
	// end inline asm
	mov.b32 	%f137, %r460;
	cvt.f64.f32 	%fd532, %f137;
	add.s32 	%r612, %r461, 256;

$L__BB7_58:
	setp.ge.s32 	%p67, %r612, %r43;
	@%p67 bra 	$L__BB7_65;

	not.b32 	%r462, %r612;
	add.s32 	%r47, %r462, %r43;
	shr.u32 	%r463, %r47, 8;
	add.s32 	%r464, %r463, 1;
	and.b32  	%r611, %r464, 3;
	setp.eq.s32 	%p68, %r611, 0;
	@%p68 bra 	$L__BB7_62;

	mul.wide.s32 	%rd242, %r612, 4;
	add.s64 	%rd287, %rd48, %rd242;

$L__BB7_61:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r465, [%rd287];
	// end inline asm
	mov.b32 	%f138, %r465;
	cvt.f64.f32 	%fd437, %f138;
	add.f64 	%fd532, %fd532, %fd437;
	add.s32 	%r612, %r612, 256;
	add.s64 	%rd287, %rd287, 1024;
	add.s32 	%r611, %r611, -1;
	setp.ne.s32 	%p69, %r611, 0;
	@%p69 bra 	$L__BB7_61;

$L__BB7_62:
	setp.lt.u32 	%p70, %r47, 768;
	@%p70 bra 	$L__BB7_65;

	mul.wide.s32 	%rd244, %r612, 4;
	add.s64 	%rd288, %rd48, %rd244;

$L__BB7_64:
	// begin inline asm
	ld.global.nc.u32 %r466, [%rd288];
	// end inline asm
	mov.b32 	%f139, %r466;
	cvt.f64.f32 	%fd438, %f139;
	add.f64 	%fd439, %fd532, %fd438;
	add.s64 	%rd246, %rd288, 1024;
	// begin inline asm
	ld.global.nc.u32 %r467, [%rd246];
	// end inline asm
	mov.b32 	%f140, %r467;
	cvt.f64.f32 	%fd440, %f140;
	add.f64 	%fd441, %fd439, %fd440;
	add.s64 	%rd247, %rd288, 2048;
	// begin inline asm
	ld.global.nc.u32 %r468, [%rd247];
	// end inline asm
	mov.b32 	%f141, %r468;
	cvt.f64.f32 	%fd442, %f141;
	add.f64 	%fd443, %fd441, %fd442;
	add.s64 	%rd248, %rd288, 3072;
	// begin inline asm
	ld.global.nc.u32 %r469, [%rd248];
	// end inline asm
	mov.b32 	%f142, %r469;
	cvt.f64.f32 	%fd444, %f142;
	add.f64 	%fd532, %fd443, %fd444;
	add.s64 	%rd288, %rd288, 4096;
	add.s32 	%r612, %r612, 1024;
	setp.lt.s32 	%p71, %r612, %r43;
	@%p71 bra 	$L__BB7_64;

$L__BB7_65:
	mov.u32 	%r471, %tid.x;
	shr.s32 	%r472, %r471, 31;
	shr.u32 	%r473, %r472, 27;
	add.s32 	%r474, %r471, %r473;
	shr.s32 	%r56, %r474, 5;
	// begin inline asm
	mov.u32 %r470, %laneid;
	// end inline asm
	mov.b64 	%rd249, %fd532;
	mov.b64 	{%r58, %r59}, %rd249;
	shl.b32 	%r475, %r56, 3;
	mov.u32 	%r476, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r477, %r476, %r475;
	setp.gt.s32 	%p72, %r43, 255;
	@%p72 bra 	$L__BB7_71;
	bra.uni 	$L__BB7_66;

$L__BB7_71:
	setp.ne.s32 	%p88, %r470, 0;
	// begin inline asm
	mov.u32 %r538, %laneid;
	// end inline asm
	mov.u32 	%r546, 1;
	mov.u32 	%r587, 31;
	mov.u32 	%r588, -1;
	// begin inline asm
	shfl.sync.down.b32 %r539, %r58, %r546, %r587, %r588;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r544, %r59, %r546, %r587, %r588;
	// end inline asm
	mov.b64 	%rd259, {%r539, %r544};
	mov.b64 	%fd477, %rd259;
	setp.gt.s32 	%p89, %r538, 30;
	add.f64 	%fd478, %fd532, %fd477;
	selp.f64 	%fd479, %fd532, %fd478, %p89;
	mov.b64 	%rd260, %fd479;
	mov.u32 	%r556, 2;
	mov.b64 	{%r550, %r555}, %rd260;
	// begin inline asm
	shfl.sync.down.b32 %r549, %r550, %r556, %r587, %r588;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r554, %r555, %r556, %r587, %r588;
	// end inline asm
	mov.b64 	%rd261, {%r549, %r554};
	mov.b64 	%fd480, %rd261;
	setp.gt.s32 	%p90, %r538, 29;
	add.f64 	%fd481, %fd479, %fd480;
	selp.f64 	%fd482, %fd479, %fd481, %p90;
	mov.b64 	%rd262, %fd482;
	mov.u32 	%r566, 4;
	mov.b64 	{%r560, %r565}, %rd262;
	// begin inline asm
	shfl.sync.down.b32 %r559, %r560, %r566, %r587, %r588;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r564, %r565, %r566, %r587, %r588;
	// end inline asm
	mov.b64 	%rd263, {%r559, %r564};
	mov.b64 	%fd483, %rd263;
	setp.gt.s32 	%p91, %r538, 27;
	add.f64 	%fd484, %fd482, %fd483;
	selp.f64 	%fd485, %fd482, %fd484, %p91;
	mov.b64 	%rd264, %fd485;
	mov.u32 	%r576, 8;
	mov.b64 	{%r570, %r575}, %rd264;
	// begin inline asm
	shfl.sync.down.b32 %r569, %r570, %r576, %r587, %r588;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r574, %r575, %r576, %r587, %r588;
	// end inline asm
	mov.b64 	%rd265, {%r569, %r574};
	mov.b64 	%fd486, %rd265;
	setp.gt.s32 	%p92, %r538, 23;
	add.f64 	%fd487, %fd485, %fd486;
	selp.f64 	%fd488, %fd485, %fd487, %p92;
	mov.b64 	%rd266, %fd488;
	mov.u32 	%r586, 16;
	mov.b64 	{%r580, %r585}, %rd266;
	// begin inline asm
	shfl.sync.down.b32 %r579, %r580, %r586, %r587, %r588;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r584, %r585, %r586, %r587, %r588;
	// end inline asm
	mov.b64 	%rd267, {%r579, %r584};
	mov.b64 	%fd489, %rd267;
	setp.gt.s32 	%p93, %r538, 15;
	add.f64 	%fd490, %fd488, %fd489;
	selp.f64 	%fd533, %fd488, %fd490, %p93;
	@%p88 bra 	$L__BB7_73;

	add.s32 	%r595, %r477, 8;
	st.shared.f64 	[%r595], %fd533;

$L__BB7_73:
	bar.sync 	0;
	setp.ne.s32 	%p94, %r471, 0;
	@%p94 bra 	$L__BB7_75;

	ld.shared.f64 	%fd491, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd492, %fd533, %fd491;
	ld.shared.f64 	%fd493, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd494, %fd492, %fd493;
	ld.shared.f64 	%fd495, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd496, %fd494, %fd495;
	ld.shared.f64 	%fd497, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd498, %fd496, %fd497;
	ld.shared.f64 	%fd499, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd500, %fd498, %fd499;
	ld.shared.f64 	%fd501, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd502, %fd500, %fd501;
	ld.shared.f64 	%fd503, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd502, %fd503;
	bra.uni 	$L__BB7_75;

$L__BB7_3:
	mov.u32 	%r81, %tid.x;
	cvt.s64.s32 	%rd1, %r81;
	mul.wide.s32 	%rd73, %r81, 4;
	add.s64 	%rd270, %rd48, %rd73;
	// begin inline asm
	ld.global.nc.u32 %r61, [%rd270];
	// end inline asm
	mov.b32 	%f1, %r61;
	cvt.f64.f32 	%fd60, %f1;
	add.s64 	%rd53, %rd270, 1024;
	// begin inline asm
	ld.global.nc.u32 %r62, [%rd53];
	// end inline asm
	mov.b32 	%f2, %r62;
	cvt.f64.f32 	%fd61, %f2;
	add.s64 	%rd54, %rd270, 2048;
	// begin inline asm
	ld.global.nc.u32 %r63, [%rd54];
	// end inline asm
	mov.b32 	%f3, %r63;
	cvt.f64.f32 	%fd62, %f3;
	add.s64 	%rd55, %rd270, 3072;
	// begin inline asm
	ld.global.nc.u32 %r64, [%rd55];
	// end inline asm
	mov.b32 	%f4, %r64;
	cvt.f64.f32 	%fd63, %f4;
	add.s64 	%rd56, %rd270, 4096;
	// begin inline asm
	ld.global.nc.u32 %r65, [%rd56];
	// end inline asm
	mov.b32 	%f5, %r65;
	cvt.f64.f32 	%fd64, %f5;
	add.s64 	%rd57, %rd270, 5120;
	mov.u64 	%rd273, 5120;
	// begin inline asm
	ld.global.nc.u32 %r66, [%rd57];
	// end inline asm
	mov.b32 	%f6, %r66;
	cvt.f64.f32 	%fd65, %f6;
	add.s64 	%rd58, %rd270, 6144;
	// begin inline asm
	ld.global.nc.u32 %r67, [%rd58];
	// end inline asm
	mov.b32 	%f7, %r67;
	cvt.f64.f32 	%fd66, %f7;
	add.s64 	%rd59, %rd270, 7168;
	// begin inline asm
	ld.global.nc.u32 %r68, [%rd59];
	// end inline asm
	mov.b32 	%f8, %r68;
	cvt.f64.f32 	%fd67, %f8;
	add.s64 	%rd60, %rd270, 8192;
	// begin inline asm
	ld.global.nc.u32 %r69, [%rd60];
	// end inline asm
	mov.b32 	%f9, %r69;
	cvt.f64.f32 	%fd68, %f9;
	add.s64 	%rd61, %rd270, 9216;
	// begin inline asm
	ld.global.nc.u32 %r70, [%rd61];
	// end inline asm
	mov.b32 	%f10, %r70;
	cvt.f64.f32 	%fd69, %f10;
	add.s64 	%rd62, %rd270, 10240;
	// begin inline asm
	ld.global.nc.u32 %r71, [%rd62];
	// end inline asm
	mov.b32 	%f11, %r71;
	cvt.f64.f32 	%fd70, %f11;
	add.s64 	%rd63, %rd270, 11264;
	// begin inline asm
	ld.global.nc.u32 %r72, [%rd63];
	// end inline asm
	mov.b32 	%f12, %r72;
	cvt.f64.f32 	%fd71, %f12;
	add.s64 	%rd64, %rd270, 12288;
	// begin inline asm
	ld.global.nc.u32 %r73, [%rd64];
	// end inline asm
	mov.b32 	%f13, %r73;
	cvt.f64.f32 	%fd72, %f13;
	add.s64 	%rd65, %rd270, 13312;
	// begin inline asm
	ld.global.nc.u32 %r74, [%rd65];
	// end inline asm
	mov.b32 	%f14, %r74;
	cvt.f64.f32 	%fd73, %f14;
	add.s64 	%rd66, %rd270, 14336;
	// begin inline asm
	ld.global.nc.u32 %r75, [%rd66];
	// end inline asm
	mov.b32 	%f15, %r75;
	cvt.f64.f32 	%fd74, %f15;
	add.s64 	%rd67, %rd270, 15360;
	// begin inline asm
	ld.global.nc.u32 %r76, [%rd67];
	// end inline asm
	mov.b32 	%f16, %r76;
	cvt.f64.f32 	%fd75, %f16;
	add.s64 	%rd68, %rd270, 16384;
	// begin inline asm
	ld.global.nc.u32 %r77, [%rd68];
	// end inline asm
	mov.b32 	%f17, %r77;
	cvt.f64.f32 	%fd76, %f17;
	add.s64 	%rd69, %rd270, 17408;
	// begin inline asm
	ld.global.nc.u32 %r78, [%rd69];
	// end inline asm
	mov.b32 	%f18, %r78;
	cvt.f64.f32 	%fd77, %f18;
	add.s64 	%rd70, %rd270, 18432;
	// begin inline asm
	ld.global.nc.u32 %r79, [%rd70];
	// end inline asm
	mov.b32 	%f19, %r79;
	cvt.f64.f32 	%fd78, %f19;
	add.s64 	%rd71, %rd270, 19456;
	// begin inline asm
	ld.global.nc.u32 %r80, [%rd71];
	// end inline asm
	mov.b32 	%f20, %r80;
	cvt.f64.f32 	%fd79, %f20;
	add.f64 	%fd80, %fd60, %fd61;
	add.f64 	%fd81, %fd80, %fd62;
	add.f64 	%fd82, %fd81, %fd63;
	add.f64 	%fd83, %fd82, %fd64;
	add.f64 	%fd84, %fd83, %fd65;
	add.f64 	%fd85, %fd84, %fd66;
	add.f64 	%fd86, %fd85, %fd67;
	add.f64 	%fd87, %fd86, %fd68;
	add.f64 	%fd88, %fd87, %fd69;
	add.f64 	%fd89, %fd88, %fd70;
	add.f64 	%fd90, %fd89, %fd71;
	add.f64 	%fd91, %fd90, %fd72;
	add.f64 	%fd92, %fd91, %fd73;
	add.f64 	%fd93, %fd92, %fd74;
	add.f64 	%fd94, %fd93, %fd75;
	add.f64 	%fd95, %fd94, %fd76;
	add.f64 	%fd96, %fd95, %fd77;
	add.f64 	%fd97, %fd96, %fd78;
	add.f64 	%fd511, %fd97, %fd79;
	setp.lt.s64 	%p4, %rd50, 10240;
	@%p4 bra 	$L__BB7_6;

	mov.u64 	%rd272, 10240;

$L__BB7_5:
	add.s64 	%rd76, %rd270, 20480;
	// begin inline asm
	ld.global.nc.u32 %r82, [%rd76];
	// end inline asm
	mov.b32 	%f21, %r82;
	cvt.f64.f32 	%fd98, %f21;
	add.s64 	%rd77, %rd270, 21504;
	// begin inline asm
	ld.global.nc.u32 %r83, [%rd77];
	// end inline asm
	mov.b32 	%f22, %r83;
	cvt.f64.f32 	%fd99, %f22;
	add.s64 	%rd78, %rd270, 22528;
	// begin inline asm
	ld.global.nc.u32 %r84, [%rd78];
	// end inline asm
	mov.b32 	%f23, %r84;
	cvt.f64.f32 	%fd100, %f23;
	add.s64 	%rd79, %rd270, 23552;
	// begin inline asm
	ld.global.nc.u32 %r85, [%rd79];
	// end inline asm
	mov.b32 	%f24, %r85;
	cvt.f64.f32 	%fd101, %f24;
	add.s64 	%rd80, %rd270, 24576;
	// begin inline asm
	ld.global.nc.u32 %r86, [%rd80];
	// end inline asm
	mov.b32 	%f25, %r86;
	cvt.f64.f32 	%fd102, %f25;
	add.s64 	%rd81, %rd270, 25600;
	// begin inline asm
	ld.global.nc.u32 %r87, [%rd81];
	// end inline asm
	mov.b32 	%f26, %r87;
	cvt.f64.f32 	%fd103, %f26;
	add.s64 	%rd82, %rd270, 26624;
	// begin inline asm
	ld.global.nc.u32 %r88, [%rd82];
	// end inline asm
	mov.b32 	%f27, %r88;
	cvt.f64.f32 	%fd104, %f27;
	add.s64 	%rd83, %rd270, 27648;
	// begin inline asm
	ld.global.nc.u32 %r89, [%rd83];
	// end inline asm
	mov.b32 	%f28, %r89;
	cvt.f64.f32 	%fd105, %f28;
	add.s64 	%rd84, %rd270, 28672;
	// begin inline asm
	ld.global.nc.u32 %r90, [%rd84];
	// end inline asm
	mov.b32 	%f29, %r90;
	cvt.f64.f32 	%fd106, %f29;
	add.s64 	%rd85, %rd270, 29696;
	// begin inline asm
	ld.global.nc.u32 %r91, [%rd85];
	// end inline asm
	mov.b32 	%f30, %r91;
	cvt.f64.f32 	%fd107, %f30;
	add.s64 	%rd86, %rd270, 30720;
	// begin inline asm
	ld.global.nc.u32 %r92, [%rd86];
	// end inline asm
	mov.b32 	%f31, %r92;
	cvt.f64.f32 	%fd108, %f31;
	add.s64 	%rd87, %rd270, 31744;
	// begin inline asm
	ld.global.nc.u32 %r93, [%rd87];
	// end inline asm
	mov.b32 	%f32, %r93;
	cvt.f64.f32 	%fd109, %f32;
	add.s64 	%rd88, %rd270, 32768;
	// begin inline asm
	ld.global.nc.u32 %r94, [%rd88];
	// end inline asm
	mov.b32 	%f33, %r94;
	cvt.f64.f32 	%fd110, %f33;
	add.s64 	%rd89, %rd270, 33792;
	// begin inline asm
	ld.global.nc.u32 %r95, [%rd89];
	// end inline asm
	mov.b32 	%f34, %r95;
	cvt.f64.f32 	%fd111, %f34;
	add.s64 	%rd90, %rd270, 34816;
	// begin inline asm
	ld.global.nc.u32 %r96, [%rd90];
	// end inline asm
	mov.b32 	%f35, %r96;
	cvt.f64.f32 	%fd112, %f35;
	add.s64 	%rd91, %rd270, 35840;
	// begin inline asm
	ld.global.nc.u32 %r97, [%rd91];
	// end inline asm
	mov.b32 	%f36, %r97;
	cvt.f64.f32 	%fd113, %f36;
	add.s64 	%rd92, %rd270, 36864;
	// begin inline asm
	ld.global.nc.u32 %r98, [%rd92];
	// end inline asm
	mov.b32 	%f37, %r98;
	cvt.f64.f32 	%fd114, %f37;
	add.s64 	%rd93, %rd270, 37888;
	// begin inline asm
	ld.global.nc.u32 %r99, [%rd93];
	// end inline asm
	mov.b32 	%f38, %r99;
	cvt.f64.f32 	%fd115, %f38;
	add.s64 	%rd94, %rd270, 38912;
	// begin inline asm
	ld.global.nc.u32 %r100, [%rd94];
	// end inline asm
	mov.b32 	%f39, %r100;
	cvt.f64.f32 	%fd116, %f39;
	add.s64 	%rd95, %rd270, 39936;
	// begin inline asm
	ld.global.nc.u32 %r101, [%rd95];
	// end inline asm
	mov.b32 	%f40, %r101;
	cvt.f64.f32 	%fd117, %f40;
	add.f64 	%fd118, %fd511, %fd98;
	add.f64 	%fd119, %fd118, %fd99;
	add.f64 	%fd120, %fd119, %fd100;
	add.f64 	%fd121, %fd120, %fd101;
	add.f64 	%fd122, %fd121, %fd102;
	add.f64 	%fd123, %fd122, %fd103;
	add.f64 	%fd124, %fd123, %fd104;
	add.f64 	%fd125, %fd124, %fd105;
	add.f64 	%fd126, %fd125, %fd106;
	add.f64 	%fd127, %fd126, %fd107;
	add.f64 	%fd128, %fd127, %fd108;
	add.f64 	%fd129, %fd128, %fd109;
	add.f64 	%fd130, %fd129, %fd110;
	add.f64 	%fd131, %fd130, %fd111;
	add.f64 	%fd132, %fd131, %fd112;
	add.f64 	%fd133, %fd132, %fd113;
	add.f64 	%fd134, %fd133, %fd114;
	add.f64 	%fd135, %fd134, %fd115;
	add.f64 	%fd136, %fd135, %fd116;
	add.f64 	%fd511, %fd136, %fd117;
	add.s64 	%rd273, %rd273, 5120;
	add.s64 	%rd272, %rd272, 5120;
	setp.le.s64 	%p5, %rd272, %rd50;
	mov.u64 	%rd270, %rd76;
	@%p5 bra 	$L__BB7_5;

$L__BB7_6:
	setp.ge.s64 	%p6, %rd273, %rd50;
	@%p6 bra 	$L__BB7_14;

	sub.s64 	%rd96, %rd50, %rd273;
	cvt.u32.u64 	%r1, %rd96;
	setp.ge.s32 	%p7, %r81, %r1;
	@%p7 bra 	$L__BB7_14;

	cvt.u32.u64 	%r103, %rd50;
	not.b32 	%r104, %r81;
	add.s32 	%r105, %r104, %r103;
	cvt.u32.u64 	%r106, %rd273;
	sub.s32 	%r3, %r105, %r106;
	shr.u32 	%r107, %r3, 8;
	add.s32 	%r108, %r107, 1;
	and.b32  	%r597, %r108, 3;
	setp.eq.s32 	%p8, %r597, 0;
	mov.u32 	%r598, %r81;
	@%p8 bra 	$L__BB7_11;

	add.s64 	%rd97, %rd273, %rd1;
	shl.b64 	%rd98, %rd97, 2;
	add.s64 	%rd274, %rd48, %rd98;
	mov.u32 	%r598, %tid.x;

$L__BB7_10:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r109, [%rd274];
	// end inline asm
	mov.b32 	%f41, %r109;
	cvt.f64.f32 	%fd138, %f41;
	add.f64 	%fd511, %fd511, %fd138;
	add.s32 	%r598, %r598, 256;
	add.s64 	%rd274, %rd274, 1024;
	add.s32 	%r597, %r597, -1;
	setp.ne.s32 	%p9, %r597, 0;
	@%p9 bra 	$L__BB7_10;

$L__BB7_11:
	setp.lt.u32 	%p10, %r3, 768;
	@%p10 bra 	$L__BB7_14;

	cvt.s64.s32 	%rd100, %r598;
	add.s64 	%rd101, %rd273, %rd100;
	shl.b64 	%rd102, %rd101, 2;
	add.s64 	%rd275, %rd48, %rd102;

$L__BB7_13:
	// begin inline asm
	ld.global.nc.u32 %r110, [%rd275];
	// end inline asm
	mov.b32 	%f42, %r110;
	cvt.f64.f32 	%fd139, %f42;
	add.f64 	%fd140, %fd511, %fd139;
	add.s64 	%rd104, %rd275, 1024;
	// begin inline asm
	ld.global.nc.u32 %r111, [%rd104];
	// end inline asm
	mov.b32 	%f43, %r111;
	cvt.f64.f32 	%fd141, %f43;
	add.f64 	%fd142, %fd140, %fd141;
	add.s64 	%rd105, %rd275, 2048;
	// begin inline asm
	ld.global.nc.u32 %r112, [%rd105];
	// end inline asm
	mov.b32 	%f44, %r112;
	cvt.f64.f32 	%fd143, %f44;
	add.f64 	%fd144, %fd142, %fd143;
	add.s64 	%rd106, %rd275, 3072;
	// begin inline asm
	ld.global.nc.u32 %r113, [%rd106];
	// end inline asm
	mov.b32 	%f45, %r113;
	cvt.f64.f32 	%fd145, %f45;
	add.f64 	%fd511, %fd144, %fd145;
	add.s64 	%rd275, %rd275, 4096;
	add.s32 	%r598, %r598, 1024;
	setp.lt.s32 	%p11, %r598, %r1;
	@%p11 bra 	$L__BB7_13;

$L__BB7_14:
	// begin inline asm
	mov.u32 %r114, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r115, %laneid;
	// end inline asm
	mov.b64 	%rd107, %fd511;
	mov.u32 	%r123, 1;
	mov.u32 	%r164, 31;
	mov.u32 	%r165, -1;
	mov.b64 	{%r117, %r122}, %rd107;
	// begin inline asm
	shfl.sync.down.b32 %r116, %r117, %r123, %r164, %r165;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r121, %r122, %r123, %r164, %r165;
	// end inline asm
	mov.b64 	%rd108, {%r116, %r121};
	mov.b64 	%fd146, %rd108;
	setp.gt.s32 	%p12, %r115, 30;
	add.f64 	%fd147, %fd511, %fd146;
	selp.f64 	%fd148, %fd511, %fd147, %p12;
	mov.b64 	%rd109, %fd148;
	mov.u32 	%r133, 2;
	mov.b64 	{%r127, %r132}, %rd109;
	// begin inline asm
	shfl.sync.down.b32 %r126, %r127, %r133, %r164, %r165;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r131, %r132, %r133, %r164, %r165;
	// end inline asm
	mov.b64 	%rd110, {%r126, %r131};
	mov.b64 	%fd149, %rd110;
	setp.gt.s32 	%p13, %r115, 29;
	add.f64 	%fd150, %fd148, %fd149;
	selp.f64 	%fd151, %fd148, %fd150, %p13;
	mov.b64 	%rd111, %fd151;
	mov.u32 	%r143, 4;
	mov.b64 	{%r137, %r142}, %rd111;
	// begin inline asm
	shfl.sync.down.b32 %r136, %r137, %r143, %r164, %r165;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r141, %r142, %r143, %r164, %r165;
	// end inline asm
	mov.b64 	%rd112, {%r136, %r141};
	mov.b64 	%fd152, %rd112;
	setp.gt.s32 	%p14, %r115, 27;
	add.f64 	%fd153, %fd151, %fd152;
	selp.f64 	%fd154, %fd151, %fd153, %p14;
	mov.b64 	%rd113, %fd154;
	mov.u32 	%r153, 8;
	mov.b64 	{%r147, %r152}, %rd113;
	// begin inline asm
	shfl.sync.down.b32 %r146, %r147, %r153, %r164, %r165;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r151, %r152, %r153, %r164, %r165;
	// end inline asm
	mov.b64 	%rd114, {%r146, %r151};
	mov.b64 	%fd155, %rd114;
	setp.gt.s32 	%p15, %r115, 23;
	add.f64 	%fd156, %fd154, %fd155;
	selp.f64 	%fd157, %fd154, %fd156, %p15;
	mov.b64 	%rd115, %fd157;
	mov.u32 	%r163, 16;
	mov.b64 	{%r157, %r162}, %rd115;
	// begin inline asm
	shfl.sync.down.b32 %r156, %r157, %r163, %r164, %r165;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r161, %r162, %r163, %r164, %r165;
	// end inline asm
	mov.b64 	%rd116, {%r156, %r161};
	mov.b64 	%fd158, %rd116;
	setp.gt.s32 	%p16, %r115, 15;
	add.f64 	%fd159, %fd157, %fd158;
	selp.f64 	%fd533, %fd157, %fd159, %p16;
	setp.ne.s32 	%p17, %r114, 0;
	@%p17 bra 	$L__BB7_16;

	shr.s32 	%r167, %r81, 31;
	shr.u32 	%r168, %r167, 27;
	add.s32 	%r169, %r81, %r168;
	shr.s32 	%r170, %r169, 5;
	shl.b32 	%r171, %r170, 3;
	mov.u32 	%r172, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r173, %r172, %r171;
	st.shared.f64 	[%r173+8], %fd533;

$L__BB7_16:
	bar.sync 	0;
	setp.ne.s32 	%p18, %r81, 0;
	@%p18 bra 	$L__BB7_75;

	ld.shared.f64 	%fd160, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd161, %fd533, %fd160;
	ld.shared.f64 	%fd162, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd163, %fd161, %fd162;
	ld.shared.f64 	%fd164, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd165, %fd163, %fd164;
	ld.shared.f64 	%fd166, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd167, %fd165, %fd166;
	ld.shared.f64 	%fd168, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd169, %fd167, %fd168;
	ld.shared.f64 	%fd170, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd171, %fd169, %fd170;
	ld.shared.f64 	%fd172, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd171, %fd172;
	bra.uni 	$L__BB7_75;

$L__BB7_38:
	mov.u32 	%r305, %tid.x;
	shl.b32 	%r306, %r305, 2;
	mul.wide.u32 	%rd161, %r306, 4;
	add.s64 	%rd147, %rd48, %rd161;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd145, %rd146}, [%rd147];
	// end inline asm
	mov.b64 	{%r307, %r308}, %rd146;
	mov.b64 	{%r309, %r310}, %rd145;
	mov.b32 	%f52, %r309;
	mov.b32 	%f53, %r310;
	mov.b32 	%f54, %r307;
	mov.b32 	%f55, %r308;
	add.s64 	%rd150, %rd147, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd148, %rd149}, [%rd150];
	// end inline asm
	mov.b64 	{%r311, %r312}, %rd149;
	mov.b64 	{%r313, %r314}, %rd148;
	mov.b32 	%f56, %r313;
	mov.b32 	%f57, %r314;
	mov.b32 	%f58, %r311;
	mov.b32 	%f59, %r312;
	add.s64 	%rd153, %rd147, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd151, %rd152}, [%rd153];
	// end inline asm
	mov.b64 	{%r315, %r316}, %rd152;
	mov.b64 	{%r317, %r318}, %rd151;
	mov.b32 	%f60, %r317;
	mov.b32 	%f61, %r318;
	mov.b32 	%f62, %r315;
	mov.b32 	%f63, %r316;
	add.s64 	%rd156, %rd147, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd154, %rd155}, [%rd156];
	// end inline asm
	mov.b64 	{%r319, %r320}, %rd155;
	mov.b64 	{%r321, %r322}, %rd154;
	mov.b32 	%f64, %r321;
	mov.b32 	%f65, %r322;
	mov.b32 	%f66, %r319;
	mov.b32 	%f67, %r320;
	add.s64 	%rd159, %rd147, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd157, %rd158}, [%rd159];
	// end inline asm
	mov.b64 	{%r323, %r324}, %rd158;
	mov.b64 	{%r325, %r326}, %rd157;
	mov.b32 	%f68, %r325;
	mov.b32 	%f69, %r326;
	mov.b32 	%f70, %r323;
	mov.b32 	%f71, %r324;
	cvt.f64.f32 	%fd242, %f52;
	cvt.f64.f32 	%fd243, %f53;
	cvt.f64.f32 	%fd244, %f54;
	cvt.f64.f32 	%fd245, %f55;
	cvt.f64.f32 	%fd246, %f56;
	cvt.f64.f32 	%fd247, %f57;
	cvt.f64.f32 	%fd248, %f58;
	cvt.f64.f32 	%fd249, %f59;
	cvt.f64.f32 	%fd250, %f60;
	cvt.f64.f32 	%fd251, %f61;
	cvt.f64.f32 	%fd252, %f62;
	cvt.f64.f32 	%fd253, %f63;
	cvt.f64.f32 	%fd254, %f64;
	cvt.f64.f32 	%fd255, %f65;
	cvt.f64.f32 	%fd256, %f66;
	cvt.f64.f32 	%fd257, %f67;
	cvt.f64.f32 	%fd258, %f68;
	cvt.f64.f32 	%fd259, %f69;
	cvt.f64.f32 	%fd260, %f70;
	cvt.f64.f32 	%fd261, %f71;
	add.f64 	%fd262, %fd242, %fd243;
	add.f64 	%fd263, %fd262, %fd244;
	add.f64 	%fd264, %fd263, %fd245;
	add.f64 	%fd265, %fd264, %fd246;
	add.f64 	%fd266, %fd265, %fd247;
	add.f64 	%fd267, %fd266, %fd248;
	add.f64 	%fd268, %fd267, %fd249;
	add.f64 	%fd269, %fd268, %fd250;
	add.f64 	%fd270, %fd269, %fd251;
	add.f64 	%fd271, %fd270, %fd252;
	add.f64 	%fd272, %fd271, %fd253;
	add.f64 	%fd273, %fd272, %fd254;
	add.f64 	%fd274, %fd273, %fd255;
	add.f64 	%fd275, %fd274, %fd256;
	add.f64 	%fd276, %fd275, %fd257;
	add.f64 	%fd277, %fd276, %fd258;
	add.f64 	%fd278, %fd277, %fd259;
	add.f64 	%fd279, %fd278, %fd260;
	add.f64 	%fd526, %fd279, %fd261;
	setp.lt.s64 	%p49, %rd50, 10240;
	mov.u64 	%rd281, 5120;
	@%p49 bra 	$L__BB7_44;

	add.s64 	%rd165, %rd50, -10240;
	mul.hi.u64 	%rd166, %rd165, -3689348814741910323;
	shr.u64 	%rd167, %rd166, 12;
	mov.u64 	%rd281, 5120;
	add.s64 	%rd23, %rd167, 1;
	and.b64  	%rd24, %rd23, 1;
	setp.lt.u64 	%p50, %rd165, 5120;
	mov.u64 	%rd282, 10240;
	@%p50 bra 	$L__BB7_42;

	sub.s64 	%rd280, %rd23, %rd24;

$L__BB7_41:
	shl.b64 	%rd200, %rd281, 2;
	add.s64 	%rd172, %rd147, %rd200;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd170, %rd171}, [%rd172];
	// end inline asm
	mov.b64 	{%r327, %r328}, %rd171;
	mov.b64 	{%r329, %r330}, %rd170;
	mov.b32 	%f72, %r329;
	mov.b32 	%f73, %r330;
	mov.b32 	%f74, %r327;
	mov.b32 	%f75, %r328;
	add.s64 	%rd175, %rd172, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd173, %rd174}, [%rd175];
	// end inline asm
	mov.b64 	{%r331, %r332}, %rd174;
	mov.b64 	{%r333, %r334}, %rd173;
	mov.b32 	%f76, %r333;
	mov.b32 	%f77, %r334;
	mov.b32 	%f78, %r331;
	mov.b32 	%f79, %r332;
	add.s64 	%rd178, %rd172, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd176, %rd177}, [%rd178];
	// end inline asm
	mov.b64 	{%r335, %r336}, %rd177;
	mov.b64 	{%r337, %r338}, %rd176;
	mov.b32 	%f80, %r337;
	mov.b32 	%f81, %r338;
	mov.b32 	%f82, %r335;
	mov.b32 	%f83, %r336;
	add.s64 	%rd181, %rd172, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd179, %rd180}, [%rd181];
	// end inline asm
	mov.b64 	{%r339, %r340}, %rd180;
	mov.b64 	{%r341, %r342}, %rd179;
	mov.b32 	%f84, %r341;
	mov.b32 	%f85, %r342;
	mov.b32 	%f86, %r339;
	mov.b32 	%f87, %r340;
	add.s64 	%rd184, %rd172, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd182, %rd183}, [%rd184];
	// end inline asm
	mov.b64 	{%r343, %r344}, %rd183;
	mov.b64 	{%r345, %r346}, %rd182;
	mov.b32 	%f88, %r345;
	mov.b32 	%f89, %r346;
	mov.b32 	%f90, %r343;
	mov.b32 	%f91, %r344;
	cvt.f64.f32 	%fd281, %f72;
	cvt.f64.f32 	%fd282, %f73;
	cvt.f64.f32 	%fd283, %f74;
	cvt.f64.f32 	%fd284, %f75;
	cvt.f64.f32 	%fd285, %f76;
	cvt.f64.f32 	%fd286, %f77;
	cvt.f64.f32 	%fd287, %f78;
	cvt.f64.f32 	%fd288, %f79;
	cvt.f64.f32 	%fd289, %f80;
	cvt.f64.f32 	%fd290, %f81;
	cvt.f64.f32 	%fd291, %f82;
	cvt.f64.f32 	%fd292, %f83;
	cvt.f64.f32 	%fd293, %f84;
	cvt.f64.f32 	%fd294, %f85;
	cvt.f64.f32 	%fd295, %f86;
	cvt.f64.f32 	%fd296, %f87;
	cvt.f64.f32 	%fd297, %f88;
	cvt.f64.f32 	%fd298, %f89;
	cvt.f64.f32 	%fd299, %f90;
	cvt.f64.f32 	%fd300, %f91;
	add.f64 	%fd301, %fd526, %fd281;
	add.f64 	%fd302, %fd301, %fd282;
	add.f64 	%fd303, %fd302, %fd283;
	add.f64 	%fd304, %fd303, %fd284;
	add.f64 	%fd305, %fd304, %fd285;
	add.f64 	%fd306, %fd305, %fd286;
	add.f64 	%fd307, %fd306, %fd287;
	add.f64 	%fd308, %fd307, %fd288;
	add.f64 	%fd309, %fd308, %fd289;
	add.f64 	%fd310, %fd309, %fd290;
	add.f64 	%fd311, %fd310, %fd291;
	add.f64 	%fd312, %fd311, %fd292;
	add.f64 	%fd313, %fd312, %fd293;
	add.f64 	%fd314, %fd313, %fd294;
	add.f64 	%fd315, %fd314, %fd295;
	add.f64 	%fd316, %fd315, %fd296;
	add.f64 	%fd317, %fd316, %fd297;
	add.f64 	%fd318, %fd317, %fd298;
	add.f64 	%fd319, %fd318, %fd299;
	add.f64 	%fd320, %fd319, %fd300;
	add.s64 	%rd281, %rd282, 5120;
	shl.b64 	%rd201, %rd282, 2;
	add.s64 	%rd187, %rd147, %rd201;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd185, %rd186}, [%rd187];
	// end inline asm
	mov.b64 	{%r347, %r348}, %rd186;
	mov.b64 	{%r349, %r350}, %rd185;
	mov.b32 	%f92, %r349;
	mov.b32 	%f93, %r350;
	mov.b32 	%f94, %r347;
	mov.b32 	%f95, %r348;
	add.s64 	%rd190, %rd187, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd188, %rd189}, [%rd190];
	// end inline asm
	mov.b64 	{%r351, %r352}, %rd189;
	mov.b64 	{%r353, %r354}, %rd188;
	mov.b32 	%f96, %r353;
	mov.b32 	%f97, %r354;
	mov.b32 	%f98, %r351;
	mov.b32 	%f99, %r352;
	add.s64 	%rd193, %rd187, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd191, %rd192}, [%rd193];
	// end inline asm
	mov.b64 	{%r355, %r356}, %rd192;
	mov.b64 	{%r357, %r358}, %rd191;
	mov.b32 	%f100, %r357;
	mov.b32 	%f101, %r358;
	mov.b32 	%f102, %r355;
	mov.b32 	%f103, %r356;
	add.s64 	%rd196, %rd187, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd194, %rd195}, [%rd196];
	// end inline asm
	mov.b64 	{%r359, %r360}, %rd195;
	mov.b64 	{%r361, %r362}, %rd194;
	mov.b32 	%f104, %r361;
	mov.b32 	%f105, %r362;
	mov.b32 	%f106, %r359;
	mov.b32 	%f107, %r360;
	add.s64 	%rd199, %rd187, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd197, %rd198}, [%rd199];
	// end inline asm
	mov.b64 	{%r363, %r364}, %rd198;
	mov.b64 	{%r365, %r366}, %rd197;
	mov.b32 	%f108, %r365;
	mov.b32 	%f109, %r366;
	mov.b32 	%f110, %r363;
	mov.b32 	%f111, %r364;
	cvt.f64.f32 	%fd321, %f92;
	cvt.f64.f32 	%fd322, %f93;
	cvt.f64.f32 	%fd323, %f94;
	cvt.f64.f32 	%fd324, %f95;
	cvt.f64.f32 	%fd325, %f96;
	cvt.f64.f32 	%fd326, %f97;
	cvt.f64.f32 	%fd327, %f98;
	cvt.f64.f32 	%fd328, %f99;
	cvt.f64.f32 	%fd329, %f100;
	cvt.f64.f32 	%fd330, %f101;
	cvt.f64.f32 	%fd331, %f102;
	cvt.f64.f32 	%fd332, %f103;
	cvt.f64.f32 	%fd333, %f104;
	cvt.f64.f32 	%fd334, %f105;
	cvt.f64.f32 	%fd335, %f106;
	cvt.f64.f32 	%fd336, %f107;
	cvt.f64.f32 	%fd337, %f108;
	cvt.f64.f32 	%fd338, %f109;
	cvt.f64.f32 	%fd339, %f110;
	cvt.f64.f32 	%fd340, %f111;
	add.f64 	%fd341, %fd320, %fd321;
	add.f64 	%fd342, %fd341, %fd322;
	add.f64 	%fd343, %fd342, %fd323;
	add.f64 	%fd344, %fd343, %fd324;
	add.f64 	%fd345, %fd344, %fd325;
	add.f64 	%fd346, %fd345, %fd326;
	add.f64 	%fd347, %fd346, %fd327;
	add.f64 	%fd348, %fd347, %fd328;
	add.f64 	%fd349, %fd348, %fd329;
	add.f64 	%fd350, %fd349, %fd330;
	add.f64 	%fd351, %fd350, %fd331;
	add.f64 	%fd352, %fd351, %fd332;
	add.f64 	%fd353, %fd352, %fd333;
	add.f64 	%fd354, %fd353, %fd334;
	add.f64 	%fd355, %fd354, %fd335;
	add.f64 	%fd356, %fd355, %fd336;
	add.f64 	%fd357, %fd356, %fd337;
	add.f64 	%fd358, %fd357, %fd338;
	add.f64 	%fd359, %fd358, %fd339;
	add.f64 	%fd526, %fd359, %fd340;
	add.s64 	%rd282, %rd282, 10240;
	add.s64 	%rd280, %rd280, -2;
	setp.ne.s64 	%p51, %rd280, 0;
	@%p51 bra 	$L__BB7_41;

$L__BB7_42:
	setp.eq.s64 	%p52, %rd24, 0;
	@%p52 bra 	$L__BB7_44;

	shl.b64 	%rd217, %rd281, 2;
	add.s64 	%rd204, %rd147, %rd217;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd202, %rd203}, [%rd204];
	// end inline asm
	mov.b64 	{%r367, %r368}, %rd203;
	mov.b64 	{%r369, %r370}, %rd202;
	mov.b32 	%f112, %r369;
	mov.b32 	%f113, %r370;
	mov.b32 	%f114, %r367;
	mov.b32 	%f115, %r368;
	add.s64 	%rd207, %rd204, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd205, %rd206}, [%rd207];
	// end inline asm
	mov.b64 	{%r371, %r372}, %rd206;
	mov.b64 	{%r373, %r374}, %rd205;
	mov.b32 	%f116, %r373;
	mov.b32 	%f117, %r374;
	mov.b32 	%f118, %r371;
	mov.b32 	%f119, %r372;
	add.s64 	%rd210, %rd204, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd208, %rd209}, [%rd210];
	// end inline asm
	mov.b64 	{%r375, %r376}, %rd209;
	mov.b64 	{%r377, %r378}, %rd208;
	mov.b32 	%f120, %r377;
	mov.b32 	%f121, %r378;
	mov.b32 	%f122, %r375;
	mov.b32 	%f123, %r376;
	add.s64 	%rd213, %rd204, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd211, %rd212}, [%rd213];
	// end inline asm
	mov.b64 	{%r379, %r380}, %rd212;
	mov.b64 	{%r381, %r382}, %rd211;
	mov.b32 	%f124, %r381;
	mov.b32 	%f125, %r382;
	mov.b32 	%f126, %r379;
	mov.b32 	%f127, %r380;
	add.s64 	%rd216, %rd204, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd214, %rd215}, [%rd216];
	// end inline asm
	mov.b64 	{%r383, %r384}, %rd215;
	mov.b64 	{%r385, %r386}, %rd214;
	mov.b32 	%f128, %r385;
	mov.b32 	%f129, %r386;
	mov.b32 	%f130, %r383;
	mov.b32 	%f131, %r384;
	cvt.f64.f32 	%fd360, %f112;
	cvt.f64.f32 	%fd361, %f113;
	cvt.f64.f32 	%fd362, %f114;
	cvt.f64.f32 	%fd363, %f115;
	cvt.f64.f32 	%fd364, %f116;
	cvt.f64.f32 	%fd365, %f117;
	cvt.f64.f32 	%fd366, %f118;
	cvt.f64.f32 	%fd367, %f119;
	cvt.f64.f32 	%fd368, %f120;
	cvt.f64.f32 	%fd369, %f121;
	cvt.f64.f32 	%fd370, %f122;
	cvt.f64.f32 	%fd371, %f123;
	cvt.f64.f32 	%fd372, %f124;
	cvt.f64.f32 	%fd373, %f125;
	cvt.f64.f32 	%fd374, %f126;
	cvt.f64.f32 	%fd375, %f127;
	cvt.f64.f32 	%fd376, %f128;
	cvt.f64.f32 	%fd377, %f129;
	cvt.f64.f32 	%fd378, %f130;
	cvt.f64.f32 	%fd379, %f131;
	add.f64 	%fd380, %fd526, %fd360;
	add.f64 	%fd381, %fd380, %fd361;
	add.f64 	%fd382, %fd381, %fd362;
	add.f64 	%fd383, %fd382, %fd363;
	add.f64 	%fd384, %fd383, %fd364;
	add.f64 	%fd385, %fd384, %fd365;
	add.f64 	%fd386, %fd385, %fd366;
	add.f64 	%fd387, %fd386, %fd367;
	add.f64 	%fd388, %fd387, %fd368;
	add.f64 	%fd389, %fd388, %fd369;
	add.f64 	%fd390, %fd389, %fd370;
	add.f64 	%fd391, %fd390, %fd371;
	add.f64 	%fd392, %fd391, %fd372;
	add.f64 	%fd393, %fd392, %fd373;
	add.f64 	%fd394, %fd393, %fd374;
	add.f64 	%fd395, %fd394, %fd375;
	add.f64 	%fd396, %fd395, %fd376;
	add.f64 	%fd397, %fd396, %fd377;
	add.f64 	%fd398, %fd397, %fd378;
	add.f64 	%fd526, %fd398, %fd379;
	mov.u64 	%rd281, %rd282;

$L__BB7_44:
	setp.ge.s64 	%p53, %rd281, %rd50;
	@%p53 bra 	$L__BB7_52;

	sub.s64 	%rd218, %rd50, %rd281;
	cvt.u32.u64 	%r31, %rd218;
	setp.ge.s32 	%p54, %r305, %r31;
	@%p54 bra 	$L__BB7_52;

	cvt.u32.u64 	%r388, %rd50;
	not.b32 	%r389, %r305;
	add.s32 	%r390, %r389, %r388;
	cvt.u32.u64 	%r391, %rd281;
	sub.s32 	%r33, %r390, %r391;
	shr.u32 	%r392, %r33, 8;
	add.s32 	%r393, %r392, 1;
	and.b32  	%r606, %r393, 3;
	setp.eq.s32 	%p55, %r606, 0;
	mov.u32 	%r607, %r305;
	@%p55 bra 	$L__BB7_49;

	mov.u32 	%r607, %tid.x;
	cvt.s64.s32 	%rd219, %r607;
	add.s64 	%rd220, %rd281, %rd219;
	shl.b64 	%rd221, %rd220, 2;
	add.s64 	%rd285, %rd48, %rd221;

$L__BB7_48:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r394, [%rd285];
	// end inline asm
	mov.b32 	%f132, %r394;
	cvt.f64.f32 	%fd400, %f132;
	add.f64 	%fd526, %fd526, %fd400;
	add.s32 	%r607, %r607, 256;
	add.s64 	%rd285, %rd285, 1024;
	add.s32 	%r606, %r606, -1;
	setp.ne.s32 	%p56, %r606, 0;
	@%p56 bra 	$L__BB7_48;

$L__BB7_49:
	setp.lt.u32 	%p57, %r33, 768;
	@%p57 bra 	$L__BB7_52;

	cvt.s64.s32 	%rd223, %r607;
	add.s64 	%rd224, %rd281, %rd223;
	shl.b64 	%rd225, %rd224, 2;
	add.s64 	%rd286, %rd48, %rd225;

$L__BB7_51:
	// begin inline asm
	ld.global.nc.u32 %r395, [%rd286];
	// end inline asm
	mov.b32 	%f133, %r395;
	cvt.f64.f32 	%fd401, %f133;
	add.f64 	%fd402, %fd526, %fd401;
	add.s64 	%rd227, %rd286, 1024;
	// begin inline asm
	ld.global.nc.u32 %r396, [%rd227];
	// end inline asm
	mov.b32 	%f134, %r396;
	cvt.f64.f32 	%fd403, %f134;
	add.f64 	%fd404, %fd402, %fd403;
	add.s64 	%rd228, %rd286, 2048;
	// begin inline asm
	ld.global.nc.u32 %r397, [%rd228];
	// end inline asm
	mov.b32 	%f135, %r397;
	cvt.f64.f32 	%fd405, %f135;
	add.f64 	%fd406, %fd404, %fd405;
	add.s64 	%rd229, %rd286, 3072;
	// begin inline asm
	ld.global.nc.u32 %r398, [%rd229];
	// end inline asm
	mov.b32 	%f136, %r398;
	cvt.f64.f32 	%fd407, %f136;
	add.f64 	%fd526, %fd406, %fd407;
	add.s64 	%rd286, %rd286, 4096;
	add.s32 	%r607, %r607, 1024;
	setp.lt.s32 	%p58, %r607, %r31;
	@%p58 bra 	$L__BB7_51;

$L__BB7_52:
	// begin inline asm
	mov.u32 %r399, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r400, %laneid;
	// end inline asm
	mov.b64 	%rd230, %fd526;
	mov.u32 	%r408, 1;
	mov.u32 	%r449, 31;
	mov.u32 	%r450, -1;
	mov.b64 	{%r402, %r407}, %rd230;
	// begin inline asm
	shfl.sync.down.b32 %r401, %r402, %r408, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r406, %r407, %r408, %r449, %r450;
	// end inline asm
	mov.b64 	%rd231, {%r401, %r406};
	mov.b64 	%fd408, %rd231;
	setp.gt.s32 	%p59, %r400, 30;
	add.f64 	%fd409, %fd526, %fd408;
	selp.f64 	%fd410, %fd526, %fd409, %p59;
	mov.b64 	%rd232, %fd410;
	mov.u32 	%r418, 2;
	mov.b64 	{%r412, %r417}, %rd232;
	// begin inline asm
	shfl.sync.down.b32 %r411, %r412, %r418, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r416, %r417, %r418, %r449, %r450;
	// end inline asm
	mov.b64 	%rd233, {%r411, %r416};
	mov.b64 	%fd411, %rd233;
	setp.gt.s32 	%p60, %r400, 29;
	add.f64 	%fd412, %fd410, %fd411;
	selp.f64 	%fd413, %fd410, %fd412, %p60;
	mov.b64 	%rd234, %fd413;
	mov.u32 	%r428, 4;
	mov.b64 	{%r422, %r427}, %rd234;
	// begin inline asm
	shfl.sync.down.b32 %r421, %r422, %r428, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r426, %r427, %r428, %r449, %r450;
	// end inline asm
	mov.b64 	%rd235, {%r421, %r426};
	mov.b64 	%fd414, %rd235;
	setp.gt.s32 	%p61, %r400, 27;
	add.f64 	%fd415, %fd413, %fd414;
	selp.f64 	%fd416, %fd413, %fd415, %p61;
	mov.b64 	%rd236, %fd416;
	mov.u32 	%r438, 8;
	mov.b64 	{%r432, %r437}, %rd236;
	// begin inline asm
	shfl.sync.down.b32 %r431, %r432, %r438, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r436, %r437, %r438, %r449, %r450;
	// end inline asm
	mov.b64 	%rd237, {%r431, %r436};
	mov.b64 	%fd417, %rd237;
	setp.gt.s32 	%p62, %r400, 23;
	add.f64 	%fd418, %fd416, %fd417;
	selp.f64 	%fd419, %fd416, %fd418, %p62;
	mov.b64 	%rd238, %fd419;
	mov.u32 	%r448, 16;
	mov.b64 	{%r442, %r447}, %rd238;
	// begin inline asm
	shfl.sync.down.b32 %r441, %r442, %r448, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r446, %r447, %r448, %r449, %r450;
	// end inline asm
	mov.b64 	%rd239, {%r441, %r446};
	mov.b64 	%fd420, %rd239;
	setp.gt.s32 	%p63, %r400, 15;
	add.f64 	%fd421, %fd419, %fd420;
	selp.f64 	%fd533, %fd419, %fd421, %p63;
	setp.ne.s32 	%p64, %r399, 0;
	@%p64 bra 	$L__BB7_54;

	shr.s32 	%r452, %r305, 31;
	shr.u32 	%r453, %r452, 27;
	add.s32 	%r454, %r305, %r453;
	shr.s32 	%r455, %r454, 5;
	shl.b32 	%r456, %r455, 3;
	mov.u32 	%r457, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r458, %r457, %r456;
	st.shared.f64 	[%r458+8], %fd533;

$L__BB7_54:
	bar.sync 	0;
	setp.ne.s32 	%p65, %r305, 0;
	@%p65 bra 	$L__BB7_75;

	ld.shared.f64 	%fd422, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd423, %fd533, %fd422;
	ld.shared.f64 	%fd424, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd425, %fd423, %fd424;
	ld.shared.f64 	%fd426, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd427, %fd425, %fd426;
	ld.shared.f64 	%fd428, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd429, %fd427, %fd428;
	ld.shared.f64 	%fd430, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd431, %fd429, %fd430;
	ld.shared.f64 	%fd432, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd433, %fd431, %fd432;
	ld.shared.f64 	%fd434, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd433, %fd434;
	bra.uni 	$L__BB7_75;

$L__BB7_28:
	setp.ne.s32 	%p26, %r185, 0;
	shl.b32 	%r244, %r26, 5;
	add.s32 	%r245, %r244, 32;
	setp.gt.s32 	%p27, %r245, %r13;
	// begin inline asm
	mov.u32 %r193, %laneid;
	// end inline asm
	not.b32 	%r246, %r244;
	mov.u32 	%r243, -1;
	add.s32 	%r247, %r13, %r246;
	selp.b32 	%r242, %r247, 31, %p27;
	mov.u32 	%r201, 1;
	// begin inline asm
	shfl.sync.down.b32 %r194, %r28, %r201, %r242, %r243;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r199, %r29, %r201, %r242, %r243;
	// end inline asm
	mov.b64 	%rd127, {%r194, %r199};
	mov.b64 	%fd183, %rd127;
	setp.lt.s32 	%p28, %r193, %r242;
	add.f64 	%fd184, %fd517, %fd183;
	selp.f64 	%fd185, %fd184, %fd517, %p28;
	mov.b64 	%rd128, %fd185;
	mov.u32 	%r211, 2;
	mov.b64 	{%r205, %r210}, %rd128;
	// begin inline asm
	shfl.sync.down.b32 %r204, %r205, %r211, %r242, %r243;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r209, %r210, %r211, %r242, %r243;
	// end inline asm
	mov.b64 	%rd129, {%r204, %r209};
	mov.b64 	%fd186, %rd129;
	add.s32 	%r248, %r193, 2;
	setp.gt.s32 	%p29, %r248, %r242;
	add.f64 	%fd187, %fd185, %fd186;
	selp.f64 	%fd188, %fd185, %fd187, %p29;
	mov.b64 	%rd130, %fd188;
	mov.u32 	%r221, 4;
	mov.b64 	{%r215, %r220}, %rd130;
	// begin inline asm
	shfl.sync.down.b32 %r214, %r215, %r221, %r242, %r243;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r219, %r220, %r221, %r242, %r243;
	// end inline asm
	mov.b64 	%rd131, {%r214, %r219};
	mov.b64 	%fd189, %rd131;
	add.s32 	%r249, %r193, 4;
	setp.gt.s32 	%p30, %r249, %r242;
	add.f64 	%fd190, %fd188, %fd189;
	selp.f64 	%fd191, %fd188, %fd190, %p30;
	mov.b64 	%rd132, %fd191;
	mov.u32 	%r231, 8;
	mov.b64 	{%r225, %r230}, %rd132;
	// begin inline asm
	shfl.sync.down.b32 %r224, %r225, %r231, %r242, %r243;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r229, %r230, %r231, %r242, %r243;
	// end inline asm
	mov.b64 	%rd133, {%r224, %r229};
	mov.b64 	%fd192, %rd133;
	add.s32 	%r250, %r193, 8;
	setp.gt.s32 	%p31, %r250, %r242;
	add.f64 	%fd193, %fd191, %fd192;
	selp.f64 	%fd194, %fd191, %fd193, %p31;
	mov.b64 	%rd134, %fd194;
	mov.u32 	%r241, 16;
	mov.b64 	{%r235, %r240}, %rd134;
	// begin inline asm
	shfl.sync.down.b32 %r234, %r235, %r241, %r242, %r243;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r239, %r240, %r241, %r242, %r243;
	// end inline asm
	mov.b64 	%rd135, {%r234, %r239};
	mov.b64 	%fd195, %rd135;
	add.s32 	%r251, %r193, 16;
	setp.gt.s32 	%p32, %r251, %r242;
	add.f64 	%fd196, %fd194, %fd195;
	selp.f64 	%fd533, %fd194, %fd196, %p32;
	@%p26 bra 	$L__BB7_30;

	add.s32 	%r592, %r192, 8;
	st.shared.f64 	[%r592], %fd533;

$L__BB7_30:
	bar.sync 	0;
	setp.ne.s32 	%p33, %r186, 0;
	@%p33 bra 	$L__BB7_75;

	setp.gt.s32 	%p34, %r13, 32;
	ld.shared.f64 	%fd197, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd198, %fd533, %fd197;
	selp.f64 	%fd199, %fd198, %fd533, %p34;
	ld.shared.f64 	%fd200, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd201, %fd199, %fd200;
	setp.gt.s32 	%p35, %r13, 64;
	selp.f64 	%fd202, %fd201, %fd199, %p35;
	ld.shared.f64 	%fd203, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd204, %fd202, %fd203;
	setp.gt.s32 	%p36, %r13, 96;
	selp.f64 	%fd205, %fd204, %fd202, %p36;
	ld.shared.f64 	%fd206, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd207, %fd205, %fd206;
	setp.gt.s32 	%p37, %r13, 128;
	selp.f64 	%fd208, %fd207, %fd205, %p37;
	ld.shared.f64 	%fd209, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd210, %fd208, %fd209;
	setp.gt.s32 	%p38, %r13, 160;
	selp.f64 	%fd211, %fd210, %fd208, %p38;
	ld.shared.f64 	%fd212, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd213, %fd211, %fd212;
	setp.gt.s32 	%p39, %r13, 192;
	selp.f64 	%fd533, %fd213, %fd211, %p39;
	setp.lt.s32 	%p40, %r13, 225;
	@%p40 bra 	$L__BB7_75;

	ld.shared.f64 	%fd214, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd533, %fd214;
	bra.uni 	$L__BB7_75;

$L__BB7_66:
	setp.ne.s32 	%p73, %r470, 0;
	shl.b32 	%r529, %r56, 5;
	add.s32 	%r530, %r529, 32;
	setp.gt.s32 	%p74, %r530, %r43;
	// begin inline asm
	mov.u32 %r478, %laneid;
	// end inline asm
	not.b32 	%r531, %r529;
	mov.u32 	%r528, -1;
	add.s32 	%r532, %r43, %r531;
	selp.b32 	%r527, %r532, 31, %p74;
	mov.u32 	%r486, 1;
	// begin inline asm
	shfl.sync.down.b32 %r479, %r58, %r486, %r527, %r528;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r484, %r59, %r486, %r527, %r528;
	// end inline asm
	mov.b64 	%rd250, {%r479, %r484};
	mov.b64 	%fd445, %rd250;
	setp.lt.s32 	%p75, %r478, %r527;
	add.f64 	%fd446, %fd532, %fd445;
	selp.f64 	%fd447, %fd446, %fd532, %p75;
	mov.b64 	%rd251, %fd447;
	mov.u32 	%r496, 2;
	mov.b64 	{%r490, %r495}, %rd251;
	// begin inline asm
	shfl.sync.down.b32 %r489, %r490, %r496, %r527, %r528;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r494, %r495, %r496, %r527, %r528;
	// end inline asm
	mov.b64 	%rd252, {%r489, %r494};
	mov.b64 	%fd448, %rd252;
	add.s32 	%r533, %r478, 2;
	setp.gt.s32 	%p76, %r533, %r527;
	add.f64 	%fd449, %fd447, %fd448;
	selp.f64 	%fd450, %fd447, %fd449, %p76;
	mov.b64 	%rd253, %fd450;
	mov.u32 	%r506, 4;
	mov.b64 	{%r500, %r505}, %rd253;
	// begin inline asm
	shfl.sync.down.b32 %r499, %r500, %r506, %r527, %r528;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r504, %r505, %r506, %r527, %r528;
	// end inline asm
	mov.b64 	%rd254, {%r499, %r504};
	mov.b64 	%fd451, %rd254;
	add.s32 	%r534, %r478, 4;
	setp.gt.s32 	%p77, %r534, %r527;
	add.f64 	%fd452, %fd450, %fd451;
	selp.f64 	%fd453, %fd450, %fd452, %p77;
	mov.b64 	%rd255, %fd453;
	mov.u32 	%r516, 8;
	mov.b64 	{%r510, %r515}, %rd255;
	// begin inline asm
	shfl.sync.down.b32 %r509, %r510, %r516, %r527, %r528;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r514, %r515, %r516, %r527, %r528;
	// end inline asm
	mov.b64 	%rd256, {%r509, %r514};
	mov.b64 	%fd454, %rd256;
	add.s32 	%r535, %r478, 8;
	setp.gt.s32 	%p78, %r535, %r527;
	add.f64 	%fd455, %fd453, %fd454;
	selp.f64 	%fd456, %fd453, %fd455, %p78;
	mov.b64 	%rd257, %fd456;
	mov.u32 	%r526, 16;
	mov.b64 	{%r520, %r525}, %rd257;
	// begin inline asm
	shfl.sync.down.b32 %r519, %r520, %r526, %r527, %r528;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r524, %r525, %r526, %r527, %r528;
	// end inline asm
	mov.b64 	%rd258, {%r519, %r524};
	mov.b64 	%fd457, %rd258;
	add.s32 	%r536, %r478, 16;
	setp.gt.s32 	%p79, %r536, %r527;
	add.f64 	%fd458, %fd456, %fd457;
	selp.f64 	%fd533, %fd456, %fd458, %p79;
	@%p73 bra 	$L__BB7_68;

	add.s32 	%r594, %r477, 8;
	st.shared.f64 	[%r594], %fd533;

$L__BB7_68:
	bar.sync 	0;
	setp.ne.s32 	%p80, %r471, 0;
	@%p80 bra 	$L__BB7_75;

	setp.gt.s32 	%p81, %r43, 32;
	ld.shared.f64 	%fd459, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd460, %fd533, %fd459;
	selp.f64 	%fd461, %fd460, %fd533, %p81;
	ld.shared.f64 	%fd462, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd463, %fd461, %fd462;
	setp.gt.s32 	%p82, %r43, 64;
	selp.f64 	%fd464, %fd463, %fd461, %p82;
	ld.shared.f64 	%fd465, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd466, %fd464, %fd465;
	setp.gt.s32 	%p83, %r43, 96;
	selp.f64 	%fd467, %fd466, %fd464, %p83;
	ld.shared.f64 	%fd468, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd469, %fd467, %fd468;
	setp.gt.s32 	%p84, %r43, 128;
	selp.f64 	%fd470, %fd469, %fd467, %p84;
	ld.shared.f64 	%fd471, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd472, %fd470, %fd471;
	setp.gt.s32 	%p85, %r43, 160;
	selp.f64 	%fd473, %fd472, %fd470, %p85;
	ld.shared.f64 	%fd474, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd475, %fd473, %fd474;
	setp.gt.s32 	%p86, %r43, 192;
	selp.f64 	%fd533, %fd475, %fd473, %p86;
	setp.lt.s32 	%p87, %r43, 225;
	@%p87 bra 	$L__BB7_75;

	ld.shared.f64 	%fd476, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd533, %fd533, %fd476;

$L__BB7_75:
	mov.u32 	%r590, %tid.x;
	setp.ne.s32 	%p95, %r590, 0;
	@%p95 bra 	$L__BB7_79;

	add.f64 	%fd504, %fd533, %fd59;
	cvta.to.global.u64 	%rd268, %rd49;
	st.global.f64 	[%rd268], %fd504;

$L__BB7_79:
	ret;

}
	// .globl	_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_
.visible .entry _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_(
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_0,
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_1,
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_2,
	.param .align 8 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_3[72],
	.param .align 1 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_4[1]
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<93>;
	.reg .f32 	%f<103>;
	.reg .b32 	%r<617>;
	.reg .f64 	%fd<447>;
	.reg .b64 	%rd<372>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage[80];

	ld.param.u64 	%rd55, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_0];
	ld.param.u64 	%rd56, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_1];
	ld.param.u64 	%rd1, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_3+32];
	ld.param.u32 	%r79, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3__param_3+40];
	mul.lo.s32 	%r1, %r79, 5120;
	cvt.s64.s32 	%rd2, %r1;
	mov.u32 	%r80, %ctaid.x;
	mul.lo.s32 	%r81, %r80, 5120;
	cvt.s64.s32 	%rd3, %r81;
	and.b64  	%rd57, %rd55, 15;
	setp.eq.s64 	%p1, %rd57, 0;
	@%p1 bra 	$L__BB8_36;

	add.s64 	%rd59, %rd3, 5120;
	setp.gt.s64 	%p2, %rd59, %rd1;
	@%p2 bra 	$L__BB8_17;
	bra.uni 	$L__BB8_2;

$L__BB8_17:
	cvt.u32.u64 	%r210, %rd3;
	cvt.u32.u64 	%r29, %rd1;
	sub.s32 	%r30, %r29, %r210;
	mov.u32 	%r606, %tid.x;
	setp.ge.s32 	%p18, %r606, %r30;
	@%p18 bra 	$L__BB8_19;

	mov.u32 	%r212, %tid.x;
	cvt.s64.s32 	%rd230, %r212;
	add.s64 	%rd231, %rd3, %rd230;
	shl.b64 	%rd232, %rd231, 2;
	add.s64 	%rd229, %rd55, %rd232;
	// begin inline asm
	ld.global.nc.u32 %r211, [%rd229];
	// end inline asm
	mov.b32 	%f46, %r211;
	cvt.f64.f32 	%fd432, %f46;
	add.s32 	%r606, %r212, 256;

$L__BB8_19:
	setp.ge.s32 	%p19, %r606, %r30;
	@%p19 bra 	$L__BB8_26;

	not.b32 	%r213, %r606;
	add.s32 	%r214, %r213, %r29;
	mad.lo.s32 	%r34, %r80, -5120, %r214;
	shr.u32 	%r216, %r34, 8;
	add.s32 	%r217, %r216, 1;
	and.b32  	%r605, %r217, 3;
	setp.eq.s32 	%p20, %r605, 0;
	@%p20 bra 	$L__BB8_23;

	cvt.s64.s32 	%rd233, %r606;
	add.s64 	%rd234, %rd233, %rd3;
	shl.b64 	%rd235, %rd234, 2;
	add.s64 	%rd364, %rd55, %rd235;

$L__BB8_22:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r218, [%rd364];
	// end inline asm
	mov.b32 	%f47, %r218;
	cvt.f64.f32 	%fd171, %f47;
	add.f64 	%fd432, %fd432, %fd171;
	add.s32 	%r606, %r606, 256;
	add.s64 	%rd364, %rd364, 1024;
	add.s32 	%r605, %r605, -1;
	setp.ne.s32 	%p21, %r605, 0;
	@%p21 bra 	$L__BB8_22;

$L__BB8_23:
	setp.lt.u32 	%p22, %r34, 768;
	@%p22 bra 	$L__BB8_26;

	cvt.s64.s32 	%rd237, %r606;
	add.s64 	%rd238, %rd237, %rd3;
	shl.b64 	%rd239, %rd238, 2;
	add.s64 	%rd365, %rd55, %rd239;

$L__BB8_25:
	// begin inline asm
	ld.global.nc.u32 %r219, [%rd365];
	// end inline asm
	mov.b32 	%f48, %r219;
	cvt.f64.f32 	%fd172, %f48;
	add.f64 	%fd173, %fd432, %fd172;
	add.s64 	%rd241, %rd365, 1024;
	// begin inline asm
	ld.global.nc.u32 %r220, [%rd241];
	// end inline asm
	mov.b32 	%f49, %r220;
	cvt.f64.f32 	%fd174, %f49;
	add.f64 	%fd175, %fd173, %fd174;
	add.s64 	%rd242, %rd365, 2048;
	// begin inline asm
	ld.global.nc.u32 %r221, [%rd242];
	// end inline asm
	mov.b32 	%f50, %r221;
	cvt.f64.f32 	%fd176, %f50;
	add.f64 	%fd177, %fd175, %fd176;
	add.s64 	%rd243, %rd365, 3072;
	// begin inline asm
	ld.global.nc.u32 %r222, [%rd243];
	// end inline asm
	mov.b32 	%f51, %r222;
	cvt.f64.f32 	%fd178, %f51;
	add.f64 	%fd432, %fd177, %fd178;
	add.s64 	%rd365, %rd365, 4096;
	add.s32 	%r606, %r606, 1024;
	setp.lt.s32 	%p23, %r606, %r30;
	@%p23 bra 	$L__BB8_25;

$L__BB8_26:
	mov.u32 	%r224, %tid.x;
	shr.s32 	%r225, %r224, 31;
	shr.u32 	%r226, %r225, 27;
	add.s32 	%r227, %r224, %r226;
	shr.s32 	%r43, %r227, 5;
	// begin inline asm
	mov.u32 %r223, %laneid;
	// end inline asm
	mov.b64 	%rd244, %fd432;
	mov.b64 	{%r45, %r46}, %rd244;
	shl.b32 	%r228, %r43, 3;
	mov.u32 	%r229, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r230, %r229, %r228;
	setp.gt.s32 	%p24, %r30, 255;
	@%p24 bra 	$L__BB8_32;
	bra.uni 	$L__BB8_27;

$L__BB8_32:
	setp.ne.s32 	%p40, %r223, 0;
	// begin inline asm
	mov.u32 %r291, %laneid;
	// end inline asm
	mov.u32 	%r299, 1;
	mov.u32 	%r340, 31;
	mov.u32 	%r341, -1;
	// begin inline asm
	shfl.sync.down.b32 %r292, %r45, %r299, %r340, %r341;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r297, %r46, %r299, %r340, %r341;
	// end inline asm
	mov.b64 	%rd254, {%r292, %r297};
	mov.b64 	%fd211, %rd254;
	setp.gt.s32 	%p41, %r291, 30;
	add.f64 	%fd212, %fd432, %fd211;
	selp.f64 	%fd213, %fd432, %fd212, %p41;
	mov.b64 	%rd255, %fd213;
	mov.u32 	%r309, 2;
	mov.b64 	{%r303, %r308}, %rd255;
	// begin inline asm
	shfl.sync.down.b32 %r302, %r303, %r309, %r340, %r341;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r307, %r308, %r309, %r340, %r341;
	// end inline asm
	mov.b64 	%rd256, {%r302, %r307};
	mov.b64 	%fd214, %rd256;
	setp.gt.s32 	%p42, %r291, 29;
	add.f64 	%fd215, %fd213, %fd214;
	selp.f64 	%fd216, %fd213, %fd215, %p42;
	mov.b64 	%rd257, %fd216;
	mov.u32 	%r319, 4;
	mov.b64 	{%r313, %r318}, %rd257;
	// begin inline asm
	shfl.sync.down.b32 %r312, %r313, %r319, %r340, %r341;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r317, %r318, %r319, %r340, %r341;
	// end inline asm
	mov.b64 	%rd258, {%r312, %r317};
	mov.b64 	%fd217, %rd258;
	setp.gt.s32 	%p43, %r291, 27;
	add.f64 	%fd218, %fd216, %fd217;
	selp.f64 	%fd219, %fd216, %fd218, %p43;
	mov.b64 	%rd259, %fd219;
	mov.u32 	%r329, 8;
	mov.b64 	{%r323, %r328}, %rd259;
	// begin inline asm
	shfl.sync.down.b32 %r322, %r323, %r329, %r340, %r341;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r327, %r328, %r329, %r340, %r341;
	// end inline asm
	mov.b64 	%rd260, {%r322, %r327};
	mov.b64 	%fd220, %rd260;
	setp.gt.s32 	%p44, %r291, 23;
	add.f64 	%fd221, %fd219, %fd220;
	selp.f64 	%fd222, %fd219, %fd221, %p44;
	mov.b64 	%rd261, %fd222;
	mov.u32 	%r339, 16;
	mov.b64 	{%r333, %r338}, %rd261;
	// begin inline asm
	shfl.sync.down.b32 %r332, %r333, %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r337, %r338, %r339, %r340, %r341;
	// end inline asm
	mov.b64 	%rd262, {%r332, %r337};
	mov.b64 	%fd223, %rd262;
	setp.gt.s32 	%p45, %r291, 15;
	add.f64 	%fd224, %fd222, %fd223;
	selp.f64 	%fd446, %fd222, %fd224, %p45;
	@%p40 bra 	$L__BB8_34;

	add.s32 	%r596, %r230, 8;
	st.shared.f64 	[%r596], %fd446;

$L__BB8_34:
	bar.sync 	0;
	setp.ne.s32 	%p46, %r224, 0;
	@%p46 bra 	$L__BB8_71;

	ld.shared.f64 	%fd225, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd226, %fd446, %fd225;
	ld.shared.f64 	%fd227, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd228, %fd226, %fd227;
	ld.shared.f64 	%fd229, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd230, %fd228, %fd229;
	ld.shared.f64 	%fd231, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd232, %fd230, %fd231;
	ld.shared.f64 	%fd233, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd234, %fd232, %fd233;
	ld.shared.f64 	%fd235, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd236, %fd234, %fd235;
	ld.shared.f64 	%fd237, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd236, %fd237;
	bra.uni 	$L__BB8_71;

$L__BB8_36:
	add.s64 	%rd264, %rd3, 5120;
	setp.gt.s64 	%p47, %rd264, %rd1;
	@%p47 bra 	$L__BB8_52;
	bra.uni 	$L__BB8_37;

$L__BB8_52:
	cvt.u32.u64 	%r460, %rd3;
	cvt.u32.u64 	%r60, %rd1;
	sub.s32 	%r61, %r60, %r460;
	mov.u32 	%r615, %tid.x;
	setp.ge.s32 	%p63, %r615, %r61;
	@%p63 bra 	$L__BB8_54;

	mov.u32 	%r462, %tid.x;
	cvt.s64.s32 	%rd324, %r462;
	add.s64 	%rd325, %rd3, %rd324;
	shl.b64 	%rd326, %rd325, 2;
	add.s64 	%rd323, %rd55, %rd326;
	// begin inline asm
	ld.global.nc.u32 %r461, [%rd323];
	// end inline asm
	mov.b32 	%f97, %r461;
	cvt.f64.f32 	%fd445, %f97;
	add.s32 	%r615, %r462, 256;

$L__BB8_54:
	setp.ge.s32 	%p64, %r615, %r61;
	@%p64 bra 	$L__BB8_61;

	not.b32 	%r463, %r615;
	add.s32 	%r464, %r463, %r60;
	mad.lo.s32 	%r65, %r80, -5120, %r464;
	shr.u32 	%r466, %r65, 8;
	add.s32 	%r467, %r466, 1;
	and.b32  	%r614, %r467, 3;
	setp.eq.s32 	%p65, %r614, 0;
	@%p65 bra 	$L__BB8_58;

	cvt.s64.s32 	%rd327, %r615;
	add.s64 	%rd328, %rd327, %rd3;
	shl.b64 	%rd329, %rd328, 2;
	add.s64 	%rd370, %rd55, %rd329;

$L__BB8_57:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r468, [%rd370];
	// end inline asm
	mov.b32 	%f98, %r468;
	cvt.f64.f32 	%fd353, %f98;
	add.f64 	%fd445, %fd445, %fd353;
	add.s32 	%r615, %r615, 256;
	add.s64 	%rd370, %rd370, 1024;
	add.s32 	%r614, %r614, -1;
	setp.ne.s32 	%p66, %r614, 0;
	@%p66 bra 	$L__BB8_57;

$L__BB8_58:
	setp.lt.u32 	%p67, %r65, 768;
	@%p67 bra 	$L__BB8_61;

	cvt.s64.s32 	%rd331, %r615;
	add.s64 	%rd332, %rd331, %rd3;
	shl.b64 	%rd333, %rd332, 2;
	add.s64 	%rd371, %rd55, %rd333;

$L__BB8_60:
	// begin inline asm
	ld.global.nc.u32 %r469, [%rd371];
	// end inline asm
	mov.b32 	%f99, %r469;
	cvt.f64.f32 	%fd354, %f99;
	add.f64 	%fd355, %fd445, %fd354;
	add.s64 	%rd335, %rd371, 1024;
	// begin inline asm
	ld.global.nc.u32 %r470, [%rd335];
	// end inline asm
	mov.b32 	%f100, %r470;
	cvt.f64.f32 	%fd356, %f100;
	add.f64 	%fd357, %fd355, %fd356;
	add.s64 	%rd336, %rd371, 2048;
	// begin inline asm
	ld.global.nc.u32 %r471, [%rd336];
	// end inline asm
	mov.b32 	%f101, %r471;
	cvt.f64.f32 	%fd358, %f101;
	add.f64 	%fd359, %fd357, %fd358;
	add.s64 	%rd337, %rd371, 3072;
	// begin inline asm
	ld.global.nc.u32 %r472, [%rd337];
	// end inline asm
	mov.b32 	%f102, %r472;
	cvt.f64.f32 	%fd360, %f102;
	add.f64 	%fd445, %fd359, %fd360;
	add.s64 	%rd371, %rd371, 4096;
	add.s32 	%r615, %r615, 1024;
	setp.lt.s32 	%p68, %r615, %r61;
	@%p68 bra 	$L__BB8_60;

$L__BB8_61:
	mov.u32 	%r474, %tid.x;
	shr.s32 	%r475, %r474, 31;
	shr.u32 	%r476, %r475, 27;
	add.s32 	%r477, %r474, %r476;
	shr.s32 	%r74, %r477, 5;
	// begin inline asm
	mov.u32 %r473, %laneid;
	// end inline asm
	mov.b64 	%rd338, %fd445;
	mov.b64 	{%r76, %r77}, %rd338;
	shl.b32 	%r478, %r74, 3;
	mov.u32 	%r479, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r480, %r479, %r478;
	setp.gt.s32 	%p69, %r61, 255;
	@%p69 bra 	$L__BB8_67;
	bra.uni 	$L__BB8_62;

$L__BB8_67:
	setp.ne.s32 	%p85, %r473, 0;
	// begin inline asm
	mov.u32 %r541, %laneid;
	// end inline asm
	mov.u32 	%r549, 1;
	mov.u32 	%r590, 31;
	mov.u32 	%r591, -1;
	// begin inline asm
	shfl.sync.down.b32 %r542, %r76, %r549, %r590, %r591;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r547, %r77, %r549, %r590, %r591;
	// end inline asm
	mov.b64 	%rd348, {%r542, %r547};
	mov.b64 	%fd393, %rd348;
	setp.gt.s32 	%p86, %r541, 30;
	add.f64 	%fd394, %fd445, %fd393;
	selp.f64 	%fd395, %fd445, %fd394, %p86;
	mov.b64 	%rd349, %fd395;
	mov.u32 	%r559, 2;
	mov.b64 	{%r553, %r558}, %rd349;
	// begin inline asm
	shfl.sync.down.b32 %r552, %r553, %r559, %r590, %r591;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r557, %r558, %r559, %r590, %r591;
	// end inline asm
	mov.b64 	%rd350, {%r552, %r557};
	mov.b64 	%fd396, %rd350;
	setp.gt.s32 	%p87, %r541, 29;
	add.f64 	%fd397, %fd395, %fd396;
	selp.f64 	%fd398, %fd395, %fd397, %p87;
	mov.b64 	%rd351, %fd398;
	mov.u32 	%r569, 4;
	mov.b64 	{%r563, %r568}, %rd351;
	// begin inline asm
	shfl.sync.down.b32 %r562, %r563, %r569, %r590, %r591;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r567, %r568, %r569, %r590, %r591;
	// end inline asm
	mov.b64 	%rd352, {%r562, %r567};
	mov.b64 	%fd399, %rd352;
	setp.gt.s32 	%p88, %r541, 27;
	add.f64 	%fd400, %fd398, %fd399;
	selp.f64 	%fd401, %fd398, %fd400, %p88;
	mov.b64 	%rd353, %fd401;
	mov.u32 	%r579, 8;
	mov.b64 	{%r573, %r578}, %rd353;
	// begin inline asm
	shfl.sync.down.b32 %r572, %r573, %r579, %r590, %r591;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r577, %r578, %r579, %r590, %r591;
	// end inline asm
	mov.b64 	%rd354, {%r572, %r577};
	mov.b64 	%fd402, %rd354;
	setp.gt.s32 	%p89, %r541, 23;
	add.f64 	%fd403, %fd401, %fd402;
	selp.f64 	%fd404, %fd401, %fd403, %p89;
	mov.b64 	%rd355, %fd404;
	mov.u32 	%r589, 16;
	mov.b64 	{%r583, %r588}, %rd355;
	// begin inline asm
	shfl.sync.down.b32 %r582, %r583, %r589, %r590, %r591;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r587, %r588, %r589, %r590, %r591;
	// end inline asm
	mov.b64 	%rd356, {%r582, %r587};
	mov.b64 	%fd405, %rd356;
	setp.gt.s32 	%p90, %r541, 15;
	add.f64 	%fd406, %fd404, %fd405;
	selp.f64 	%fd446, %fd404, %fd406, %p90;
	@%p85 bra 	$L__BB8_69;

	add.s32 	%r598, %r480, 8;
	st.shared.f64 	[%r598], %fd446;

$L__BB8_69:
	bar.sync 	0;
	setp.ne.s32 	%p91, %r474, 0;
	@%p91 bra 	$L__BB8_71;

	ld.shared.f64 	%fd407, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd408, %fd446, %fd407;
	ld.shared.f64 	%fd409, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd410, %fd408, %fd409;
	ld.shared.f64 	%fd411, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd412, %fd410, %fd411;
	ld.shared.f64 	%fd413, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd414, %fd412, %fd413;
	ld.shared.f64 	%fd415, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd416, %fd414, %fd415;
	ld.shared.f64 	%fd417, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd418, %fd416, %fd417;
	ld.shared.f64 	%fd419, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd418, %fd419;
	bra.uni 	$L__BB8_71;

$L__BB8_2:
	mov.u32 	%r104, %tid.x;
	cvt.s64.s32 	%rd80, %r104;
	add.s64 	%rd81, %rd80, %rd3;
	shl.b64 	%rd82, %rd81, 2;
	add.s64 	%rd60, %rd55, %rd82;
	// begin inline asm
	ld.global.nc.u32 %r84, [%rd60];
	// end inline asm
	mov.b32 	%f1, %r84;
	cvt.f64.f32 	%fd56, %f1;
	add.s32 	%r105, %r104, 256;
	cvt.s64.s32 	%rd83, %r105;
	add.s64 	%rd84, %rd83, %rd3;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd61, %rd55, %rd85;
	// begin inline asm
	ld.global.nc.u32 %r85, [%rd61];
	// end inline asm
	mov.b32 	%f2, %r85;
	cvt.f64.f32 	%fd57, %f2;
	add.s32 	%r106, %r104, 512;
	cvt.s64.s32 	%rd86, %r106;
	add.s64 	%rd87, %rd86, %rd3;
	shl.b64 	%rd88, %rd87, 2;
	add.s64 	%rd62, %rd55, %rd88;
	// begin inline asm
	ld.global.nc.u32 %r86, [%rd62];
	// end inline asm
	mov.b32 	%f3, %r86;
	cvt.f64.f32 	%fd58, %f3;
	add.s32 	%r2, %r104, 768;
	cvt.s64.s32 	%rd89, %r2;
	add.s64 	%rd90, %rd89, %rd3;
	shl.b64 	%rd91, %rd90, 2;
	add.s64 	%rd63, %rd55, %rd91;
	// begin inline asm
	ld.global.nc.u32 %r87, [%rd63];
	// end inline asm
	mov.b32 	%f4, %r87;
	cvt.f64.f32 	%fd59, %f4;
	add.s32 	%r3, %r104, 1024;
	cvt.s64.s32 	%rd92, %r3;
	add.s64 	%rd93, %rd92, %rd3;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd64, %rd55, %rd94;
	// begin inline asm
	ld.global.nc.u32 %r88, [%rd64];
	// end inline asm
	mov.b32 	%f5, %r88;
	cvt.f64.f32 	%fd60, %f5;
	add.s32 	%r4, %r104, 1280;
	cvt.s64.s32 	%rd95, %r4;
	add.s64 	%rd96, %rd95, %rd3;
	shl.b64 	%rd97, %rd96, 2;
	add.s64 	%rd65, %rd55, %rd97;
	// begin inline asm
	ld.global.nc.u32 %r89, [%rd65];
	// end inline asm
	mov.b32 	%f6, %r89;
	cvt.f64.f32 	%fd61, %f6;
	add.s32 	%r5, %r104, 1536;
	cvt.s64.s32 	%rd98, %r5;
	add.s64 	%rd99, %rd98, %rd3;
	shl.b64 	%rd100, %rd99, 2;
	add.s64 	%rd66, %rd55, %rd100;
	// begin inline asm
	ld.global.nc.u32 %r90, [%rd66];
	// end inline asm
	mov.b32 	%f7, %r90;
	cvt.f64.f32 	%fd62, %f7;
	add.s32 	%r6, %r104, 1792;
	cvt.s64.s32 	%rd101, %r6;
	add.s64 	%rd102, %rd101, %rd3;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd67, %rd55, %rd103;
	// begin inline asm
	ld.global.nc.u32 %r91, [%rd67];
	// end inline asm
	mov.b32 	%f8, %r91;
	cvt.f64.f32 	%fd63, %f8;
	add.s32 	%r7, %r104, 2048;
	cvt.s64.s32 	%rd104, %r7;
	add.s64 	%rd105, %rd104, %rd3;
	shl.b64 	%rd106, %rd105, 2;
	add.s64 	%rd68, %rd55, %rd106;
	// begin inline asm
	ld.global.nc.u32 %r92, [%rd68];
	// end inline asm
	mov.b32 	%f9, %r92;
	cvt.f64.f32 	%fd64, %f9;
	add.s32 	%r8, %r104, 2304;
	cvt.s64.s32 	%rd107, %r8;
	add.s64 	%rd108, %rd107, %rd3;
	shl.b64 	%rd109, %rd108, 2;
	add.s64 	%rd69, %rd55, %rd109;
	// begin inline asm
	ld.global.nc.u32 %r93, [%rd69];
	// end inline asm
	mov.b32 	%f10, %r93;
	cvt.f64.f32 	%fd65, %f10;
	add.s32 	%r9, %r104, 2560;
	cvt.s64.s32 	%rd110, %r9;
	add.s64 	%rd111, %rd110, %rd3;
	shl.b64 	%rd112, %rd111, 2;
	add.s64 	%rd70, %rd55, %rd112;
	// begin inline asm
	ld.global.nc.u32 %r94, [%rd70];
	// end inline asm
	mov.b32 	%f11, %r94;
	cvt.f64.f32 	%fd66, %f11;
	add.s32 	%r10, %r104, 2816;
	cvt.s64.s32 	%rd113, %r10;
	add.s64 	%rd114, %rd113, %rd3;
	shl.b64 	%rd115, %rd114, 2;
	add.s64 	%rd71, %rd55, %rd115;
	// begin inline asm
	ld.global.nc.u32 %r95, [%rd71];
	// end inline asm
	mov.b32 	%f12, %r95;
	cvt.f64.f32 	%fd67, %f12;
	add.s32 	%r11, %r104, 3072;
	cvt.s64.s32 	%rd116, %r11;
	add.s64 	%rd117, %rd116, %rd3;
	shl.b64 	%rd118, %rd117, 2;
	add.s64 	%rd72, %rd55, %rd118;
	// begin inline asm
	ld.global.nc.u32 %r96, [%rd72];
	// end inline asm
	mov.b32 	%f13, %r96;
	cvt.f64.f32 	%fd68, %f13;
	add.s32 	%r12, %r104, 3328;
	cvt.s64.s32 	%rd119, %r12;
	add.s64 	%rd120, %rd119, %rd3;
	shl.b64 	%rd121, %rd120, 2;
	add.s64 	%rd73, %rd55, %rd121;
	// begin inline asm
	ld.global.nc.u32 %r97, [%rd73];
	// end inline asm
	mov.b32 	%f14, %r97;
	cvt.f64.f32 	%fd69, %f14;
	add.s32 	%r13, %r104, 3584;
	cvt.s64.s32 	%rd122, %r13;
	add.s64 	%rd123, %rd122, %rd3;
	shl.b64 	%rd124, %rd123, 2;
	add.s64 	%rd74, %rd55, %rd124;
	// begin inline asm
	ld.global.nc.u32 %r98, [%rd74];
	// end inline asm
	mov.b32 	%f15, %r98;
	cvt.f64.f32 	%fd70, %f15;
	add.s32 	%r14, %r104, 3840;
	cvt.s64.s32 	%rd125, %r14;
	add.s64 	%rd126, %rd125, %rd3;
	shl.b64 	%rd127, %rd126, 2;
	add.s64 	%rd75, %rd55, %rd127;
	// begin inline asm
	ld.global.nc.u32 %r99, [%rd75];
	// end inline asm
	mov.b32 	%f16, %r99;
	cvt.f64.f32 	%fd71, %f16;
	add.s32 	%r15, %r104, 4096;
	cvt.s64.s32 	%rd128, %r15;
	add.s64 	%rd129, %rd128, %rd3;
	shl.b64 	%rd130, %rd129, 2;
	add.s64 	%rd76, %rd55, %rd130;
	// begin inline asm
	ld.global.nc.u32 %r100, [%rd76];
	// end inline asm
	mov.b32 	%f17, %r100;
	cvt.f64.f32 	%fd72, %f17;
	add.s32 	%r16, %r104, 4352;
	cvt.s64.s32 	%rd131, %r16;
	add.s64 	%rd132, %rd131, %rd3;
	shl.b64 	%rd133, %rd132, 2;
	add.s64 	%rd77, %rd55, %rd133;
	// begin inline asm
	ld.global.nc.u32 %r101, [%rd77];
	// end inline asm
	mov.b32 	%f18, %r101;
	cvt.f64.f32 	%fd73, %f18;
	add.s32 	%r107, %r104, 4608;
	cvt.s64.s32 	%rd134, %r107;
	add.s64 	%rd135, %rd134, %rd3;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd78, %rd55, %rd136;
	// begin inline asm
	ld.global.nc.u32 %r102, [%rd78];
	// end inline asm
	mov.b32 	%f19, %r102;
	cvt.f64.f32 	%fd74, %f19;
	add.s32 	%r108, %r104, 4864;
	cvt.s64.s32 	%rd137, %r108;
	add.s64 	%rd138, %rd137, %rd3;
	shl.b64 	%rd139, %rd138, 2;
	add.s64 	%rd79, %rd55, %rd139;
	// begin inline asm
	ld.global.nc.u32 %r103, [%rd79];
	// end inline asm
	mov.b32 	%f20, %r103;
	cvt.f64.f32 	%fd75, %f20;
	add.f64 	%fd76, %fd56, %fd57;
	add.f64 	%fd77, %fd76, %fd58;
	add.f64 	%fd78, %fd77, %fd59;
	add.f64 	%fd79, %fd78, %fd60;
	add.f64 	%fd80, %fd79, %fd61;
	add.f64 	%fd81, %fd80, %fd62;
	add.f64 	%fd82, %fd81, %fd63;
	add.f64 	%fd83, %fd82, %fd64;
	add.f64 	%fd84, %fd83, %fd65;
	add.f64 	%fd85, %fd84, %fd66;
	add.f64 	%fd86, %fd85, %fd67;
	add.f64 	%fd87, %fd86, %fd68;
	add.f64 	%fd88, %fd87, %fd69;
	add.f64 	%fd89, %fd88, %fd70;
	add.f64 	%fd90, %fd89, %fd71;
	add.f64 	%fd91, %fd90, %fd72;
	add.f64 	%fd92, %fd91, %fd73;
	add.f64 	%fd93, %fd92, %fd74;
	add.f64 	%fd426, %fd93, %fd75;
	add.s64 	%rd361, %rd3, %rd2;
	add.s64 	%rd140, %rd361, 5120;
	setp.gt.s64 	%p3, %rd140, %rd1;
	@%p3 bra 	$L__BB8_5;

	add.s64 	%rd361, %rd3, %rd2;

$L__BB8_4:
	add.s64 	%rd162, %rd361, %rd80;
	shl.b64 	%rd163, %rd162, 2;
	add.s64 	%rd142, %rd55, %rd163;
	// begin inline asm
	ld.global.nc.u32 %r112, [%rd142];
	// end inline asm
	mov.b32 	%f21, %r112;
	cvt.f64.f32 	%fd94, %f21;
	add.s64 	%rd165, %rd361, %rd83;
	shl.b64 	%rd166, %rd165, 2;
	add.s64 	%rd143, %rd55, %rd166;
	// begin inline asm
	ld.global.nc.u32 %r113, [%rd143];
	// end inline asm
	mov.b32 	%f22, %r113;
	cvt.f64.f32 	%fd95, %f22;
	add.s64 	%rd168, %rd361, %rd86;
	shl.b64 	%rd169, %rd168, 2;
	add.s64 	%rd144, %rd55, %rd169;
	// begin inline asm
	ld.global.nc.u32 %r114, [%rd144];
	// end inline asm
	mov.b32 	%f23, %r114;
	cvt.f64.f32 	%fd96, %f23;
	add.s64 	%rd170, %rd361, %rd89;
	shl.b64 	%rd171, %rd170, 2;
	add.s64 	%rd145, %rd55, %rd171;
	// begin inline asm
	ld.global.nc.u32 %r115, [%rd145];
	// end inline asm
	mov.b32 	%f24, %r115;
	cvt.f64.f32 	%fd97, %f24;
	add.s64 	%rd172, %rd361, %rd92;
	shl.b64 	%rd173, %rd172, 2;
	add.s64 	%rd146, %rd55, %rd173;
	// begin inline asm
	ld.global.nc.u32 %r116, [%rd146];
	// end inline asm
	mov.b32 	%f25, %r116;
	cvt.f64.f32 	%fd98, %f25;
	add.s64 	%rd174, %rd361, %rd95;
	shl.b64 	%rd175, %rd174, 2;
	add.s64 	%rd147, %rd55, %rd175;
	// begin inline asm
	ld.global.nc.u32 %r117, [%rd147];
	// end inline asm
	mov.b32 	%f26, %r117;
	cvt.f64.f32 	%fd99, %f26;
	add.s64 	%rd176, %rd361, %rd98;
	shl.b64 	%rd177, %rd176, 2;
	add.s64 	%rd148, %rd55, %rd177;
	// begin inline asm
	ld.global.nc.u32 %r118, [%rd148];
	// end inline asm
	mov.b32 	%f27, %r118;
	cvt.f64.f32 	%fd100, %f27;
	add.s64 	%rd178, %rd361, %rd101;
	shl.b64 	%rd179, %rd178, 2;
	add.s64 	%rd149, %rd55, %rd179;
	// begin inline asm
	ld.global.nc.u32 %r119, [%rd149];
	// end inline asm
	mov.b32 	%f28, %r119;
	cvt.f64.f32 	%fd101, %f28;
	add.s64 	%rd180, %rd361, %rd104;
	shl.b64 	%rd181, %rd180, 2;
	add.s64 	%rd150, %rd55, %rd181;
	// begin inline asm
	ld.global.nc.u32 %r120, [%rd150];
	// end inline asm
	mov.b32 	%f29, %r120;
	cvt.f64.f32 	%fd102, %f29;
	add.s64 	%rd182, %rd361, %rd107;
	shl.b64 	%rd183, %rd182, 2;
	add.s64 	%rd151, %rd55, %rd183;
	// begin inline asm
	ld.global.nc.u32 %r121, [%rd151];
	// end inline asm
	mov.b32 	%f30, %r121;
	cvt.f64.f32 	%fd103, %f30;
	add.s64 	%rd184, %rd361, %rd110;
	shl.b64 	%rd185, %rd184, 2;
	add.s64 	%rd152, %rd55, %rd185;
	// begin inline asm
	ld.global.nc.u32 %r122, [%rd152];
	// end inline asm
	mov.b32 	%f31, %r122;
	cvt.f64.f32 	%fd104, %f31;
	add.s64 	%rd186, %rd361, %rd113;
	shl.b64 	%rd187, %rd186, 2;
	add.s64 	%rd153, %rd55, %rd187;
	// begin inline asm
	ld.global.nc.u32 %r123, [%rd153];
	// end inline asm
	mov.b32 	%f32, %r123;
	cvt.f64.f32 	%fd105, %f32;
	add.s64 	%rd188, %rd361, %rd116;
	shl.b64 	%rd189, %rd188, 2;
	add.s64 	%rd154, %rd55, %rd189;
	// begin inline asm
	ld.global.nc.u32 %r124, [%rd154];
	// end inline asm
	mov.b32 	%f33, %r124;
	cvt.f64.f32 	%fd106, %f33;
	add.s64 	%rd190, %rd361, %rd119;
	shl.b64 	%rd191, %rd190, 2;
	add.s64 	%rd155, %rd55, %rd191;
	// begin inline asm
	ld.global.nc.u32 %r125, [%rd155];
	// end inline asm
	mov.b32 	%f34, %r125;
	cvt.f64.f32 	%fd107, %f34;
	add.s64 	%rd192, %rd361, %rd122;
	shl.b64 	%rd193, %rd192, 2;
	add.s64 	%rd156, %rd55, %rd193;
	// begin inline asm
	ld.global.nc.u32 %r126, [%rd156];
	// end inline asm
	mov.b32 	%f35, %r126;
	cvt.f64.f32 	%fd108, %f35;
	add.s64 	%rd194, %rd361, %rd125;
	shl.b64 	%rd195, %rd194, 2;
	add.s64 	%rd157, %rd55, %rd195;
	// begin inline asm
	ld.global.nc.u32 %r127, [%rd157];
	// end inline asm
	mov.b32 	%f36, %r127;
	cvt.f64.f32 	%fd109, %f36;
	add.s64 	%rd196, %rd361, %rd128;
	shl.b64 	%rd197, %rd196, 2;
	add.s64 	%rd158, %rd55, %rd197;
	// begin inline asm
	ld.global.nc.u32 %r128, [%rd158];
	// end inline asm
	mov.b32 	%f37, %r128;
	cvt.f64.f32 	%fd110, %f37;
	add.s64 	%rd198, %rd361, %rd131;
	shl.b64 	%rd199, %rd198, 2;
	add.s64 	%rd159, %rd55, %rd199;
	// begin inline asm
	ld.global.nc.u32 %r129, [%rd159];
	// end inline asm
	mov.b32 	%f38, %r129;
	cvt.f64.f32 	%fd111, %f38;
	add.s64 	%rd201, %rd361, %rd134;
	shl.b64 	%rd202, %rd201, 2;
	add.s64 	%rd160, %rd55, %rd202;
	// begin inline asm
	ld.global.nc.u32 %r130, [%rd160];
	// end inline asm
	mov.b32 	%f39, %r130;
	cvt.f64.f32 	%fd112, %f39;
	add.s64 	%rd204, %rd361, %rd137;
	shl.b64 	%rd205, %rd204, 2;
	add.s64 	%rd161, %rd55, %rd205;
	// begin inline asm
	ld.global.nc.u32 %r131, [%rd161];
	// end inline asm
	mov.b32 	%f40, %r131;
	cvt.f64.f32 	%fd113, %f40;
	add.f64 	%fd114, %fd426, %fd94;
	add.f64 	%fd115, %fd114, %fd95;
	add.f64 	%fd116, %fd115, %fd96;
	add.f64 	%fd117, %fd116, %fd97;
	add.f64 	%fd118, %fd117, %fd98;
	add.f64 	%fd119, %fd118, %fd99;
	add.f64 	%fd120, %fd119, %fd100;
	add.f64 	%fd121, %fd120, %fd101;
	add.f64 	%fd122, %fd121, %fd102;
	add.f64 	%fd123, %fd122, %fd103;
	add.f64 	%fd124, %fd123, %fd104;
	add.f64 	%fd125, %fd124, %fd105;
	add.f64 	%fd126, %fd125, %fd106;
	add.f64 	%fd127, %fd126, %fd107;
	add.f64 	%fd128, %fd127, %fd108;
	add.f64 	%fd129, %fd128, %fd109;
	add.f64 	%fd130, %fd129, %fd110;
	add.f64 	%fd131, %fd130, %fd111;
	add.f64 	%fd132, %fd131, %fd112;
	add.f64 	%fd426, %fd132, %fd113;
	add.s64 	%rd361, %rd361, %rd2;
	add.s64 	%rd206, %rd361, 5120;
	setp.le.s64 	%p4, %rd206, %rd1;
	@%p4 bra 	$L__BB8_4;

$L__BB8_5:
	setp.le.s64 	%p5, %rd1, %rd361;
	@%p5 bra 	$L__BB8_13;

	sub.s64 	%rd207, %rd1, %rd361;
	cvt.u32.u64 	%r17, %rd207;
	setp.ge.s32 	%p6, %r104, %r17;
	@%p6 bra 	$L__BB8_13;

	cvt.u32.u64 	%r138, %rd1;
	not.b32 	%r139, %r104;
	add.s32 	%r140, %r139, %r138;
	cvt.u32.u64 	%r141, %rd361;
	sub.s32 	%r19, %r140, %r141;
	shr.u32 	%r142, %r19, 8;
	add.s32 	%r143, %r142, 1;
	and.b32  	%r600, %r143, 3;
	setp.eq.s32 	%p7, %r600, 0;
	mov.u32 	%r601, %r104;
	@%p7 bra 	$L__BB8_10;

	mov.u32 	%r601, %tid.x;
	cvt.s64.s32 	%rd208, %r601;
	add.s64 	%rd209, %rd361, %rd208;
	shl.b64 	%rd210, %rd209, 2;
	add.s64 	%rd362, %rd55, %rd210;

$L__BB8_9:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r144, [%rd362];
	// end inline asm
	mov.b32 	%f41, %r144;
	cvt.f64.f32 	%fd134, %f41;
	add.f64 	%fd426, %fd426, %fd134;
	add.s32 	%r601, %r601, 256;
	add.s64 	%rd362, %rd362, 1024;
	add.s32 	%r600, %r600, -1;
	setp.ne.s32 	%p8, %r600, 0;
	@%p8 bra 	$L__BB8_9;

$L__BB8_10:
	setp.lt.u32 	%p9, %r19, 768;
	@%p9 bra 	$L__BB8_13;

	cvt.s64.s32 	%rd212, %r601;
	add.s64 	%rd213, %rd361, %rd212;
	shl.b64 	%rd214, %rd213, 2;
	add.s64 	%rd363, %rd55, %rd214;

$L__BB8_12:
	// begin inline asm
	ld.global.nc.u32 %r145, [%rd363];
	// end inline asm
	mov.b32 	%f42, %r145;
	cvt.f64.f32 	%fd135, %f42;
	add.f64 	%fd136, %fd426, %fd135;
	add.s64 	%rd216, %rd363, 1024;
	// begin inline asm
	ld.global.nc.u32 %r146, [%rd216];
	// end inline asm
	mov.b32 	%f43, %r146;
	cvt.f64.f32 	%fd137, %f43;
	add.f64 	%fd138, %fd136, %fd137;
	add.s64 	%rd217, %rd363, 2048;
	// begin inline asm
	ld.global.nc.u32 %r147, [%rd217];
	// end inline asm
	mov.b32 	%f44, %r147;
	cvt.f64.f32 	%fd139, %f44;
	add.f64 	%fd140, %fd138, %fd139;
	add.s64 	%rd218, %rd363, 3072;
	// begin inline asm
	ld.global.nc.u32 %r148, [%rd218];
	// end inline asm
	mov.b32 	%f45, %r148;
	cvt.f64.f32 	%fd141, %f45;
	add.f64 	%fd426, %fd140, %fd141;
	add.s64 	%rd363, %rd363, 4096;
	add.s32 	%r601, %r601, 1024;
	setp.lt.s32 	%p10, %r601, %r17;
	@%p10 bra 	$L__BB8_12;

$L__BB8_13:
	// begin inline asm
	mov.u32 %r149, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r150, %laneid;
	// end inline asm
	mov.b64 	%rd219, %fd426;
	mov.u32 	%r158, 1;
	mov.u32 	%r199, 31;
	mov.u32 	%r200, -1;
	mov.b64 	{%r152, %r157}, %rd219;
	// begin inline asm
	shfl.sync.down.b32 %r151, %r152, %r158, %r199, %r200;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r156, %r157, %r158, %r199, %r200;
	// end inline asm
	mov.b64 	%rd220, {%r151, %r156};
	mov.b64 	%fd142, %rd220;
	setp.gt.s32 	%p11, %r150, 30;
	add.f64 	%fd143, %fd426, %fd142;
	selp.f64 	%fd144, %fd426, %fd143, %p11;
	mov.b64 	%rd221, %fd144;
	mov.u32 	%r168, 2;
	mov.b64 	{%r162, %r167}, %rd221;
	// begin inline asm
	shfl.sync.down.b32 %r161, %r162, %r168, %r199, %r200;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r166, %r167, %r168, %r199, %r200;
	// end inline asm
	mov.b64 	%rd222, {%r161, %r166};
	mov.b64 	%fd145, %rd222;
	setp.gt.s32 	%p12, %r150, 29;
	add.f64 	%fd146, %fd144, %fd145;
	selp.f64 	%fd147, %fd144, %fd146, %p12;
	mov.b64 	%rd223, %fd147;
	mov.u32 	%r178, 4;
	mov.b64 	{%r172, %r177}, %rd223;
	// begin inline asm
	shfl.sync.down.b32 %r171, %r172, %r178, %r199, %r200;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r176, %r177, %r178, %r199, %r200;
	// end inline asm
	mov.b64 	%rd224, {%r171, %r176};
	mov.b64 	%fd148, %rd224;
	setp.gt.s32 	%p13, %r150, 27;
	add.f64 	%fd149, %fd147, %fd148;
	selp.f64 	%fd150, %fd147, %fd149, %p13;
	mov.b64 	%rd225, %fd150;
	mov.u32 	%r188, 8;
	mov.b64 	{%r182, %r187}, %rd225;
	// begin inline asm
	shfl.sync.down.b32 %r181, %r182, %r188, %r199, %r200;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r186, %r187, %r188, %r199, %r200;
	// end inline asm
	mov.b64 	%rd226, {%r181, %r186};
	mov.b64 	%fd151, %rd226;
	setp.gt.s32 	%p14, %r150, 23;
	add.f64 	%fd152, %fd150, %fd151;
	selp.f64 	%fd153, %fd150, %fd152, %p14;
	mov.b64 	%rd227, %fd153;
	mov.u32 	%r198, 16;
	mov.b64 	{%r192, %r197}, %rd227;
	// begin inline asm
	shfl.sync.down.b32 %r191, %r192, %r198, %r199, %r200;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r196, %r197, %r198, %r199, %r200;
	// end inline asm
	mov.b64 	%rd228, {%r191, %r196};
	mov.b64 	%fd154, %rd228;
	setp.gt.s32 	%p15, %r150, 15;
	add.f64 	%fd155, %fd153, %fd154;
	selp.f64 	%fd446, %fd153, %fd155, %p15;
	setp.ne.s32 	%p16, %r149, 0;
	@%p16 bra 	$L__BB8_15;

	shr.s32 	%r202, %r104, 31;
	shr.u32 	%r203, %r202, 27;
	add.s32 	%r204, %r104, %r203;
	shr.s32 	%r205, %r204, 5;
	shl.b32 	%r206, %r205, 3;
	mov.u32 	%r207, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r208, %r207, %r206;
	st.shared.f64 	[%r208+8], %fd446;

$L__BB8_15:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r104, 0;
	@%p17 bra 	$L__BB8_71;

	ld.shared.f64 	%fd156, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd157, %fd446, %fd156;
	ld.shared.f64 	%fd158, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd159, %fd157, %fd158;
	ld.shared.f64 	%fd160, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd161, %fd159, %fd160;
	ld.shared.f64 	%fd162, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd163, %fd161, %fd162;
	ld.shared.f64 	%fd164, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd165, %fd163, %fd164;
	ld.shared.f64 	%fd166, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd167, %fd165, %fd166;
	ld.shared.f64 	%fd168, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd167, %fd168;
	bra.uni 	$L__BB8_71;

$L__BB8_37:
	mov.u32 	%r345, %tid.x;
	shl.b32 	%r346, %r345, 2;
	cvt.u64.u32 	%rd38, %r346;
	add.s64 	%rd280, %rd38, %rd3;
	shl.b64 	%rd281, %rd280, 2;
	add.s64 	%rd267, %rd55, %rd281;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd265, %rd266}, [%rd267];
	// end inline asm
	mov.b64 	{%r347, %r348}, %rd266;
	mov.b64 	{%r349, %r350}, %rd265;
	mov.b32 	%f52, %r349;
	mov.b32 	%f53, %r350;
	mov.b32 	%f54, %r347;
	mov.b32 	%f55, %r348;
	add.s64 	%rd270, %rd267, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd268, %rd269}, [%rd270];
	// end inline asm
	mov.b64 	{%r351, %r352}, %rd269;
	mov.b64 	{%r353, %r354}, %rd268;
	mov.b32 	%f56, %r353;
	mov.b32 	%f57, %r354;
	mov.b32 	%f58, %r351;
	mov.b32 	%f59, %r352;
	add.s64 	%rd273, %rd267, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd271, %rd272}, [%rd273];
	// end inline asm
	mov.b64 	{%r355, %r356}, %rd272;
	mov.b64 	{%r357, %r358}, %rd271;
	mov.b32 	%f60, %r357;
	mov.b32 	%f61, %r358;
	mov.b32 	%f62, %r355;
	mov.b32 	%f63, %r356;
	add.s64 	%rd276, %rd267, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd274, %rd275}, [%rd276];
	// end inline asm
	mov.b64 	{%r359, %r360}, %rd275;
	mov.b64 	{%r361, %r362}, %rd274;
	mov.b32 	%f64, %r361;
	mov.b32 	%f65, %r362;
	mov.b32 	%f66, %r359;
	mov.b32 	%f67, %r360;
	add.s64 	%rd279, %rd267, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd277, %rd278}, [%rd279];
	// end inline asm
	mov.b64 	{%r363, %r364}, %rd278;
	mov.b64 	{%r365, %r366}, %rd277;
	mov.b32 	%f68, %r365;
	mov.b32 	%f69, %r366;
	mov.b32 	%f70, %r363;
	mov.b32 	%f71, %r364;
	cvt.f64.f32 	%fd238, %f52;
	cvt.f64.f32 	%fd239, %f53;
	cvt.f64.f32 	%fd240, %f54;
	cvt.f64.f32 	%fd241, %f55;
	cvt.f64.f32 	%fd242, %f56;
	cvt.f64.f32 	%fd243, %f57;
	cvt.f64.f32 	%fd244, %f58;
	cvt.f64.f32 	%fd245, %f59;
	cvt.f64.f32 	%fd246, %f60;
	cvt.f64.f32 	%fd247, %f61;
	cvt.f64.f32 	%fd248, %f62;
	cvt.f64.f32 	%fd249, %f63;
	cvt.f64.f32 	%fd250, %f64;
	cvt.f64.f32 	%fd251, %f65;
	cvt.f64.f32 	%fd252, %f66;
	cvt.f64.f32 	%fd253, %f67;
	cvt.f64.f32 	%fd254, %f68;
	cvt.f64.f32 	%fd255, %f69;
	cvt.f64.f32 	%fd256, %f70;
	cvt.f64.f32 	%fd257, %f71;
	add.f64 	%fd258, %fd238, %fd239;
	add.f64 	%fd259, %fd258, %fd240;
	add.f64 	%fd260, %fd259, %fd241;
	add.f64 	%fd261, %fd260, %fd242;
	add.f64 	%fd262, %fd261, %fd243;
	add.f64 	%fd263, %fd262, %fd244;
	add.f64 	%fd264, %fd263, %fd245;
	add.f64 	%fd265, %fd264, %fd246;
	add.f64 	%fd266, %fd265, %fd247;
	add.f64 	%fd267, %fd266, %fd248;
	add.f64 	%fd268, %fd267, %fd249;
	add.f64 	%fd269, %fd268, %fd250;
	add.f64 	%fd270, %fd269, %fd251;
	add.f64 	%fd271, %fd270, %fd252;
	add.f64 	%fd272, %fd271, %fd253;
	add.f64 	%fd273, %fd272, %fd254;
	add.f64 	%fd274, %fd273, %fd255;
	add.f64 	%fd275, %fd274, %fd256;
	add.f64 	%fd439, %fd275, %fd257;
	add.s64 	%rd367, %rd3, %rd2;
	add.s64 	%rd282, %rd367, 5120;
	setp.gt.s64 	%p48, %rd282, %rd1;
	@%p48 bra 	$L__BB8_40;

$L__BB8_39:
	add.s64 	%rd298, %rd367, %rd38;
	shl.b64 	%rd299, %rd298, 2;
	add.s64 	%rd285, %rd55, %rd299;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd283, %rd284}, [%rd285];
	// end inline asm
	mov.b64 	{%r367, %r368}, %rd284;
	mov.b64 	{%r369, %r370}, %rd283;
	mov.b32 	%f72, %r369;
	mov.b32 	%f73, %r370;
	mov.b32 	%f74, %r367;
	mov.b32 	%f75, %r368;
	add.s64 	%rd288, %rd285, 4096;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd286, %rd287}, [%rd288];
	// end inline asm
	mov.b64 	{%r371, %r372}, %rd287;
	mov.b64 	{%r373, %r374}, %rd286;
	mov.b32 	%f76, %r373;
	mov.b32 	%f77, %r374;
	mov.b32 	%f78, %r371;
	mov.b32 	%f79, %r372;
	add.s64 	%rd291, %rd285, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd289, %rd290}, [%rd291];
	// end inline asm
	mov.b64 	{%r375, %r376}, %rd290;
	mov.b64 	{%r377, %r378}, %rd289;
	mov.b32 	%f80, %r377;
	mov.b32 	%f81, %r378;
	mov.b32 	%f82, %r375;
	mov.b32 	%f83, %r376;
	add.s64 	%rd294, %rd285, 12288;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd292, %rd293}, [%rd294];
	// end inline asm
	mov.b64 	{%r379, %r380}, %rd293;
	mov.b64 	{%r381, %r382}, %rd292;
	mov.b32 	%f84, %r381;
	mov.b32 	%f85, %r382;
	mov.b32 	%f86, %r379;
	mov.b32 	%f87, %r380;
	add.s64 	%rd297, %rd285, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd295, %rd296}, [%rd297];
	// end inline asm
	mov.b64 	{%r383, %r384}, %rd296;
	mov.b64 	{%r385, %r386}, %rd295;
	mov.b32 	%f88, %r385;
	mov.b32 	%f89, %r386;
	mov.b32 	%f90, %r383;
	mov.b32 	%f91, %r384;
	cvt.f64.f32 	%fd276, %f72;
	cvt.f64.f32 	%fd277, %f73;
	cvt.f64.f32 	%fd278, %f74;
	cvt.f64.f32 	%fd279, %f75;
	cvt.f64.f32 	%fd280, %f76;
	cvt.f64.f32 	%fd281, %f77;
	cvt.f64.f32 	%fd282, %f78;
	cvt.f64.f32 	%fd283, %f79;
	cvt.f64.f32 	%fd284, %f80;
	cvt.f64.f32 	%fd285, %f81;
	cvt.f64.f32 	%fd286, %f82;
	cvt.f64.f32 	%fd287, %f83;
	cvt.f64.f32 	%fd288, %f84;
	cvt.f64.f32 	%fd289, %f85;
	cvt.f64.f32 	%fd290, %f86;
	cvt.f64.f32 	%fd291, %f87;
	cvt.f64.f32 	%fd292, %f88;
	cvt.f64.f32 	%fd293, %f89;
	cvt.f64.f32 	%fd294, %f90;
	cvt.f64.f32 	%fd295, %f91;
	add.f64 	%fd296, %fd439, %fd276;
	add.f64 	%fd297, %fd296, %fd277;
	add.f64 	%fd298, %fd297, %fd278;
	add.f64 	%fd299, %fd298, %fd279;
	add.f64 	%fd300, %fd299, %fd280;
	add.f64 	%fd301, %fd300, %fd281;
	add.f64 	%fd302, %fd301, %fd282;
	add.f64 	%fd303, %fd302, %fd283;
	add.f64 	%fd304, %fd303, %fd284;
	add.f64 	%fd305, %fd304, %fd285;
	add.f64 	%fd306, %fd305, %fd286;
	add.f64 	%fd307, %fd306, %fd287;
	add.f64 	%fd308, %fd307, %fd288;
	add.f64 	%fd309, %fd308, %fd289;
	add.f64 	%fd310, %fd309, %fd290;
	add.f64 	%fd311, %fd310, %fd291;
	add.f64 	%fd312, %fd311, %fd292;
	add.f64 	%fd313, %fd312, %fd293;
	add.f64 	%fd314, %fd313, %fd294;
	add.f64 	%fd439, %fd314, %fd295;
	add.s64 	%rd367, %rd367, %rd2;
	add.s64 	%rd300, %rd367, 5120;
	setp.le.s64 	%p49, %rd300, %rd1;
	@%p49 bra 	$L__BB8_39;

$L__BB8_40:
	setp.le.s64 	%p50, %rd1, %rd367;
	@%p50 bra 	$L__BB8_48;

	sub.s64 	%rd301, %rd1, %rd367;
	cvt.u32.u64 	%r48, %rd301;
	setp.ge.s32 	%p51, %r345, %r48;
	@%p51 bra 	$L__BB8_48;

	cvt.u32.u64 	%r388, %rd1;
	not.b32 	%r389, %r345;
	add.s32 	%r390, %r389, %r388;
	cvt.u32.u64 	%r391, %rd367;
	sub.s32 	%r50, %r390, %r391;
	shr.u32 	%r392, %r50, 8;
	add.s32 	%r393, %r392, 1;
	and.b32  	%r609, %r393, 3;
	setp.eq.s32 	%p52, %r609, 0;
	mov.u32 	%r610, %r345;
	@%p52 bra 	$L__BB8_45;

	mov.u32 	%r610, %tid.x;
	cvt.s64.s32 	%rd302, %r610;
	add.s64 	%rd303, %rd367, %rd302;
	shl.b64 	%rd304, %rd303, 2;
	add.s64 	%rd368, %rd55, %rd304;

$L__BB8_44:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u32 %r394, [%rd368];
	// end inline asm
	mov.b32 	%f92, %r394;
	cvt.f64.f32 	%fd316, %f92;
	add.f64 	%fd439, %fd439, %fd316;
	add.s32 	%r610, %r610, 256;
	add.s64 	%rd368, %rd368, 1024;
	add.s32 	%r609, %r609, -1;
	setp.ne.s32 	%p53, %r609, 0;
	@%p53 bra 	$L__BB8_44;

$L__BB8_45:
	setp.lt.u32 	%p54, %r50, 768;
	@%p54 bra 	$L__BB8_48;

	cvt.s64.s32 	%rd306, %r610;
	add.s64 	%rd307, %rd367, %rd306;
	shl.b64 	%rd308, %rd307, 2;
	add.s64 	%rd369, %rd55, %rd308;

$L__BB8_47:
	// begin inline asm
	ld.global.nc.u32 %r395, [%rd369];
	// end inline asm
	mov.b32 	%f93, %r395;
	cvt.f64.f32 	%fd317, %f93;
	add.f64 	%fd318, %fd439, %fd317;
	add.s64 	%rd310, %rd369, 1024;
	// begin inline asm
	ld.global.nc.u32 %r396, [%rd310];
	// end inline asm
	mov.b32 	%f94, %r396;
	cvt.f64.f32 	%fd319, %f94;
	add.f64 	%fd320, %fd318, %fd319;
	add.s64 	%rd311, %rd369, 2048;
	// begin inline asm
	ld.global.nc.u32 %r397, [%rd311];
	// end inline asm
	mov.b32 	%f95, %r397;
	cvt.f64.f32 	%fd321, %f95;
	add.f64 	%fd322, %fd320, %fd321;
	add.s64 	%rd312, %rd369, 3072;
	// begin inline asm
	ld.global.nc.u32 %r398, [%rd312];
	// end inline asm
	mov.b32 	%f96, %r398;
	cvt.f64.f32 	%fd323, %f96;
	add.f64 	%fd439, %fd322, %fd323;
	add.s64 	%rd369, %rd369, 4096;
	add.s32 	%r610, %r610, 1024;
	setp.lt.s32 	%p55, %r610, %r48;
	@%p55 bra 	$L__BB8_47;

$L__BB8_48:
	// begin inline asm
	mov.u32 %r399, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r400, %laneid;
	// end inline asm
	mov.b64 	%rd313, %fd439;
	mov.u32 	%r408, 1;
	mov.u32 	%r449, 31;
	mov.u32 	%r450, -1;
	mov.b64 	{%r402, %r407}, %rd313;
	// begin inline asm
	shfl.sync.down.b32 %r401, %r402, %r408, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r406, %r407, %r408, %r449, %r450;
	// end inline asm
	mov.b64 	%rd314, {%r401, %r406};
	mov.b64 	%fd324, %rd314;
	setp.gt.s32 	%p56, %r400, 30;
	add.f64 	%fd325, %fd439, %fd324;
	selp.f64 	%fd326, %fd439, %fd325, %p56;
	mov.b64 	%rd315, %fd326;
	mov.u32 	%r418, 2;
	mov.b64 	{%r412, %r417}, %rd315;
	// begin inline asm
	shfl.sync.down.b32 %r411, %r412, %r418, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r416, %r417, %r418, %r449, %r450;
	// end inline asm
	mov.b64 	%rd316, {%r411, %r416};
	mov.b64 	%fd327, %rd316;
	setp.gt.s32 	%p57, %r400, 29;
	add.f64 	%fd328, %fd326, %fd327;
	selp.f64 	%fd329, %fd326, %fd328, %p57;
	mov.b64 	%rd317, %fd329;
	mov.u32 	%r428, 4;
	mov.b64 	{%r422, %r427}, %rd317;
	// begin inline asm
	shfl.sync.down.b32 %r421, %r422, %r428, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r426, %r427, %r428, %r449, %r450;
	// end inline asm
	mov.b64 	%rd318, {%r421, %r426};
	mov.b64 	%fd330, %rd318;
	setp.gt.s32 	%p58, %r400, 27;
	add.f64 	%fd331, %fd329, %fd330;
	selp.f64 	%fd332, %fd329, %fd331, %p58;
	mov.b64 	%rd319, %fd332;
	mov.u32 	%r438, 8;
	mov.b64 	{%r432, %r437}, %rd319;
	// begin inline asm
	shfl.sync.down.b32 %r431, %r432, %r438, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r436, %r437, %r438, %r449, %r450;
	// end inline asm
	mov.b64 	%rd320, {%r431, %r436};
	mov.b64 	%fd333, %rd320;
	setp.gt.s32 	%p59, %r400, 23;
	add.f64 	%fd334, %fd332, %fd333;
	selp.f64 	%fd335, %fd332, %fd334, %p59;
	mov.b64 	%rd321, %fd335;
	mov.u32 	%r448, 16;
	mov.b64 	{%r442, %r447}, %rd321;
	// begin inline asm
	shfl.sync.down.b32 %r441, %r442, %r448, %r449, %r450;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r446, %r447, %r448, %r449, %r450;
	// end inline asm
	mov.b64 	%rd322, {%r441, %r446};
	mov.b64 	%fd336, %rd322;
	setp.gt.s32 	%p60, %r400, 15;
	add.f64 	%fd337, %fd335, %fd336;
	selp.f64 	%fd446, %fd335, %fd337, %p60;
	setp.ne.s32 	%p61, %r399, 0;
	@%p61 bra 	$L__BB8_50;

	shr.s32 	%r452, %r345, 31;
	shr.u32 	%r453, %r452, 27;
	add.s32 	%r454, %r345, %r453;
	shr.s32 	%r455, %r454, 5;
	shl.b32 	%r456, %r455, 3;
	mov.u32 	%r457, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage;
	add.s32 	%r458, %r457, %r456;
	st.shared.f64 	[%r458+8], %fd446;

$L__BB8_50:
	bar.sync 	0;
	setp.ne.s32 	%p62, %r345, 0;
	@%p62 bra 	$L__BB8_71;

	ld.shared.f64 	%fd338, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd339, %fd446, %fd338;
	ld.shared.f64 	%fd340, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd341, %fd339, %fd340;
	ld.shared.f64 	%fd342, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd343, %fd341, %fd342;
	ld.shared.f64 	%fd344, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd345, %fd343, %fd344;
	ld.shared.f64 	%fd346, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd347, %fd345, %fd346;
	ld.shared.f64 	%fd348, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd349, %fd347, %fd348;
	ld.shared.f64 	%fd350, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd349, %fd350;
	bra.uni 	$L__BB8_71;

$L__BB8_27:
	setp.ne.s32 	%p25, %r223, 0;
	shl.b32 	%r282, %r43, 5;
	add.s32 	%r283, %r282, 32;
	setp.gt.s32 	%p26, %r283, %r30;
	// begin inline asm
	mov.u32 %r231, %laneid;
	// end inline asm
	not.b32 	%r284, %r282;
	mov.u32 	%r281, -1;
	add.s32 	%r285, %r30, %r284;
	selp.b32 	%r280, %r285, 31, %p26;
	mov.u32 	%r239, 1;
	// begin inline asm
	shfl.sync.down.b32 %r232, %r45, %r239, %r280, %r281;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r237, %r46, %r239, %r280, %r281;
	// end inline asm
	mov.b64 	%rd245, {%r232, %r237};
	mov.b64 	%fd179, %rd245;
	setp.lt.s32 	%p27, %r231, %r280;
	add.f64 	%fd180, %fd432, %fd179;
	selp.f64 	%fd181, %fd180, %fd432, %p27;
	mov.b64 	%rd246, %fd181;
	mov.u32 	%r249, 2;
	mov.b64 	{%r243, %r248}, %rd246;
	// begin inline asm
	shfl.sync.down.b32 %r242, %r243, %r249, %r280, %r281;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r247, %r248, %r249, %r280, %r281;
	// end inline asm
	mov.b64 	%rd247, {%r242, %r247};
	mov.b64 	%fd182, %rd247;
	add.s32 	%r286, %r231, 2;
	setp.gt.s32 	%p28, %r286, %r280;
	add.f64 	%fd183, %fd181, %fd182;
	selp.f64 	%fd184, %fd181, %fd183, %p28;
	mov.b64 	%rd248, %fd184;
	mov.u32 	%r259, 4;
	mov.b64 	{%r253, %r258}, %rd248;
	// begin inline asm
	shfl.sync.down.b32 %r252, %r253, %r259, %r280, %r281;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r257, %r258, %r259, %r280, %r281;
	// end inline asm
	mov.b64 	%rd249, {%r252, %r257};
	mov.b64 	%fd185, %rd249;
	add.s32 	%r287, %r231, 4;
	setp.gt.s32 	%p29, %r287, %r280;
	add.f64 	%fd186, %fd184, %fd185;
	selp.f64 	%fd187, %fd184, %fd186, %p29;
	mov.b64 	%rd250, %fd187;
	mov.u32 	%r269, 8;
	mov.b64 	{%r263, %r268}, %rd250;
	// begin inline asm
	shfl.sync.down.b32 %r262, %r263, %r269, %r280, %r281;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r267, %r268, %r269, %r280, %r281;
	// end inline asm
	mov.b64 	%rd251, {%r262, %r267};
	mov.b64 	%fd188, %rd251;
	add.s32 	%r288, %r231, 8;
	setp.gt.s32 	%p30, %r288, %r280;
	add.f64 	%fd189, %fd187, %fd188;
	selp.f64 	%fd190, %fd187, %fd189, %p30;
	mov.b64 	%rd252, %fd190;
	mov.u32 	%r279, 16;
	mov.b64 	{%r273, %r278}, %rd252;
	// begin inline asm
	shfl.sync.down.b32 %r272, %r273, %r279, %r280, %r281;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r277, %r278, %r279, %r280, %r281;
	// end inline asm
	mov.b64 	%rd253, {%r272, %r277};
	mov.b64 	%fd191, %rd253;
	add.s32 	%r289, %r231, 16;
	setp.gt.s32 	%p31, %r289, %r280;
	add.f64 	%fd192, %fd190, %fd191;
	selp.f64 	%fd446, %fd190, %fd192, %p31;
	@%p25 bra 	$L__BB8_29;

	add.s32 	%r595, %r230, 8;
	st.shared.f64 	[%r595], %fd446;

$L__BB8_29:
	bar.sync 	0;
	setp.ne.s32 	%p32, %r224, 0;
	@%p32 bra 	$L__BB8_71;

	setp.gt.s32 	%p33, %r30, 32;
	ld.shared.f64 	%fd193, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd194, %fd446, %fd193;
	selp.f64 	%fd195, %fd194, %fd446, %p33;
	ld.shared.f64 	%fd196, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd197, %fd195, %fd196;
	setp.gt.s32 	%p34, %r30, 64;
	selp.f64 	%fd198, %fd197, %fd195, %p34;
	ld.shared.f64 	%fd199, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd200, %fd198, %fd199;
	setp.gt.s32 	%p35, %r30, 96;
	selp.f64 	%fd201, %fd200, %fd198, %p35;
	ld.shared.f64 	%fd202, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd203, %fd201, %fd202;
	setp.gt.s32 	%p36, %r30, 128;
	selp.f64 	%fd204, %fd203, %fd201, %p36;
	ld.shared.f64 	%fd205, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd206, %fd204, %fd205;
	setp.gt.s32 	%p37, %r30, 160;
	selp.f64 	%fd207, %fd206, %fd204, %p37;
	ld.shared.f64 	%fd208, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd209, %fd207, %fd208;
	setp.gt.s32 	%p38, %r30, 192;
	selp.f64 	%fd446, %fd209, %fd207, %p38;
	setp.lt.s32 	%p39, %r30, 225;
	@%p39 bra 	$L__BB8_71;

	ld.shared.f64 	%fd210, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd446, %fd210;
	bra.uni 	$L__BB8_71;

$L__BB8_62:
	setp.ne.s32 	%p70, %r473, 0;
	shl.b32 	%r532, %r74, 5;
	add.s32 	%r533, %r532, 32;
	setp.gt.s32 	%p71, %r533, %r61;
	// begin inline asm
	mov.u32 %r481, %laneid;
	// end inline asm
	not.b32 	%r534, %r532;
	mov.u32 	%r531, -1;
	add.s32 	%r535, %r61, %r534;
	selp.b32 	%r530, %r535, 31, %p71;
	mov.u32 	%r489, 1;
	// begin inline asm
	shfl.sync.down.b32 %r482, %r76, %r489, %r530, %r531;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r487, %r77, %r489, %r530, %r531;
	// end inline asm
	mov.b64 	%rd339, {%r482, %r487};
	mov.b64 	%fd361, %rd339;
	setp.lt.s32 	%p72, %r481, %r530;
	add.f64 	%fd362, %fd445, %fd361;
	selp.f64 	%fd363, %fd362, %fd445, %p72;
	mov.b64 	%rd340, %fd363;
	mov.u32 	%r499, 2;
	mov.b64 	{%r493, %r498}, %rd340;
	// begin inline asm
	shfl.sync.down.b32 %r492, %r493, %r499, %r530, %r531;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r497, %r498, %r499, %r530, %r531;
	// end inline asm
	mov.b64 	%rd341, {%r492, %r497};
	mov.b64 	%fd364, %rd341;
	add.s32 	%r536, %r481, 2;
	setp.gt.s32 	%p73, %r536, %r530;
	add.f64 	%fd365, %fd363, %fd364;
	selp.f64 	%fd366, %fd363, %fd365, %p73;
	mov.b64 	%rd342, %fd366;
	mov.u32 	%r509, 4;
	mov.b64 	{%r503, %r508}, %rd342;
	// begin inline asm
	shfl.sync.down.b32 %r502, %r503, %r509, %r530, %r531;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r507, %r508, %r509, %r530, %r531;
	// end inline asm
	mov.b64 	%rd343, {%r502, %r507};
	mov.b64 	%fd367, %rd343;
	add.s32 	%r537, %r481, 4;
	setp.gt.s32 	%p74, %r537, %r530;
	add.f64 	%fd368, %fd366, %fd367;
	selp.f64 	%fd369, %fd366, %fd368, %p74;
	mov.b64 	%rd344, %fd369;
	mov.u32 	%r519, 8;
	mov.b64 	{%r513, %r518}, %rd344;
	// begin inline asm
	shfl.sync.down.b32 %r512, %r513, %r519, %r530, %r531;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r517, %r518, %r519, %r530, %r531;
	// end inline asm
	mov.b64 	%rd345, {%r512, %r517};
	mov.b64 	%fd370, %rd345;
	add.s32 	%r538, %r481, 8;
	setp.gt.s32 	%p75, %r538, %r530;
	add.f64 	%fd371, %fd369, %fd370;
	selp.f64 	%fd372, %fd369, %fd371, %p75;
	mov.b64 	%rd346, %fd372;
	mov.u32 	%r529, 16;
	mov.b64 	{%r523, %r528}, %rd346;
	// begin inline asm
	shfl.sync.down.b32 %r522, %r523, %r529, %r530, %r531;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r527, %r528, %r529, %r530, %r531;
	// end inline asm
	mov.b64 	%rd347, {%r522, %r527};
	mov.b64 	%fd373, %rd347;
	add.s32 	%r539, %r481, 16;
	setp.gt.s32 	%p76, %r539, %r530;
	add.f64 	%fd374, %fd372, %fd373;
	selp.f64 	%fd446, %fd372, %fd374, %p76;
	@%p70 bra 	$L__BB8_64;

	add.s32 	%r597, %r480, 8;
	st.shared.f64 	[%r597], %fd446;

$L__BB8_64:
	bar.sync 	0;
	setp.ne.s32 	%p77, %r474, 0;
	@%p77 bra 	$L__BB8_71;

	setp.gt.s32 	%p78, %r61, 32;
	ld.shared.f64 	%fd375, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+16];
	add.f64 	%fd376, %fd446, %fd375;
	selp.f64 	%fd377, %fd376, %fd446, %p78;
	ld.shared.f64 	%fd378, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+24];
	add.f64 	%fd379, %fd377, %fd378;
	setp.gt.s32 	%p79, %r61, 64;
	selp.f64 	%fd380, %fd379, %fd377, %p79;
	ld.shared.f64 	%fd381, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+32];
	add.f64 	%fd382, %fd380, %fd381;
	setp.gt.s32 	%p80, %r61, 96;
	selp.f64 	%fd383, %fd382, %fd380, %p80;
	ld.shared.f64 	%fd384, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+40];
	add.f64 	%fd385, %fd383, %fd384;
	setp.gt.s32 	%p81, %r61, 128;
	selp.f64 	%fd386, %fd385, %fd383, %p81;
	ld.shared.f64 	%fd387, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+48];
	add.f64 	%fd388, %fd386, %fd387;
	setp.gt.s32 	%p82, %r61, 160;
	selp.f64 	%fd389, %fd388, %fd386, %p82;
	ld.shared.f64 	%fd390, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+56];
	add.f64 	%fd391, %fd389, %fd390;
	setp.gt.s32 	%p83, %r61, 192;
	selp.f64 	%fd446, %fd391, %fd389, %p83;
	setp.lt.s32 	%p84, %r61, 225;
	@%p84 bra 	$L__BB8_71;

	ld.shared.f64 	%fd392, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPfPdlS4_EEvT0_T1_T2_NS_13GridEvenShareISB_EET3_E12temp_storage+64];
	add.f64 	%fd446, %fd446, %fd392;

$L__BB8_71:
	mov.u32 	%r593, %tid.x;
	setp.ne.s32 	%p92, %r593, 0;
	@%p92 bra 	$L__BB8_73;

	cvta.to.global.u64 	%rd357, %rd56;
	mul.wide.u32 	%rd358, %r80, 8;
	add.s64 	%rd359, %rd357, %rd358;
	st.global.f64 	[%rd359], %fd446;

$L__BB8_73:
	ret;

}
	// .globl	_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_
.visible .entry _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_(
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_0,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_1,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_2,
	.param .align 1 .b8 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_3[1],
	.param .f64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_4
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<98>;
	.reg .b32 	%r<472>;
	.reg .f64 	%fd<616>;
	.reg .b64 	%rd<480>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage[80];

	ld.param.u64 	%rd54, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_0];
	ld.param.u64 	%rd55, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_1];
	ld.param.u64 	%rd56, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_2];
	ld.param.f64 	%fd60, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_4];
	setp.eq.s64 	%p1, %rd56, 0;
	@%p1 bra 	$L__BB9_78;

	and.b64  	%rd57, %rd54, 31;
	setp.eq.s64 	%p2, %rd57, 0;
	@%p2 bra 	$L__BB9_37;

	setp.lt.s64 	%p3, %rd56, 5120;
	@%p3 bra 	$L__BB9_18;
	bra.uni 	$L__BB9_3;

$L__BB9_18:
	cvt.u32.u64 	%r13, %rd56;
	mov.u32 	%r461, %tid.x;
	setp.ge.s32 	%p19, %r461, %r13;
	@%p19 bra 	$L__BB9_20;

	mov.u32 	%r130, %tid.x;
	mul.wide.s32 	%rd170, %r130, 8;
	add.s64 	%rd169, %rd54, %rd170;
	// begin inline asm
	ld.global.nc.u64 %rd168, [%rd169];
	// end inline asm
	mov.b64 	%fd598, %rd168;
	add.s32 	%r461, %r130, 256;

$L__BB9_20:
	setp.ge.s32 	%p20, %r461, %r13;
	@%p20 bra 	$L__BB9_27;

	not.b32 	%r131, %r461;
	add.s32 	%r17, %r131, %r13;
	shr.u32 	%r132, %r17, 8;
	add.s32 	%r133, %r132, 1;
	and.b32  	%r460, %r133, 3;
	setp.eq.s32 	%p21, %r460, 0;
	@%p21 bra 	$L__BB9_24;

	mul.wide.s32 	%rd171, %r461, 8;
	add.s64 	%rd464, %rd54, %rd171;

$L__BB9_23:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd172, [%rd464];
	// end inline asm
	mov.b64 	%fd176, %rd172;
	add.f64 	%fd598, %fd598, %fd176;
	add.s32 	%r461, %r461, 256;
	add.s64 	%rd464, %rd464, 2048;
	add.s32 	%r460, %r460, -1;
	setp.ne.s32 	%p22, %r460, 0;
	@%p22 bra 	$L__BB9_23;

$L__BB9_24:
	setp.lt.u32 	%p23, %r17, 768;
	@%p23 bra 	$L__BB9_27;

	mul.wide.s32 	%rd174, %r461, 8;
	add.s64 	%rd465, %rd54, %rd174;

$L__BB9_26:
	// begin inline asm
	ld.global.nc.u64 %rd175, [%rd465];
	// end inline asm
	mov.b64 	%fd177, %rd175;
	add.f64 	%fd178, %fd598, %fd177;
	add.s64 	%rd178, %rd465, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd177, [%rd178];
	// end inline asm
	mov.b64 	%fd179, %rd177;
	add.f64 	%fd180, %fd178, %fd179;
	add.s64 	%rd180, %rd465, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd179, [%rd180];
	// end inline asm
	mov.b64 	%fd181, %rd179;
	add.f64 	%fd182, %fd180, %fd181;
	add.s64 	%rd182, %rd465, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd181, [%rd182];
	// end inline asm
	mov.b64 	%fd183, %rd181;
	add.f64 	%fd598, %fd182, %fd183;
	add.s64 	%rd465, %rd465, 8192;
	add.s32 	%r461, %r461, 1024;
	setp.lt.s32 	%p24, %r461, %r13;
	@%p24 bra 	$L__BB9_26;

$L__BB9_27:
	mov.u32 	%r135, %tid.x;
	shr.s32 	%r136, %r135, 31;
	shr.u32 	%r137, %r136, 27;
	add.s32 	%r138, %r135, %r137;
	shr.s32 	%r26, %r138, 5;
	// begin inline asm
	mov.u32 %r134, %laneid;
	// end inline asm
	mov.b64 	%rd183, %fd598;
	mov.b64 	{%r28, %r29}, %rd183;
	shl.b32 	%r139, %r26, 3;
	mov.u32 	%r140, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r141, %r140, %r139;
	setp.gt.s32 	%p25, %r13, 255;
	@%p25 bra 	$L__BB9_33;
	bra.uni 	$L__BB9_28;

$L__BB9_33:
	setp.ne.s32 	%p41, %r134, 0;
	// begin inline asm
	mov.u32 %r202, %laneid;
	// end inline asm
	mov.u32 	%r210, 1;
	mov.u32 	%r251, 31;
	mov.u32 	%r252, -1;
	// begin inline asm
	shfl.sync.down.b32 %r203, %r28, %r210, %r251, %r252;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r208, %r29, %r210, %r251, %r252;
	// end inline asm
	mov.b64 	%rd193, {%r203, %r208};
	mov.b64 	%fd216, %rd193;
	setp.gt.s32 	%p42, %r202, 30;
	add.f64 	%fd217, %fd598, %fd216;
	selp.f64 	%fd218, %fd598, %fd217, %p42;
	mov.b64 	%rd194, %fd218;
	mov.u32 	%r220, 2;
	mov.b64 	{%r214, %r219}, %rd194;
	// begin inline asm
	shfl.sync.down.b32 %r213, %r214, %r220, %r251, %r252;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r218, %r219, %r220, %r251, %r252;
	// end inline asm
	mov.b64 	%rd195, {%r213, %r218};
	mov.b64 	%fd219, %rd195;
	setp.gt.s32 	%p43, %r202, 29;
	add.f64 	%fd220, %fd218, %fd219;
	selp.f64 	%fd221, %fd218, %fd220, %p43;
	mov.b64 	%rd196, %fd221;
	mov.u32 	%r230, 4;
	mov.b64 	{%r224, %r229}, %rd196;
	// begin inline asm
	shfl.sync.down.b32 %r223, %r224, %r230, %r251, %r252;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r228, %r229, %r230, %r251, %r252;
	// end inline asm
	mov.b64 	%rd197, {%r223, %r228};
	mov.b64 	%fd222, %rd197;
	setp.gt.s32 	%p44, %r202, 27;
	add.f64 	%fd223, %fd221, %fd222;
	selp.f64 	%fd224, %fd221, %fd223, %p44;
	mov.b64 	%rd198, %fd224;
	mov.u32 	%r240, 8;
	mov.b64 	{%r234, %r239}, %rd198;
	// begin inline asm
	shfl.sync.down.b32 %r233, %r234, %r240, %r251, %r252;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r238, %r239, %r240, %r251, %r252;
	// end inline asm
	mov.b64 	%rd199, {%r233, %r238};
	mov.b64 	%fd225, %rd199;
	setp.gt.s32 	%p45, %r202, 23;
	add.f64 	%fd226, %fd224, %fd225;
	selp.f64 	%fd227, %fd224, %fd226, %p45;
	mov.b64 	%rd200, %fd227;
	mov.u32 	%r250, 16;
	mov.b64 	{%r244, %r249}, %rd200;
	// begin inline asm
	shfl.sync.down.b32 %r243, %r244, %r250, %r251, %r252;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r248, %r249, %r250, %r251, %r252;
	// end inline asm
	mov.b64 	%rd201, {%r243, %r248};
	mov.b64 	%fd228, %rd201;
	setp.gt.s32 	%p46, %r202, 15;
	add.f64 	%fd229, %fd227, %fd228;
	selp.f64 	%fd615, %fd227, %fd229, %p46;
	@%p41 bra 	$L__BB9_35;

	add.s32 	%r451, %r141, 8;
	st.shared.f64 	[%r451], %fd615;

$L__BB9_35:
	bar.sync 	0;
	setp.ne.s32 	%p47, %r135, 0;
	@%p47 bra 	$L__BB9_76;

	ld.shared.f64 	%fd230, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd231, %fd615, %fd230;
	ld.shared.f64 	%fd232, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd233, %fd231, %fd232;
	ld.shared.f64 	%fd234, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd235, %fd233, %fd234;
	ld.shared.f64 	%fd236, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd237, %fd235, %fd236;
	ld.shared.f64 	%fd238, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd239, %fd237, %fd238;
	ld.shared.f64 	%fd240, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd241, %fd239, %fd240;
	ld.shared.f64 	%fd242, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd241, %fd242;
	bra.uni 	$L__BB9_76;

$L__BB9_78:
	mov.u32 	%r449, %tid.x;
	setp.ne.s32 	%p97, %r449, 0;
	@%p97 bra 	$L__BB9_80;

	cvta.to.global.u64 	%rd457, %rd55;
	st.global.f64 	[%rd457], %fd60;
	bra.uni 	$L__BB9_80;

$L__BB9_37:
	setp.lt.s64 	%p48, %rd56, 5120;
	@%p48 bra 	$L__BB9_57;
	bra.uni 	$L__BB9_38;

$L__BB9_57:
	cvt.u32.u64 	%r43, %rd56;
	mov.u32 	%r470, %tid.x;
	setp.ge.s32 	%p67, %r470, %r43;
	@%p67 bra 	$L__BB9_59;

	mov.u32 	%r324, %tid.x;
	mul.wide.s32 	%rd424, %r324, 8;
	add.s64 	%rd423, %rd54, %rd424;
	// begin inline asm
	ld.global.nc.u64 %rd422, [%rd423];
	// end inline asm
	mov.b64 	%fd614, %rd422;
	add.s32 	%r470, %r324, 256;

$L__BB9_59:
	setp.ge.s32 	%p68, %r470, %r43;
	@%p68 bra 	$L__BB9_66;

	not.b32 	%r325, %r470;
	add.s32 	%r47, %r325, %r43;
	shr.u32 	%r326, %r47, 8;
	add.s32 	%r327, %r326, 1;
	and.b32  	%r469, %r327, 3;
	setp.eq.s32 	%p69, %r469, 0;
	@%p69 bra 	$L__BB9_63;

	mul.wide.s32 	%rd425, %r470, 8;
	add.s64 	%rd478, %rd54, %rd425;

$L__BB9_62:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd426, [%rd478];
	// end inline asm
	mov.b64 	%fd518, %rd426;
	add.f64 	%fd614, %fd614, %fd518;
	add.s32 	%r470, %r470, 256;
	add.s64 	%rd478, %rd478, 2048;
	add.s32 	%r469, %r469, -1;
	setp.ne.s32 	%p70, %r469, 0;
	@%p70 bra 	$L__BB9_62;

$L__BB9_63:
	setp.lt.u32 	%p71, %r47, 768;
	@%p71 bra 	$L__BB9_66;

	mul.wide.s32 	%rd428, %r470, 8;
	add.s64 	%rd479, %rd54, %rd428;

$L__BB9_65:
	// begin inline asm
	ld.global.nc.u64 %rd429, [%rd479];
	// end inline asm
	mov.b64 	%fd519, %rd429;
	add.f64 	%fd520, %fd614, %fd519;
	add.s64 	%rd432, %rd479, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd431, [%rd432];
	// end inline asm
	mov.b64 	%fd521, %rd431;
	add.f64 	%fd522, %fd520, %fd521;
	add.s64 	%rd434, %rd479, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd433, [%rd434];
	// end inline asm
	mov.b64 	%fd523, %rd433;
	add.f64 	%fd524, %fd522, %fd523;
	add.s64 	%rd436, %rd479, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd435, [%rd436];
	// end inline asm
	mov.b64 	%fd525, %rd435;
	add.f64 	%fd614, %fd524, %fd525;
	add.s64 	%rd479, %rd479, 8192;
	add.s32 	%r470, %r470, 1024;
	setp.lt.s32 	%p72, %r470, %r43;
	@%p72 bra 	$L__BB9_65;

$L__BB9_66:
	mov.u32 	%r329, %tid.x;
	shr.s32 	%r330, %r329, 31;
	shr.u32 	%r331, %r330, 27;
	add.s32 	%r332, %r329, %r331;
	shr.s32 	%r56, %r332, 5;
	// begin inline asm
	mov.u32 %r328, %laneid;
	// end inline asm
	mov.b64 	%rd437, %fd614;
	mov.b64 	{%r58, %r59}, %rd437;
	shl.b32 	%r333, %r56, 3;
	mov.u32 	%r334, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r335, %r334, %r333;
	setp.gt.s32 	%p73, %r43, 255;
	@%p73 bra 	$L__BB9_72;
	bra.uni 	$L__BB9_67;

$L__BB9_72:
	setp.ne.s32 	%p89, %r328, 0;
	// begin inline asm
	mov.u32 %r396, %laneid;
	// end inline asm
	mov.u32 	%r404, 1;
	mov.u32 	%r445, 31;
	mov.u32 	%r446, -1;
	// begin inline asm
	shfl.sync.down.b32 %r397, %r58, %r404, %r445, %r446;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r402, %r59, %r404, %r445, %r446;
	// end inline asm
	mov.b64 	%rd447, {%r397, %r402};
	mov.b64 	%fd558, %rd447;
	setp.gt.s32 	%p90, %r396, 30;
	add.f64 	%fd559, %fd614, %fd558;
	selp.f64 	%fd560, %fd614, %fd559, %p90;
	mov.b64 	%rd448, %fd560;
	mov.u32 	%r414, 2;
	mov.b64 	{%r408, %r413}, %rd448;
	// begin inline asm
	shfl.sync.down.b32 %r407, %r408, %r414, %r445, %r446;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r412, %r413, %r414, %r445, %r446;
	// end inline asm
	mov.b64 	%rd449, {%r407, %r412};
	mov.b64 	%fd561, %rd449;
	setp.gt.s32 	%p91, %r396, 29;
	add.f64 	%fd562, %fd560, %fd561;
	selp.f64 	%fd563, %fd560, %fd562, %p91;
	mov.b64 	%rd450, %fd563;
	mov.u32 	%r424, 4;
	mov.b64 	{%r418, %r423}, %rd450;
	// begin inline asm
	shfl.sync.down.b32 %r417, %r418, %r424, %r445, %r446;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r422, %r423, %r424, %r445, %r446;
	// end inline asm
	mov.b64 	%rd451, {%r417, %r422};
	mov.b64 	%fd564, %rd451;
	setp.gt.s32 	%p92, %r396, 27;
	add.f64 	%fd565, %fd563, %fd564;
	selp.f64 	%fd566, %fd563, %fd565, %p92;
	mov.b64 	%rd452, %fd566;
	mov.u32 	%r434, 8;
	mov.b64 	{%r428, %r433}, %rd452;
	// begin inline asm
	shfl.sync.down.b32 %r427, %r428, %r434, %r445, %r446;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r432, %r433, %r434, %r445, %r446;
	// end inline asm
	mov.b64 	%rd453, {%r427, %r432};
	mov.b64 	%fd567, %rd453;
	setp.gt.s32 	%p93, %r396, 23;
	add.f64 	%fd568, %fd566, %fd567;
	selp.f64 	%fd569, %fd566, %fd568, %p93;
	mov.b64 	%rd454, %fd569;
	mov.u32 	%r444, 16;
	mov.b64 	{%r438, %r443}, %rd454;
	// begin inline asm
	shfl.sync.down.b32 %r437, %r438, %r444, %r445, %r446;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r442, %r443, %r444, %r445, %r446;
	// end inline asm
	mov.b64 	%rd455, {%r437, %r442};
	mov.b64 	%fd570, %rd455;
	setp.gt.s32 	%p94, %r396, 15;
	add.f64 	%fd571, %fd569, %fd570;
	selp.f64 	%fd615, %fd569, %fd571, %p94;
	@%p89 bra 	$L__BB9_74;

	add.s32 	%r453, %r335, 8;
	st.shared.f64 	[%r453], %fd615;

$L__BB9_74:
	bar.sync 	0;
	setp.ne.s32 	%p95, %r329, 0;
	@%p95 bra 	$L__BB9_76;

	ld.shared.f64 	%fd572, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd573, %fd615, %fd572;
	ld.shared.f64 	%fd574, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd575, %fd573, %fd574;
	ld.shared.f64 	%fd576, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd577, %fd575, %fd576;
	ld.shared.f64 	%fd578, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd579, %fd577, %fd578;
	ld.shared.f64 	%fd580, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd581, %fd579, %fd580;
	ld.shared.f64 	%fd582, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd583, %fd581, %fd582;
	ld.shared.f64 	%fd584, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd583, %fd584;
	bra.uni 	$L__BB9_76;

$L__BB9_3:
	mov.u32 	%r61, %tid.x;
	cvt.s64.s32 	%rd1, %r61;
	mul.wide.s32 	%rd99, %r61, 8;
	add.s64 	%rd458, %rd54, %rd99;
	// begin inline asm
	ld.global.nc.u64 %rd58, [%rd458];
	// end inline asm
	add.s64 	%rd61, %rd458, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd60, [%rd61];
	// end inline asm
	add.s64 	%rd63, %rd458, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd62, [%rd63];
	// end inline asm
	add.s64 	%rd65, %rd458, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd64, [%rd65];
	// end inline asm
	add.s64 	%rd67, %rd458, 8192;
	// begin inline asm
	ld.global.nc.u64 %rd66, [%rd67];
	// end inline asm
	add.s64 	%rd69, %rd458, 10240;
	// begin inline asm
	ld.global.nc.u64 %rd68, [%rd69];
	// end inline asm
	add.s64 	%rd71, %rd458, 12288;
	// begin inline asm
	ld.global.nc.u64 %rd70, [%rd71];
	// end inline asm
	add.s64 	%rd73, %rd458, 14336;
	// begin inline asm
	ld.global.nc.u64 %rd72, [%rd73];
	// end inline asm
	add.s64 	%rd75, %rd458, 16384;
	// begin inline asm
	ld.global.nc.u64 %rd74, [%rd75];
	// end inline asm
	add.s64 	%rd77, %rd458, 18432;
	// begin inline asm
	ld.global.nc.u64 %rd76, [%rd77];
	// end inline asm
	add.s64 	%rd79, %rd458, 20480;
	// begin inline asm
	ld.global.nc.u64 %rd78, [%rd79];
	// end inline asm
	add.s64 	%rd81, %rd458, 22528;
	// begin inline asm
	ld.global.nc.u64 %rd80, [%rd81];
	// end inline asm
	add.s64 	%rd83, %rd458, 24576;
	// begin inline asm
	ld.global.nc.u64 %rd82, [%rd83];
	// end inline asm
	add.s64 	%rd85, %rd458, 26624;
	// begin inline asm
	ld.global.nc.u64 %rd84, [%rd85];
	// end inline asm
	add.s64 	%rd87, %rd458, 28672;
	// begin inline asm
	ld.global.nc.u64 %rd86, [%rd87];
	// end inline asm
	add.s64 	%rd89, %rd458, 30720;
	// begin inline asm
	ld.global.nc.u64 %rd88, [%rd89];
	// end inline asm
	add.s64 	%rd91, %rd458, 32768;
	// begin inline asm
	ld.global.nc.u64 %rd90, [%rd91];
	// end inline asm
	add.s64 	%rd93, %rd458, 34816;
	// begin inline asm
	ld.global.nc.u64 %rd92, [%rd93];
	// end inline asm
	add.s64 	%rd95, %rd458, 36864;
	// begin inline asm
	ld.global.nc.u64 %rd94, [%rd95];
	// end inline asm
	add.s64 	%rd97, %rd458, 38912;
	// begin inline asm
	ld.global.nc.u64 %rd96, [%rd97];
	// end inline asm
	mov.b64 	%fd61, %rd58;
	mov.b64 	%fd62, %rd60;
	add.f64 	%fd63, %fd61, %fd62;
	mov.b64 	%fd64, %rd62;
	add.f64 	%fd65, %fd63, %fd64;
	mov.b64 	%fd66, %rd64;
	add.f64 	%fd67, %fd65, %fd66;
	mov.b64 	%fd68, %rd66;
	add.f64 	%fd69, %fd67, %fd68;
	mov.b64 	%fd70, %rd68;
	add.f64 	%fd71, %fd69, %fd70;
	mov.b64 	%fd72, %rd70;
	add.f64 	%fd73, %fd71, %fd72;
	mov.b64 	%fd74, %rd72;
	add.f64 	%fd75, %fd73, %fd74;
	mov.b64 	%fd76, %rd74;
	add.f64 	%fd77, %fd75, %fd76;
	mov.b64 	%fd78, %rd76;
	add.f64 	%fd79, %fd77, %fd78;
	mov.b64 	%fd80, %rd78;
	add.f64 	%fd81, %fd79, %fd80;
	mov.b64 	%fd82, %rd80;
	add.f64 	%fd83, %fd81, %fd82;
	mov.b64 	%fd84, %rd82;
	add.f64 	%fd85, %fd83, %fd84;
	mov.b64 	%fd86, %rd84;
	add.f64 	%fd87, %fd85, %fd86;
	mov.b64 	%fd88, %rd86;
	add.f64 	%fd89, %fd87, %fd88;
	mov.b64 	%fd90, %rd88;
	add.f64 	%fd91, %fd89, %fd90;
	mov.b64 	%fd92, %rd90;
	add.f64 	%fd93, %fd91, %fd92;
	mov.b64 	%fd94, %rd92;
	add.f64 	%fd95, %fd93, %fd94;
	mov.b64 	%fd96, %rd94;
	add.f64 	%fd97, %fd95, %fd96;
	mov.b64 	%fd98, %rd96;
	add.f64 	%fd592, %fd97, %fd98;
	setp.lt.s64 	%p4, %rd56, 10240;
	mov.u64 	%rd461, 5120;
	@%p4 bra 	$L__BB9_6;

	mov.u64 	%rd460, 10240;

$L__BB9_5:
	add.s64 	%rd103, %rd458, 40960;
	// begin inline asm
	ld.global.nc.u64 %rd102, [%rd103];
	// end inline asm
	add.s64 	%rd105, %rd458, 43008;
	// begin inline asm
	ld.global.nc.u64 %rd104, [%rd105];
	// end inline asm
	add.s64 	%rd107, %rd458, 45056;
	// begin inline asm
	ld.global.nc.u64 %rd106, [%rd107];
	// end inline asm
	add.s64 	%rd109, %rd458, 47104;
	// begin inline asm
	ld.global.nc.u64 %rd108, [%rd109];
	// end inline asm
	add.s64 	%rd111, %rd458, 49152;
	// begin inline asm
	ld.global.nc.u64 %rd110, [%rd111];
	// end inline asm
	add.s64 	%rd113, %rd458, 51200;
	// begin inline asm
	ld.global.nc.u64 %rd112, [%rd113];
	// end inline asm
	add.s64 	%rd115, %rd458, 53248;
	// begin inline asm
	ld.global.nc.u64 %rd114, [%rd115];
	// end inline asm
	add.s64 	%rd117, %rd458, 55296;
	// begin inline asm
	ld.global.nc.u64 %rd116, [%rd117];
	// end inline asm
	add.s64 	%rd119, %rd458, 57344;
	// begin inline asm
	ld.global.nc.u64 %rd118, [%rd119];
	// end inline asm
	add.s64 	%rd121, %rd458, 59392;
	// begin inline asm
	ld.global.nc.u64 %rd120, [%rd121];
	// end inline asm
	add.s64 	%rd123, %rd458, 61440;
	// begin inline asm
	ld.global.nc.u64 %rd122, [%rd123];
	// end inline asm
	add.s64 	%rd125, %rd458, 63488;
	// begin inline asm
	ld.global.nc.u64 %rd124, [%rd125];
	// end inline asm
	add.s64 	%rd127, %rd458, 65536;
	// begin inline asm
	ld.global.nc.u64 %rd126, [%rd127];
	// end inline asm
	add.s64 	%rd129, %rd458, 67584;
	// begin inline asm
	ld.global.nc.u64 %rd128, [%rd129];
	// end inline asm
	add.s64 	%rd131, %rd458, 69632;
	// begin inline asm
	ld.global.nc.u64 %rd130, [%rd131];
	// end inline asm
	add.s64 	%rd133, %rd458, 71680;
	// begin inline asm
	ld.global.nc.u64 %rd132, [%rd133];
	// end inline asm
	add.s64 	%rd135, %rd458, 73728;
	// begin inline asm
	ld.global.nc.u64 %rd134, [%rd135];
	// end inline asm
	add.s64 	%rd137, %rd458, 75776;
	// begin inline asm
	ld.global.nc.u64 %rd136, [%rd137];
	// end inline asm
	add.s64 	%rd139, %rd458, 77824;
	// begin inline asm
	ld.global.nc.u64 %rd138, [%rd139];
	// end inline asm
	add.s64 	%rd141, %rd458, 79872;
	// begin inline asm
	ld.global.nc.u64 %rd140, [%rd141];
	// end inline asm
	mov.b64 	%fd99, %rd102;
	add.f64 	%fd100, %fd592, %fd99;
	mov.b64 	%fd101, %rd104;
	add.f64 	%fd102, %fd100, %fd101;
	mov.b64 	%fd103, %rd106;
	add.f64 	%fd104, %fd102, %fd103;
	mov.b64 	%fd105, %rd108;
	add.f64 	%fd106, %fd104, %fd105;
	mov.b64 	%fd107, %rd110;
	add.f64 	%fd108, %fd106, %fd107;
	mov.b64 	%fd109, %rd112;
	add.f64 	%fd110, %fd108, %fd109;
	mov.b64 	%fd111, %rd114;
	add.f64 	%fd112, %fd110, %fd111;
	mov.b64 	%fd113, %rd116;
	add.f64 	%fd114, %fd112, %fd113;
	mov.b64 	%fd115, %rd118;
	add.f64 	%fd116, %fd114, %fd115;
	mov.b64 	%fd117, %rd120;
	add.f64 	%fd118, %fd116, %fd117;
	mov.b64 	%fd119, %rd122;
	add.f64 	%fd120, %fd118, %fd119;
	mov.b64 	%fd121, %rd124;
	add.f64 	%fd122, %fd120, %fd121;
	mov.b64 	%fd123, %rd126;
	add.f64 	%fd124, %fd122, %fd123;
	mov.b64 	%fd125, %rd128;
	add.f64 	%fd126, %fd124, %fd125;
	mov.b64 	%fd127, %rd130;
	add.f64 	%fd128, %fd126, %fd127;
	mov.b64 	%fd129, %rd132;
	add.f64 	%fd130, %fd128, %fd129;
	mov.b64 	%fd131, %rd134;
	add.f64 	%fd132, %fd130, %fd131;
	mov.b64 	%fd133, %rd136;
	add.f64 	%fd134, %fd132, %fd133;
	mov.b64 	%fd135, %rd138;
	add.f64 	%fd136, %fd134, %fd135;
	mov.b64 	%fd137, %rd140;
	add.f64 	%fd592, %fd136, %fd137;
	add.s64 	%rd461, %rd461, 5120;
	add.s64 	%rd460, %rd460, 5120;
	setp.le.s64 	%p5, %rd460, %rd56;
	mov.u64 	%rd458, %rd103;
	@%p5 bra 	$L__BB9_5;

$L__BB9_6:
	setp.ge.s64 	%p6, %rd461, %rd56;
	@%p6 bra 	$L__BB9_14;

	sub.s64 	%rd142, %rd56, %rd461;
	cvt.u32.u64 	%r1, %rd142;
	setp.ge.s32 	%p7, %r61, %r1;
	@%p7 bra 	$L__BB9_14;

	cvt.u32.u64 	%r63, %rd56;
	not.b32 	%r64, %r61;
	add.s32 	%r65, %r64, %r63;
	cvt.u32.u64 	%r66, %rd461;
	sub.s32 	%r3, %r65, %r66;
	shr.u32 	%r67, %r3, 8;
	add.s32 	%r68, %r67, 1;
	and.b32  	%r455, %r68, 3;
	setp.eq.s32 	%p8, %r455, 0;
	mov.u32 	%r456, %r61;
	@%p8 bra 	$L__BB9_11;

	add.s64 	%rd143, %rd461, %rd1;
	shl.b64 	%rd144, %rd143, 3;
	add.s64 	%rd462, %rd54, %rd144;
	mov.u32 	%r456, %tid.x;

$L__BB9_10:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd145, [%rd462];
	// end inline asm
	mov.b64 	%fd139, %rd145;
	add.f64 	%fd592, %fd592, %fd139;
	add.s32 	%r456, %r456, 256;
	add.s64 	%rd462, %rd462, 2048;
	add.s32 	%r455, %r455, -1;
	setp.ne.s32 	%p9, %r455, 0;
	@%p9 bra 	$L__BB9_10;

$L__BB9_11:
	setp.lt.u32 	%p10, %r3, 768;
	@%p10 bra 	$L__BB9_14;

	cvt.s64.s32 	%rd147, %r456;
	add.s64 	%rd148, %rd461, %rd147;
	shl.b64 	%rd149, %rd148, 3;
	add.s64 	%rd463, %rd54, %rd149;

$L__BB9_13:
	// begin inline asm
	ld.global.nc.u64 %rd150, [%rd463];
	// end inline asm
	mov.b64 	%fd140, %rd150;
	add.f64 	%fd141, %fd592, %fd140;
	add.s64 	%rd153, %rd463, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd152, [%rd153];
	// end inline asm
	mov.b64 	%fd142, %rd152;
	add.f64 	%fd143, %fd141, %fd142;
	add.s64 	%rd155, %rd463, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd154, [%rd155];
	// end inline asm
	mov.b64 	%fd144, %rd154;
	add.f64 	%fd145, %fd143, %fd144;
	add.s64 	%rd157, %rd463, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd156, [%rd157];
	// end inline asm
	mov.b64 	%fd146, %rd156;
	add.f64 	%fd592, %fd145, %fd146;
	add.s64 	%rd463, %rd463, 8192;
	add.s32 	%r456, %r456, 1024;
	setp.lt.s32 	%p11, %r456, %r1;
	@%p11 bra 	$L__BB9_13;

$L__BB9_14:
	// begin inline asm
	mov.u32 %r69, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r70, %laneid;
	// end inline asm
	mov.b64 	%rd158, %fd592;
	mov.u32 	%r78, 1;
	mov.u32 	%r119, 31;
	mov.u32 	%r120, -1;
	mov.b64 	{%r72, %r77}, %rd158;
	// begin inline asm
	shfl.sync.down.b32 %r71, %r72, %r78, %r119, %r120;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r76, %r77, %r78, %r119, %r120;
	// end inline asm
	mov.b64 	%rd159, {%r71, %r76};
	mov.b64 	%fd147, %rd159;
	setp.gt.s32 	%p12, %r70, 30;
	add.f64 	%fd148, %fd592, %fd147;
	selp.f64 	%fd149, %fd592, %fd148, %p12;
	mov.b64 	%rd160, %fd149;
	mov.u32 	%r88, 2;
	mov.b64 	{%r82, %r87}, %rd160;
	// begin inline asm
	shfl.sync.down.b32 %r81, %r82, %r88, %r119, %r120;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r86, %r87, %r88, %r119, %r120;
	// end inline asm
	mov.b64 	%rd161, {%r81, %r86};
	mov.b64 	%fd150, %rd161;
	setp.gt.s32 	%p13, %r70, 29;
	add.f64 	%fd151, %fd149, %fd150;
	selp.f64 	%fd152, %fd149, %fd151, %p13;
	mov.b64 	%rd162, %fd152;
	mov.u32 	%r98, 4;
	mov.b64 	{%r92, %r97}, %rd162;
	// begin inline asm
	shfl.sync.down.b32 %r91, %r92, %r98, %r119, %r120;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r96, %r97, %r98, %r119, %r120;
	// end inline asm
	mov.b64 	%rd163, {%r91, %r96};
	mov.b64 	%fd153, %rd163;
	setp.gt.s32 	%p14, %r70, 27;
	add.f64 	%fd154, %fd152, %fd153;
	selp.f64 	%fd155, %fd152, %fd154, %p14;
	mov.b64 	%rd164, %fd155;
	mov.u32 	%r108, 8;
	mov.b64 	{%r102, %r107}, %rd164;
	// begin inline asm
	shfl.sync.down.b32 %r101, %r102, %r108, %r119, %r120;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r106, %r107, %r108, %r119, %r120;
	// end inline asm
	mov.b64 	%rd165, {%r101, %r106};
	mov.b64 	%fd156, %rd165;
	setp.gt.s32 	%p15, %r70, 23;
	add.f64 	%fd157, %fd155, %fd156;
	selp.f64 	%fd158, %fd155, %fd157, %p15;
	mov.b64 	%rd166, %fd158;
	mov.u32 	%r118, 16;
	mov.b64 	{%r112, %r117}, %rd166;
	// begin inline asm
	shfl.sync.down.b32 %r111, %r112, %r118, %r119, %r120;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r116, %r117, %r118, %r119, %r120;
	// end inline asm
	mov.b64 	%rd167, {%r111, %r116};
	mov.b64 	%fd159, %rd167;
	setp.gt.s32 	%p16, %r70, 15;
	add.f64 	%fd160, %fd158, %fd159;
	selp.f64 	%fd615, %fd158, %fd160, %p16;
	setp.ne.s32 	%p17, %r69, 0;
	@%p17 bra 	$L__BB9_16;

	shr.s32 	%r122, %r61, 31;
	shr.u32 	%r123, %r122, 27;
	add.s32 	%r124, %r61, %r123;
	shr.s32 	%r125, %r124, 5;
	shl.b32 	%r126, %r125, 3;
	mov.u32 	%r127, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r128, %r127, %r126;
	st.shared.f64 	[%r128+8], %fd615;

$L__BB9_16:
	bar.sync 	0;
	setp.ne.s32 	%p18, %r61, 0;
	@%p18 bra 	$L__BB9_76;

	ld.shared.f64 	%fd161, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd162, %fd615, %fd161;
	ld.shared.f64 	%fd163, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd164, %fd162, %fd163;
	ld.shared.f64 	%fd165, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd166, %fd164, %fd165;
	ld.shared.f64 	%fd167, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd168, %fd166, %fd167;
	ld.shared.f64 	%fd169, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd170, %fd168, %fd169;
	ld.shared.f64 	%fd171, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd172, %fd170, %fd171;
	ld.shared.f64 	%fd173, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd172, %fd173;
	bra.uni 	$L__BB9_76;

$L__BB9_38:
	mov.u32 	%r254, %tid.x;
	shl.b32 	%r255, %r254, 2;
	mul.wide.u32 	%rd233, %r255, 8;
	add.s64 	%rd204, %rd54, %rd233;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd202, %rd203}, [%rd204];
	// end inline asm
	add.s64 	%rd207, %rd204, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd205, %rd206}, [%rd207];
	// end inline asm
	add.s64 	%rd210, %rd204, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd208, %rd209}, [%rd210];
	// end inline asm
	add.s64 	%rd213, %rd204, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd211, %rd212}, [%rd213];
	// end inline asm
	add.s64 	%rd216, %rd204, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd214, %rd215}, [%rd216];
	// end inline asm
	add.s64 	%rd219, %rd204, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd217, %rd218}, [%rd219];
	// end inline asm
	add.s64 	%rd222, %rd204, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd220, %rd221}, [%rd222];
	// end inline asm
	add.s64 	%rd225, %rd204, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd223, %rd224}, [%rd225];
	// end inline asm
	add.s64 	%rd228, %rd204, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd226, %rd227}, [%rd228];
	// end inline asm
	add.s64 	%rd231, %rd204, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd229, %rd230}, [%rd231];
	// end inline asm
	mov.b64 	%fd243, %rd202;
	mov.b64 	%fd244, %rd203;
	mov.b64 	%fd245, %rd205;
	mov.b64 	%fd246, %rd206;
	mov.b64 	%fd247, %rd208;
	mov.b64 	%fd248, %rd209;
	mov.b64 	%fd249, %rd211;
	mov.b64 	%fd250, %rd212;
	mov.b64 	%fd251, %rd214;
	mov.b64 	%fd252, %rd215;
	mov.b64 	%fd253, %rd217;
	mov.b64 	%fd254, %rd218;
	mov.b64 	%fd255, %rd220;
	mov.b64 	%fd256, %rd221;
	mov.b64 	%fd257, %rd223;
	mov.b64 	%fd258, %rd224;
	mov.b64 	%fd259, %rd226;
	mov.b64 	%fd260, %rd227;
	mov.b64 	%fd261, %rd229;
	mov.b64 	%fd262, %rd230;
	add.f64 	%fd263, %fd243, %fd244;
	add.f64 	%fd264, %fd263, %fd245;
	add.f64 	%fd265, %fd264, %fd246;
	add.f64 	%fd266, %fd265, %fd247;
	add.f64 	%fd267, %fd266, %fd248;
	add.f64 	%fd268, %fd267, %fd249;
	add.f64 	%fd269, %fd268, %fd250;
	add.f64 	%fd270, %fd269, %fd251;
	add.f64 	%fd271, %fd270, %fd252;
	add.f64 	%fd272, %fd271, %fd253;
	add.f64 	%fd273, %fd272, %fd254;
	add.f64 	%fd274, %fd273, %fd255;
	add.f64 	%fd275, %fd274, %fd256;
	add.f64 	%fd276, %fd275, %fd257;
	add.f64 	%fd277, %fd276, %fd258;
	add.f64 	%fd278, %fd277, %fd259;
	add.f64 	%fd279, %fd278, %fd260;
	add.f64 	%fd280, %fd279, %fd261;
	add.f64 	%fd608, %fd280, %fd262;
	setp.lt.s64 	%p49, %rd56, 10240;
	mov.u64 	%rd469, 5120;
	@%p49 bra 	$L__BB9_45;

	add.s64 	%rd237, %rd56, -10240;
	mul.hi.u64 	%rd238, %rd237, -3689348814741910323;
	shr.u64 	%rd239, %rd238, 12;
	mov.u64 	%rd469, 5120;
	add.s64 	%rd23, %rd239, 1;
	and.b64  	%rd24, %rd23, 3;
	setp.lt.u64 	%p50, %rd237, 15360;
	mov.u64 	%rd470, 10240;
	@%p50 bra 	$L__BB9_42;

	sub.s64 	%rd468, %rd23, %rd24;

$L__BB9_41:
	shl.b64 	%rd362, %rd469, 3;
	add.s64 	%rd244, %rd204, %rd362;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd242, %rd243}, [%rd244];
	// end inline asm
	add.s64 	%rd247, %rd244, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd245, %rd246}, [%rd247];
	// end inline asm
	add.s64 	%rd250, %rd244, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd248, %rd249}, [%rd250];
	// end inline asm
	add.s64 	%rd253, %rd244, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd251, %rd252}, [%rd253];
	// end inline asm
	add.s64 	%rd256, %rd244, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd254, %rd255}, [%rd256];
	// end inline asm
	add.s64 	%rd259, %rd244, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd257, %rd258}, [%rd259];
	// end inline asm
	add.s64 	%rd262, %rd244, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd260, %rd261}, [%rd262];
	// end inline asm
	add.s64 	%rd265, %rd244, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd263, %rd264}, [%rd265];
	// end inline asm
	add.s64 	%rd268, %rd244, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd266, %rd267}, [%rd268];
	// end inline asm
	add.s64 	%rd271, %rd244, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd269, %rd270}, [%rd271];
	// end inline asm
	mov.b64 	%fd282, %rd242;
	mov.b64 	%fd283, %rd243;
	mov.b64 	%fd284, %rd245;
	mov.b64 	%fd285, %rd246;
	mov.b64 	%fd286, %rd248;
	mov.b64 	%fd287, %rd249;
	mov.b64 	%fd288, %rd251;
	mov.b64 	%fd289, %rd252;
	mov.b64 	%fd290, %rd254;
	mov.b64 	%fd291, %rd255;
	mov.b64 	%fd292, %rd257;
	mov.b64 	%fd293, %rd258;
	mov.b64 	%fd294, %rd260;
	mov.b64 	%fd295, %rd261;
	mov.b64 	%fd296, %rd263;
	mov.b64 	%fd297, %rd264;
	mov.b64 	%fd298, %rd266;
	mov.b64 	%fd299, %rd267;
	mov.b64 	%fd300, %rd269;
	mov.b64 	%fd301, %rd270;
	add.f64 	%fd302, %fd608, %fd282;
	add.f64 	%fd303, %fd302, %fd283;
	add.f64 	%fd304, %fd303, %fd284;
	add.f64 	%fd305, %fd304, %fd285;
	add.f64 	%fd306, %fd305, %fd286;
	add.f64 	%fd307, %fd306, %fd287;
	add.f64 	%fd308, %fd307, %fd288;
	add.f64 	%fd309, %fd308, %fd289;
	add.f64 	%fd310, %fd309, %fd290;
	add.f64 	%fd311, %fd310, %fd291;
	add.f64 	%fd312, %fd311, %fd292;
	add.f64 	%fd313, %fd312, %fd293;
	add.f64 	%fd314, %fd313, %fd294;
	add.f64 	%fd315, %fd314, %fd295;
	add.f64 	%fd316, %fd315, %fd296;
	add.f64 	%fd317, %fd316, %fd297;
	add.f64 	%fd318, %fd317, %fd298;
	add.f64 	%fd319, %fd318, %fd299;
	add.f64 	%fd320, %fd319, %fd300;
	add.f64 	%fd321, %fd320, %fd301;
	shl.b64 	%rd363, %rd470, 3;
	add.s64 	%rd274, %rd204, %rd363;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd272, %rd273}, [%rd274];
	// end inline asm
	add.s64 	%rd277, %rd274, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd275, %rd276}, [%rd277];
	// end inline asm
	add.s64 	%rd280, %rd274, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd278, %rd279}, [%rd280];
	// end inline asm
	add.s64 	%rd283, %rd274, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd281, %rd282}, [%rd283];
	// end inline asm
	add.s64 	%rd286, %rd274, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd284, %rd285}, [%rd286];
	// end inline asm
	add.s64 	%rd289, %rd274, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd287, %rd288}, [%rd289];
	// end inline asm
	add.s64 	%rd292, %rd274, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd290, %rd291}, [%rd292];
	// end inline asm
	add.s64 	%rd295, %rd274, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd293, %rd294}, [%rd295];
	// end inline asm
	add.s64 	%rd298, %rd274, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd296, %rd297}, [%rd298];
	// end inline asm
	add.s64 	%rd301, %rd274, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd299, %rd300}, [%rd301];
	// end inline asm
	mov.b64 	%fd322, %rd272;
	mov.b64 	%fd323, %rd273;
	mov.b64 	%fd324, %rd275;
	mov.b64 	%fd325, %rd276;
	mov.b64 	%fd326, %rd278;
	mov.b64 	%fd327, %rd279;
	mov.b64 	%fd328, %rd281;
	mov.b64 	%fd329, %rd282;
	mov.b64 	%fd330, %rd284;
	mov.b64 	%fd331, %rd285;
	mov.b64 	%fd332, %rd287;
	mov.b64 	%fd333, %rd288;
	mov.b64 	%fd334, %rd290;
	mov.b64 	%fd335, %rd291;
	mov.b64 	%fd336, %rd293;
	mov.b64 	%fd337, %rd294;
	mov.b64 	%fd338, %rd296;
	mov.b64 	%fd339, %rd297;
	mov.b64 	%fd340, %rd299;
	mov.b64 	%fd341, %rd300;
	add.f64 	%fd342, %fd321, %fd322;
	add.f64 	%fd343, %fd342, %fd323;
	add.f64 	%fd344, %fd343, %fd324;
	add.f64 	%fd345, %fd344, %fd325;
	add.f64 	%fd346, %fd345, %fd326;
	add.f64 	%fd347, %fd346, %fd327;
	add.f64 	%fd348, %fd347, %fd328;
	add.f64 	%fd349, %fd348, %fd329;
	add.f64 	%fd350, %fd349, %fd330;
	add.f64 	%fd351, %fd350, %fd331;
	add.f64 	%fd352, %fd351, %fd332;
	add.f64 	%fd353, %fd352, %fd333;
	add.f64 	%fd354, %fd353, %fd334;
	add.f64 	%fd355, %fd354, %fd335;
	add.f64 	%fd356, %fd355, %fd336;
	add.f64 	%fd357, %fd356, %fd337;
	add.f64 	%fd358, %fd357, %fd338;
	add.f64 	%fd359, %fd358, %fd339;
	add.f64 	%fd360, %fd359, %fd340;
	add.f64 	%fd361, %fd360, %fd341;
	add.s64 	%rd304, %rd274, 40960;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd302, %rd303}, [%rd304];
	// end inline asm
	add.s64 	%rd307, %rd274, 40976;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd305, %rd306}, [%rd307];
	// end inline asm
	add.s64 	%rd310, %rd274, 49152;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd308, %rd309}, [%rd310];
	// end inline asm
	add.s64 	%rd313, %rd274, 49168;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd311, %rd312}, [%rd313];
	// end inline asm
	add.s64 	%rd316, %rd274, 57344;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd314, %rd315}, [%rd316];
	// end inline asm
	add.s64 	%rd319, %rd274, 57360;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd317, %rd318}, [%rd319];
	// end inline asm
	add.s64 	%rd322, %rd274, 65536;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd320, %rd321}, [%rd322];
	// end inline asm
	add.s64 	%rd325, %rd274, 65552;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd323, %rd324}, [%rd325];
	// end inline asm
	add.s64 	%rd328, %rd274, 73728;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd326, %rd327}, [%rd328];
	// end inline asm
	add.s64 	%rd331, %rd274, 73744;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd329, %rd330}, [%rd331];
	// end inline asm
	mov.b64 	%fd362, %rd302;
	mov.b64 	%fd363, %rd303;
	mov.b64 	%fd364, %rd305;
	mov.b64 	%fd365, %rd306;
	mov.b64 	%fd366, %rd308;
	mov.b64 	%fd367, %rd309;
	mov.b64 	%fd368, %rd311;
	mov.b64 	%fd369, %rd312;
	mov.b64 	%fd370, %rd314;
	mov.b64 	%fd371, %rd315;
	mov.b64 	%fd372, %rd317;
	mov.b64 	%fd373, %rd318;
	mov.b64 	%fd374, %rd320;
	mov.b64 	%fd375, %rd321;
	mov.b64 	%fd376, %rd323;
	mov.b64 	%fd377, %rd324;
	mov.b64 	%fd378, %rd326;
	mov.b64 	%fd379, %rd327;
	mov.b64 	%fd380, %rd329;
	mov.b64 	%fd381, %rd330;
	add.f64 	%fd382, %fd361, %fd362;
	add.f64 	%fd383, %fd382, %fd363;
	add.f64 	%fd384, %fd383, %fd364;
	add.f64 	%fd385, %fd384, %fd365;
	add.f64 	%fd386, %fd385, %fd366;
	add.f64 	%fd387, %fd386, %fd367;
	add.f64 	%fd388, %fd387, %fd368;
	add.f64 	%fd389, %fd388, %fd369;
	add.f64 	%fd390, %fd389, %fd370;
	add.f64 	%fd391, %fd390, %fd371;
	add.f64 	%fd392, %fd391, %fd372;
	add.f64 	%fd393, %fd392, %fd373;
	add.f64 	%fd394, %fd393, %fd374;
	add.f64 	%fd395, %fd394, %fd375;
	add.f64 	%fd396, %fd395, %fd376;
	add.f64 	%fd397, %fd396, %fd377;
	add.f64 	%fd398, %fd397, %fd378;
	add.f64 	%fd399, %fd398, %fd379;
	add.f64 	%fd400, %fd399, %fd380;
	add.f64 	%fd401, %fd400, %fd381;
	add.s64 	%rd469, %rd470, 15360;
	add.s64 	%rd334, %rd274, 81920;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd332, %rd333}, [%rd334];
	// end inline asm
	add.s64 	%rd337, %rd274, 81936;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd335, %rd336}, [%rd337];
	// end inline asm
	add.s64 	%rd340, %rd274, 90112;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd338, %rd339}, [%rd340];
	// end inline asm
	add.s64 	%rd343, %rd274, 90128;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd341, %rd342}, [%rd343];
	// end inline asm
	add.s64 	%rd346, %rd274, 98304;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd344, %rd345}, [%rd346];
	// end inline asm
	add.s64 	%rd349, %rd274, 98320;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd347, %rd348}, [%rd349];
	// end inline asm
	add.s64 	%rd352, %rd274, 106496;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd350, %rd351}, [%rd352];
	// end inline asm
	add.s64 	%rd355, %rd274, 106512;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd353, %rd354}, [%rd355];
	// end inline asm
	add.s64 	%rd358, %rd274, 114688;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd356, %rd357}, [%rd358];
	// end inline asm
	add.s64 	%rd361, %rd274, 114704;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd359, %rd360}, [%rd361];
	// end inline asm
	mov.b64 	%fd402, %rd332;
	mov.b64 	%fd403, %rd333;
	mov.b64 	%fd404, %rd335;
	mov.b64 	%fd405, %rd336;
	mov.b64 	%fd406, %rd338;
	mov.b64 	%fd407, %rd339;
	mov.b64 	%fd408, %rd341;
	mov.b64 	%fd409, %rd342;
	mov.b64 	%fd410, %rd344;
	mov.b64 	%fd411, %rd345;
	mov.b64 	%fd412, %rd347;
	mov.b64 	%fd413, %rd348;
	mov.b64 	%fd414, %rd350;
	mov.b64 	%fd415, %rd351;
	mov.b64 	%fd416, %rd353;
	mov.b64 	%fd417, %rd354;
	mov.b64 	%fd418, %rd356;
	mov.b64 	%fd419, %rd357;
	mov.b64 	%fd420, %rd359;
	mov.b64 	%fd421, %rd360;
	add.f64 	%fd422, %fd401, %fd402;
	add.f64 	%fd423, %fd422, %fd403;
	add.f64 	%fd424, %fd423, %fd404;
	add.f64 	%fd425, %fd424, %fd405;
	add.f64 	%fd426, %fd425, %fd406;
	add.f64 	%fd427, %fd426, %fd407;
	add.f64 	%fd428, %fd427, %fd408;
	add.f64 	%fd429, %fd428, %fd409;
	add.f64 	%fd430, %fd429, %fd410;
	add.f64 	%fd431, %fd430, %fd411;
	add.f64 	%fd432, %fd431, %fd412;
	add.f64 	%fd433, %fd432, %fd413;
	add.f64 	%fd434, %fd433, %fd414;
	add.f64 	%fd435, %fd434, %fd415;
	add.f64 	%fd436, %fd435, %fd416;
	add.f64 	%fd437, %fd436, %fd417;
	add.f64 	%fd438, %fd437, %fd418;
	add.f64 	%fd439, %fd438, %fd419;
	add.f64 	%fd440, %fd439, %fd420;
	add.f64 	%fd608, %fd440, %fd421;
	add.s64 	%rd470, %rd470, 20480;
	add.s64 	%rd468, %rd468, -4;
	setp.ne.s64 	%p51, %rd468, 0;
	@%p51 bra 	$L__BB9_41;

$L__BB9_42:
	setp.eq.s64 	%p52, %rd24, 0;
	@%p52 bra 	$L__BB9_45;

	neg.s64 	%rd472, %rd24;
	mov.u64 	%rd474, %rd469;

$L__BB9_44:
	.pragma "nounroll";
	mov.u64 	%rd469, %rd470;
	shl.b64 	%rd394, %rd474, 3;
	add.s64 	%rd366, %rd204, %rd394;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd364, %rd365}, [%rd366];
	// end inline asm
	add.s64 	%rd369, %rd366, 16;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd367, %rd368}, [%rd369];
	// end inline asm
	add.s64 	%rd372, %rd366, 8192;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd370, %rd371}, [%rd372];
	// end inline asm
	add.s64 	%rd375, %rd366, 8208;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd373, %rd374}, [%rd375];
	// end inline asm
	add.s64 	%rd378, %rd366, 16384;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd376, %rd377}, [%rd378];
	// end inline asm
	add.s64 	%rd381, %rd366, 16400;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd379, %rd380}, [%rd381];
	// end inline asm
	add.s64 	%rd384, %rd366, 24576;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd382, %rd383}, [%rd384];
	// end inline asm
	add.s64 	%rd387, %rd366, 24592;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd385, %rd386}, [%rd387];
	// end inline asm
	add.s64 	%rd390, %rd366, 32768;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd388, %rd389}, [%rd390];
	// end inline asm
	add.s64 	%rd393, %rd366, 32784;
	// begin inline asm
	ld.global.nc.v2.u64 {%rd391, %rd392}, [%rd393];
	// end inline asm
	mov.b64 	%fd441, %rd364;
	mov.b64 	%fd442, %rd365;
	mov.b64 	%fd443, %rd367;
	mov.b64 	%fd444, %rd368;
	mov.b64 	%fd445, %rd370;
	mov.b64 	%fd446, %rd371;
	mov.b64 	%fd447, %rd373;
	mov.b64 	%fd448, %rd374;
	mov.b64 	%fd449, %rd376;
	mov.b64 	%fd450, %rd377;
	mov.b64 	%fd451, %rd379;
	mov.b64 	%fd452, %rd380;
	mov.b64 	%fd453, %rd382;
	mov.b64 	%fd454, %rd383;
	mov.b64 	%fd455, %rd385;
	mov.b64 	%fd456, %rd386;
	mov.b64 	%fd457, %rd388;
	mov.b64 	%fd458, %rd389;
	mov.b64 	%fd459, %rd391;
	mov.b64 	%fd460, %rd392;
	add.f64 	%fd461, %fd608, %fd441;
	add.f64 	%fd462, %fd461, %fd442;
	add.f64 	%fd463, %fd462, %fd443;
	add.f64 	%fd464, %fd463, %fd444;
	add.f64 	%fd465, %fd464, %fd445;
	add.f64 	%fd466, %fd465, %fd446;
	add.f64 	%fd467, %fd466, %fd447;
	add.f64 	%fd468, %fd467, %fd448;
	add.f64 	%fd469, %fd468, %fd449;
	add.f64 	%fd470, %fd469, %fd450;
	add.f64 	%fd471, %fd470, %fd451;
	add.f64 	%fd472, %fd471, %fd452;
	add.f64 	%fd473, %fd472, %fd453;
	add.f64 	%fd474, %fd473, %fd454;
	add.f64 	%fd475, %fd474, %fd455;
	add.f64 	%fd476, %fd475, %fd456;
	add.f64 	%fd477, %fd476, %fd457;
	add.f64 	%fd478, %fd477, %fd458;
	add.f64 	%fd479, %fd478, %fd459;
	add.f64 	%fd608, %fd479, %fd460;
	add.s64 	%rd470, %rd469, 5120;
	add.s64 	%rd472, %rd472, 1;
	setp.ne.s64 	%p53, %rd472, 0;
	mov.u64 	%rd474, %rd469;
	@%p53 bra 	$L__BB9_44;

$L__BB9_45:
	setp.ge.s64 	%p54, %rd469, %rd56;
	@%p54 bra 	$L__BB9_53;

	sub.s64 	%rd395, %rd56, %rd469;
	cvt.u32.u64 	%r31, %rd395;
	setp.ge.s32 	%p55, %r254, %r31;
	@%p55 bra 	$L__BB9_53;

	cvt.u32.u64 	%r257, %rd56;
	not.b32 	%r258, %r254;
	add.s32 	%r259, %r258, %r257;
	cvt.u32.u64 	%r260, %rd469;
	sub.s32 	%r33, %r259, %r260;
	shr.u32 	%r261, %r33, 8;
	add.s32 	%r262, %r261, 1;
	and.b32  	%r464, %r262, 3;
	setp.eq.s32 	%p56, %r464, 0;
	mov.u32 	%r465, %r254;
	@%p56 bra 	$L__BB9_50;

	mov.u32 	%r465, %tid.x;
	cvt.s64.s32 	%rd396, %r465;
	add.s64 	%rd397, %rd469, %rd396;
	shl.b64 	%rd398, %rd397, 3;
	add.s64 	%rd476, %rd54, %rd398;

$L__BB9_49:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd399, [%rd476];
	// end inline asm
	mov.b64 	%fd481, %rd399;
	add.f64 	%fd608, %fd608, %fd481;
	add.s32 	%r465, %r465, 256;
	add.s64 	%rd476, %rd476, 2048;
	add.s32 	%r464, %r464, -1;
	setp.ne.s32 	%p57, %r464, 0;
	@%p57 bra 	$L__BB9_49;

$L__BB9_50:
	setp.lt.u32 	%p58, %r33, 768;
	@%p58 bra 	$L__BB9_53;

	cvt.s64.s32 	%rd401, %r465;
	add.s64 	%rd402, %rd469, %rd401;
	shl.b64 	%rd403, %rd402, 3;
	add.s64 	%rd477, %rd54, %rd403;

$L__BB9_52:
	// begin inline asm
	ld.global.nc.u64 %rd404, [%rd477];
	// end inline asm
	mov.b64 	%fd482, %rd404;
	add.f64 	%fd483, %fd608, %fd482;
	add.s64 	%rd407, %rd477, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd406, [%rd407];
	// end inline asm
	mov.b64 	%fd484, %rd406;
	add.f64 	%fd485, %fd483, %fd484;
	add.s64 	%rd409, %rd477, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd408, [%rd409];
	// end inline asm
	mov.b64 	%fd486, %rd408;
	add.f64 	%fd487, %fd485, %fd486;
	add.s64 	%rd411, %rd477, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd410, [%rd411];
	// end inline asm
	mov.b64 	%fd488, %rd410;
	add.f64 	%fd608, %fd487, %fd488;
	add.s64 	%rd477, %rd477, 8192;
	add.s32 	%r465, %r465, 1024;
	setp.lt.s32 	%p59, %r465, %r31;
	@%p59 bra 	$L__BB9_52;

$L__BB9_53:
	// begin inline asm
	mov.u32 %r263, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r264, %laneid;
	// end inline asm
	mov.b64 	%rd412, %fd608;
	mov.u32 	%r272, 1;
	mov.u32 	%r313, 31;
	mov.u32 	%r314, -1;
	mov.b64 	{%r266, %r271}, %rd412;
	// begin inline asm
	shfl.sync.down.b32 %r265, %r266, %r272, %r313, %r314;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r270, %r271, %r272, %r313, %r314;
	// end inline asm
	mov.b64 	%rd413, {%r265, %r270};
	mov.b64 	%fd489, %rd413;
	setp.gt.s32 	%p60, %r264, 30;
	add.f64 	%fd490, %fd608, %fd489;
	selp.f64 	%fd491, %fd608, %fd490, %p60;
	mov.b64 	%rd414, %fd491;
	mov.u32 	%r282, 2;
	mov.b64 	{%r276, %r281}, %rd414;
	// begin inline asm
	shfl.sync.down.b32 %r275, %r276, %r282, %r313, %r314;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r280, %r281, %r282, %r313, %r314;
	// end inline asm
	mov.b64 	%rd415, {%r275, %r280};
	mov.b64 	%fd492, %rd415;
	setp.gt.s32 	%p61, %r264, 29;
	add.f64 	%fd493, %fd491, %fd492;
	selp.f64 	%fd494, %fd491, %fd493, %p61;
	mov.b64 	%rd416, %fd494;
	mov.u32 	%r292, 4;
	mov.b64 	{%r286, %r291}, %rd416;
	// begin inline asm
	shfl.sync.down.b32 %r285, %r286, %r292, %r313, %r314;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r290, %r291, %r292, %r313, %r314;
	// end inline asm
	mov.b64 	%rd417, {%r285, %r290};
	mov.b64 	%fd495, %rd417;
	setp.gt.s32 	%p62, %r264, 27;
	add.f64 	%fd496, %fd494, %fd495;
	selp.f64 	%fd497, %fd494, %fd496, %p62;
	mov.b64 	%rd418, %fd497;
	mov.u32 	%r302, 8;
	mov.b64 	{%r296, %r301}, %rd418;
	// begin inline asm
	shfl.sync.down.b32 %r295, %r296, %r302, %r313, %r314;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r300, %r301, %r302, %r313, %r314;
	// end inline asm
	mov.b64 	%rd419, {%r295, %r300};
	mov.b64 	%fd498, %rd419;
	setp.gt.s32 	%p63, %r264, 23;
	add.f64 	%fd499, %fd497, %fd498;
	selp.f64 	%fd500, %fd497, %fd499, %p63;
	mov.b64 	%rd420, %fd500;
	mov.u32 	%r312, 16;
	mov.b64 	{%r306, %r311}, %rd420;
	// begin inline asm
	shfl.sync.down.b32 %r305, %r306, %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r310, %r311, %r312, %r313, %r314;
	// end inline asm
	mov.b64 	%rd421, {%r305, %r310};
	mov.b64 	%fd501, %rd421;
	setp.gt.s32 	%p64, %r264, 15;
	add.f64 	%fd502, %fd500, %fd501;
	selp.f64 	%fd615, %fd500, %fd502, %p64;
	setp.ne.s32 	%p65, %r263, 0;
	@%p65 bra 	$L__BB9_55;

	shr.s32 	%r316, %r254, 31;
	shr.u32 	%r317, %r316, 27;
	add.s32 	%r318, %r254, %r317;
	shr.s32 	%r319, %r318, 5;
	shl.b32 	%r320, %r319, 3;
	mov.u32 	%r321, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r322, %r321, %r320;
	st.shared.f64 	[%r322+8], %fd615;

$L__BB9_55:
	bar.sync 	0;
	setp.ne.s32 	%p66, %r254, 0;
	@%p66 bra 	$L__BB9_76;

	ld.shared.f64 	%fd503, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd504, %fd615, %fd503;
	ld.shared.f64 	%fd505, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd506, %fd504, %fd505;
	ld.shared.f64 	%fd507, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd508, %fd506, %fd507;
	ld.shared.f64 	%fd509, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd510, %fd508, %fd509;
	ld.shared.f64 	%fd511, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd512, %fd510, %fd511;
	ld.shared.f64 	%fd513, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd514, %fd512, %fd513;
	ld.shared.f64 	%fd515, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd514, %fd515;
	bra.uni 	$L__BB9_76;

$L__BB9_28:
	setp.ne.s32 	%p26, %r134, 0;
	shl.b32 	%r193, %r26, 5;
	add.s32 	%r194, %r193, 32;
	setp.gt.s32 	%p27, %r194, %r13;
	// begin inline asm
	mov.u32 %r142, %laneid;
	// end inline asm
	not.b32 	%r195, %r193;
	mov.u32 	%r192, -1;
	add.s32 	%r196, %r13, %r195;
	selp.b32 	%r191, %r196, 31, %p27;
	mov.u32 	%r150, 1;
	// begin inline asm
	shfl.sync.down.b32 %r143, %r28, %r150, %r191, %r192;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r148, %r29, %r150, %r191, %r192;
	// end inline asm
	mov.b64 	%rd184, {%r143, %r148};
	mov.b64 	%fd184, %rd184;
	setp.lt.s32 	%p28, %r142, %r191;
	add.f64 	%fd185, %fd598, %fd184;
	selp.f64 	%fd186, %fd185, %fd598, %p28;
	mov.b64 	%rd185, %fd186;
	mov.u32 	%r160, 2;
	mov.b64 	{%r154, %r159}, %rd185;
	// begin inline asm
	shfl.sync.down.b32 %r153, %r154, %r160, %r191, %r192;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r158, %r159, %r160, %r191, %r192;
	// end inline asm
	mov.b64 	%rd186, {%r153, %r158};
	mov.b64 	%fd187, %rd186;
	add.s32 	%r197, %r142, 2;
	setp.gt.s32 	%p29, %r197, %r191;
	add.f64 	%fd188, %fd186, %fd187;
	selp.f64 	%fd189, %fd186, %fd188, %p29;
	mov.b64 	%rd187, %fd189;
	mov.u32 	%r170, 4;
	mov.b64 	{%r164, %r169}, %rd187;
	// begin inline asm
	shfl.sync.down.b32 %r163, %r164, %r170, %r191, %r192;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r168, %r169, %r170, %r191, %r192;
	// end inline asm
	mov.b64 	%rd188, {%r163, %r168};
	mov.b64 	%fd190, %rd188;
	add.s32 	%r198, %r142, 4;
	setp.gt.s32 	%p30, %r198, %r191;
	add.f64 	%fd191, %fd189, %fd190;
	selp.f64 	%fd192, %fd189, %fd191, %p30;
	mov.b64 	%rd189, %fd192;
	mov.u32 	%r180, 8;
	mov.b64 	{%r174, %r179}, %rd189;
	// begin inline asm
	shfl.sync.down.b32 %r173, %r174, %r180, %r191, %r192;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r178, %r179, %r180, %r191, %r192;
	// end inline asm
	mov.b64 	%rd190, {%r173, %r178};
	mov.b64 	%fd193, %rd190;
	add.s32 	%r199, %r142, 8;
	setp.gt.s32 	%p31, %r199, %r191;
	add.f64 	%fd194, %fd192, %fd193;
	selp.f64 	%fd195, %fd192, %fd194, %p31;
	mov.b64 	%rd191, %fd195;
	mov.u32 	%r190, 16;
	mov.b64 	{%r184, %r189}, %rd191;
	// begin inline asm
	shfl.sync.down.b32 %r183, %r184, %r190, %r191, %r192;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r188, %r189, %r190, %r191, %r192;
	// end inline asm
	mov.b64 	%rd192, {%r183, %r188};
	mov.b64 	%fd196, %rd192;
	add.s32 	%r200, %r142, 16;
	setp.gt.s32 	%p32, %r200, %r191;
	add.f64 	%fd197, %fd195, %fd196;
	selp.f64 	%fd615, %fd195, %fd197, %p32;
	@%p26 bra 	$L__BB9_30;

	add.s32 	%r450, %r141, 8;
	st.shared.f64 	[%r450], %fd615;

$L__BB9_30:
	bar.sync 	0;
	setp.ne.s32 	%p33, %r135, 0;
	@%p33 bra 	$L__BB9_76;

	setp.gt.s32 	%p34, %r13, 32;
	ld.shared.f64 	%fd198, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd199, %fd615, %fd198;
	selp.f64 	%fd200, %fd199, %fd615, %p34;
	ld.shared.f64 	%fd201, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd202, %fd200, %fd201;
	setp.gt.s32 	%p35, %r13, 64;
	selp.f64 	%fd203, %fd202, %fd200, %p35;
	ld.shared.f64 	%fd204, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd205, %fd203, %fd204;
	setp.gt.s32 	%p36, %r13, 96;
	selp.f64 	%fd206, %fd205, %fd203, %p36;
	ld.shared.f64 	%fd207, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd208, %fd206, %fd207;
	setp.gt.s32 	%p37, %r13, 128;
	selp.f64 	%fd209, %fd208, %fd206, %p37;
	ld.shared.f64 	%fd210, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd211, %fd209, %fd210;
	setp.gt.s32 	%p38, %r13, 160;
	selp.f64 	%fd212, %fd211, %fd209, %p38;
	ld.shared.f64 	%fd213, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd214, %fd212, %fd213;
	setp.gt.s32 	%p39, %r13, 192;
	selp.f64 	%fd615, %fd214, %fd212, %p39;
	setp.lt.s32 	%p40, %r13, 225;
	@%p40 bra 	$L__BB9_76;

	ld.shared.f64 	%fd215, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd615, %fd215;
	bra.uni 	$L__BB9_76;

$L__BB9_67:
	setp.ne.s32 	%p74, %r328, 0;
	shl.b32 	%r387, %r56, 5;
	add.s32 	%r388, %r387, 32;
	setp.gt.s32 	%p75, %r388, %r43;
	// begin inline asm
	mov.u32 %r336, %laneid;
	// end inline asm
	not.b32 	%r389, %r387;
	mov.u32 	%r386, -1;
	add.s32 	%r390, %r43, %r389;
	selp.b32 	%r385, %r390, 31, %p75;
	mov.u32 	%r344, 1;
	// begin inline asm
	shfl.sync.down.b32 %r337, %r58, %r344, %r385, %r386;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r342, %r59, %r344, %r385, %r386;
	// end inline asm
	mov.b64 	%rd438, {%r337, %r342};
	mov.b64 	%fd526, %rd438;
	setp.lt.s32 	%p76, %r336, %r385;
	add.f64 	%fd527, %fd614, %fd526;
	selp.f64 	%fd528, %fd527, %fd614, %p76;
	mov.b64 	%rd439, %fd528;
	mov.u32 	%r354, 2;
	mov.b64 	{%r348, %r353}, %rd439;
	// begin inline asm
	shfl.sync.down.b32 %r347, %r348, %r354, %r385, %r386;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r352, %r353, %r354, %r385, %r386;
	// end inline asm
	mov.b64 	%rd440, {%r347, %r352};
	mov.b64 	%fd529, %rd440;
	add.s32 	%r391, %r336, 2;
	setp.gt.s32 	%p77, %r391, %r385;
	add.f64 	%fd530, %fd528, %fd529;
	selp.f64 	%fd531, %fd528, %fd530, %p77;
	mov.b64 	%rd441, %fd531;
	mov.u32 	%r364, 4;
	mov.b64 	{%r358, %r363}, %rd441;
	// begin inline asm
	shfl.sync.down.b32 %r357, %r358, %r364, %r385, %r386;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r362, %r363, %r364, %r385, %r386;
	// end inline asm
	mov.b64 	%rd442, {%r357, %r362};
	mov.b64 	%fd532, %rd442;
	add.s32 	%r392, %r336, 4;
	setp.gt.s32 	%p78, %r392, %r385;
	add.f64 	%fd533, %fd531, %fd532;
	selp.f64 	%fd534, %fd531, %fd533, %p78;
	mov.b64 	%rd443, %fd534;
	mov.u32 	%r374, 8;
	mov.b64 	{%r368, %r373}, %rd443;
	// begin inline asm
	shfl.sync.down.b32 %r367, %r368, %r374, %r385, %r386;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r372, %r373, %r374, %r385, %r386;
	// end inline asm
	mov.b64 	%rd444, {%r367, %r372};
	mov.b64 	%fd535, %rd444;
	add.s32 	%r393, %r336, 8;
	setp.gt.s32 	%p79, %r393, %r385;
	add.f64 	%fd536, %fd534, %fd535;
	selp.f64 	%fd537, %fd534, %fd536, %p79;
	mov.b64 	%rd445, %fd537;
	mov.u32 	%r384, 16;
	mov.b64 	{%r378, %r383}, %rd445;
	// begin inline asm
	shfl.sync.down.b32 %r377, %r378, %r384, %r385, %r386;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r382, %r383, %r384, %r385, %r386;
	// end inline asm
	mov.b64 	%rd446, {%r377, %r382};
	mov.b64 	%fd538, %rd446;
	add.s32 	%r394, %r336, 16;
	setp.gt.s32 	%p80, %r394, %r385;
	add.f64 	%fd539, %fd537, %fd538;
	selp.f64 	%fd615, %fd537, %fd539, %p80;
	@%p74 bra 	$L__BB9_69;

	add.s32 	%r452, %r335, 8;
	st.shared.f64 	[%r452], %fd615;

$L__BB9_69:
	bar.sync 	0;
	setp.ne.s32 	%p81, %r329, 0;
	@%p81 bra 	$L__BB9_76;

	setp.gt.s32 	%p82, %r43, 32;
	ld.shared.f64 	%fd540, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd541, %fd615, %fd540;
	selp.f64 	%fd542, %fd541, %fd615, %p82;
	ld.shared.f64 	%fd543, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd544, %fd542, %fd543;
	setp.gt.s32 	%p83, %r43, 64;
	selp.f64 	%fd545, %fd544, %fd542, %p83;
	ld.shared.f64 	%fd546, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd547, %fd545, %fd546;
	setp.gt.s32 	%p84, %r43, 96;
	selp.f64 	%fd548, %fd547, %fd545, %p84;
	ld.shared.f64 	%fd549, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd550, %fd548, %fd549;
	setp.gt.s32 	%p85, %r43, 128;
	selp.f64 	%fd551, %fd550, %fd548, %p85;
	ld.shared.f64 	%fd552, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd553, %fd551, %fd552;
	setp.gt.s32 	%p86, %r43, 160;
	selp.f64 	%fd554, %fd553, %fd551, %p86;
	ld.shared.f64 	%fd555, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd556, %fd554, %fd555;
	setp.gt.s32 	%p87, %r43, 192;
	selp.f64 	%fd615, %fd556, %fd554, %p87;
	setp.lt.s32 	%p88, %r43, 225;
	@%p88 bra 	$L__BB9_76;

	ld.shared.f64 	%fd557, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIfdlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd615, %fd615, %fd557;

$L__BB9_76:
	mov.u32 	%r448, %tid.x;
	setp.ne.s32 	%p96, %r448, 0;
	@%p96 bra 	$L__BB9_80;

	add.f64 	%fd585, %fd615, %fd60;
	cvta.to.global.u64 	%rd456, %rd55;
	st.global.f64 	[%rd456], %fd585;

$L__BB9_80:
	ret;

}

