//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31442593
// Cuda compilation tools, release 11.7, V11.7.99
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_35
.address_size 64

	// .globl	_Z18computeImpulsivityPKdPdjjjjj
// _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage has been demoted
// _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage has been demoted
// _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage has been demoted
// _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage has been demoted
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust6system3cpp3parE[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust8cuda_cub3parE[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust3seqE[1];
.global .align 1 .b8 _ZN45_INTERNAL_f0bdf9a2_14_fgi_kernels_cu_5e178a4c6thrust6deviceE[1];

.visible .entry _Z18computeImpulsivityPKdPdjjjjj(
	.param .u64 _Z18computeImpulsivityPKdPdjjjjj_param_0,
	.param .u64 _Z18computeImpulsivityPKdPdjjjjj_param_1,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_2,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_3,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_4,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_5,
	.param .u32 _Z18computeImpulsivityPKdPdjjjjj_param_6
)
{
	.local .align 8 .b8 	__local_depot0[112];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<60>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<164>;
	.reg .f64 	%fd<69>;
	.reg .b64 	%rd<106>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd32, [_Z18computeImpulsivityPKdPdjjjjj_param_0];
	ld.param.u64 	%rd31, [_Z18computeImpulsivityPKdPdjjjjj_param_1];
	ld.param.u32 	%r67, [_Z18computeImpulsivityPKdPdjjjjj_param_2];
	ld.param.u32 	%r68, [_Z18computeImpulsivityPKdPdjjjjj_param_3];
	ld.param.u32 	%r71, [_Z18computeImpulsivityPKdPdjjjjj_param_4];
	ld.param.u32 	%r69, [_Z18computeImpulsivityPKdPdjjjjj_param_5];
	ld.param.u32 	%r70, [_Z18computeImpulsivityPKdPdjjjjj_param_6];
	cvta.to.global.u64 	%rd1, %rd32;
	add.u64 	%rd2, %SPL, 0;
	add.u64 	%rd103, %SPL, 40;
	mov.u32 	%r72, %ntid.x;
	mov.u32 	%r73, %ctaid.x;
	mov.u32 	%r74, %tid.x;
	mad.lo.s32 	%r1, %r73, %r72, %r74;
	mov.u32 	%r75, %ntid.y;
	mov.u32 	%r76, %ctaid.y;
	mov.u32 	%r77, %tid.y;
	mad.lo.s32 	%r78, %r76, %r75, %r77;
	mul.lo.s32 	%r79, %r78, %r67;
	mul.lo.s32 	%r2, %r79, %r68;
	add.s32 	%r3, %r2, %r1;
	mul.lo.s32 	%r80, %r68, %r67;
	setp.ge.u32 	%p1, %r1, %r80;
	setp.ge.u32 	%p2, %r78, %r71;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_67;

	shl.b32 	%r4, %r70, 1;
	or.b32  	%r5, %r4, 1;
	div.u32 	%r81, %r1, %r68;
	cvt.u64.u32 	%rd35, %r81;
	cvt.u64.u32 	%rd36, %r70;
	sub.s64 	%rd37, %rd35, %rd36;
	neg.s64 	%rd38, %rd37;
	shr.s64 	%rd39, %rd37, 63;
	cvt.u32.u64 	%r82, %rd38;
	cvt.u32.u64 	%r83, %rd39;
	and.b32  	%r84, %r83, %r82;
	add.s64 	%rd40, %rd35, %rd36;
	cvt.u64.u32 	%rd41, %r67;
	setp.lt.u64 	%p4, %rd40, %rd41;
	mov.u64 	%rd42, 1;
	sub.s64 	%rd43, %rd42, %rd41;
	add.s64 	%rd44, %rd43, %rd40;
	selp.b64 	%rd45, 0, %rd44, %p4;
	cvt.u32.u64 	%r85, %rd45;
	mul.lo.s32 	%r86, %r81, %r68;
	sub.s32 	%r87, %r1, %r86;
	cvt.u64.u32 	%rd46, %r87;
	sub.s64 	%rd47, %rd46, %rd36;
	neg.s64 	%rd48, %rd47;
	shr.s64 	%rd49, %rd47, 63;
	cvt.u32.u64 	%r88, %rd48;
	cvt.u32.u64 	%r89, %rd49;
	and.b32  	%r90, %r89, %r88;
	add.s64 	%rd50, %rd46, %rd36;
	cvt.u64.u32 	%rd51, %r68;
	setp.lt.u64 	%p5, %rd50, %rd51;
	sub.s64 	%rd52, %rd42, %rd51;
	add.s64 	%rd53, %rd52, %rd50;
	selp.b64 	%rd54, 0, %rd53, %p5;
	cvt.u32.u64 	%r91, %rd54;
	sub.s32 	%r92, %r5, %r84;
	sub.s32 	%r6, %r92, %r85;
	sub.s32 	%r93, %r5, %r90;
	sub.s32 	%r7, %r93, %r91;
	sub.s32 	%r94, %r81, %r70;
	add.s32 	%r143, %r94, %r84;
	sub.s32 	%r95, %r87, %r70;
	add.s32 	%r9, %r95, %r90;
	add.s32 	%r10, %r143, %r5;
	setp.ge.u32 	%p6, %r143, %r10;
	@%p6 bra 	$L__BB0_23;

	add.s32 	%r97, %r4, 1;
	add.s32 	%r11, %r9, %r97;
	add.s32 	%r12, %r6, %r143;
	add.s32 	%r13, %r7, %r9;
	and.b32  	%r14, %r97, 3;
	add.s32 	%r15, %r9, 1;
	add.s32 	%r16, %r9, 2;
	add.s32 	%r17, %r9, 3;
	mov.u32 	%r151, 0;
	setp.ge.u32 	%p7, %r9, %r11;
	setp.ge.u32 	%p8, %r9, %r13;
	setp.eq.s32 	%p11, %r14, 1;
	setp.lt.u32 	%p18, %r4, 3;

$L__BB0_3:
	@%p7 bra 	$L__BB0_22;

	mul.lo.s32 	%r20, %r143, %r68;
	setp.ge.u32 	%p9, %r143, %r12;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB0_6;

	add.s32 	%r98, %r9, %r20;
	mul.wide.s32 	%rd55, %r151, 4;
	add.s64 	%rd56, %rd2, %rd55;
	st.local.u32 	[%rd56], %r98;
	add.s32 	%r151, %r151, 1;

$L__BB0_6:
	mov.u32 	%r147, %r15;
	@%p11 bra 	$L__BB0_11;

	setp.ge.u32 	%p13, %r15, %r13;
	or.pred  	%p14, %p9, %p13;
	@%p14 bra 	$L__BB0_9;

	add.s32 	%r99, %r15, %r20;
	mul.wide.s32 	%rd57, %r151, 4;
	add.s64 	%rd58, %rd2, %rd57;
	st.local.u32 	[%rd58], %r99;
	add.s32 	%r151, %r151, 1;

$L__BB0_9:
	setp.ge.u32 	%p15, %r16, %r13;
	or.pred  	%p17, %p9, %p15;
	mov.u32 	%r147, %r17;
	@%p17 bra 	$L__BB0_11;

	add.s32 	%r100, %r16, %r20;
	mul.wide.s32 	%rd59, %r151, 4;
	add.s64 	%rd60, %rd2, %rd59;
	st.local.u32 	[%rd60], %r100;
	add.s32 	%r151, %r151, 1;
	mov.u32 	%r147, %r17;

$L__BB0_11:
	@%p18 bra 	$L__BB0_22;

$L__BB0_13:
	setp.ge.u32 	%p19, %r147, %r13;
	or.pred  	%p21, %p9, %p19;
	@%p21 bra 	$L__BB0_15;

	add.s32 	%r101, %r147, %r20;
	mul.wide.s32 	%rd61, %r151, 4;
	add.s64 	%rd62, %rd2, %rd61;
	st.local.u32 	[%rd62], %r101;
	add.s32 	%r151, %r151, 1;

$L__BB0_15:
	add.s32 	%r32, %r147, 1;
	setp.ge.u32 	%p22, %r32, %r13;
	or.pred  	%p24, %p9, %p22;
	@%p24 bra 	$L__BB0_17;

	add.s32 	%r102, %r32, %r20;
	mul.wide.s32 	%rd63, %r151, 4;
	add.s64 	%rd64, %rd2, %rd63;
	st.local.u32 	[%rd64], %r102;
	add.s32 	%r151, %r151, 1;

$L__BB0_17:
	add.s32 	%r35, %r147, 2;
	setp.ge.u32 	%p25, %r35, %r13;
	or.pred  	%p27, %p9, %p25;
	@%p27 bra 	$L__BB0_19;

	add.s32 	%r103, %r35, %r20;
	mul.wide.s32 	%rd65, %r151, 4;
	add.s64 	%rd66, %rd2, %rd65;
	st.local.u32 	[%rd66], %r103;
	add.s32 	%r151, %r151, 1;

$L__BB0_19:
	add.s32 	%r38, %r147, 3;
	setp.ge.u32 	%p28, %r38, %r13;
	or.pred  	%p30, %p9, %p28;
	@%p30 bra 	$L__BB0_21;

	add.s32 	%r104, %r38, %r20;
	mul.wide.s32 	%rd67, %r151, 4;
	add.s64 	%rd68, %rd2, %rd67;
	st.local.u32 	[%rd68], %r104;
	add.s32 	%r151, %r151, 1;

$L__BB0_21:
	add.s32 	%r147, %r147, 4;
	setp.lt.u32 	%p31, %r147, %r11;
	@%p31 bra 	$L__BB0_13;

$L__BB0_22:
	add.s32 	%r143, %r143, 1;
	setp.lt.u32 	%p32, %r143, %r10;
	@%p32 bra 	$L__BB0_3;

$L__BB0_23:
	mul.lo.s32 	%r105, %r6, %r7;
	and.b32  	%r106, %r105, 255;
	mul.lo.s32 	%r107, %r5, %r5;
	and.b32  	%r108, %r107, 255;
	min.u32 	%r44, %r106, %r108;
	min.u32 	%r45, %r44, %r69;
	setp.eq.s32 	%p33, %r44, 0;
	add.s32 	%r46, %r44, -1;
	@%p33 bra 	$L__BB0_30;

	mul.wide.u32 	%rd69, %r3, 8;
	add.s64 	%rd70, %rd1, %rd69;
	ld.global.f64 	%fd1, [%rd70];
	and.b32  	%r159, %r44, 3;
	setp.lt.u32 	%p34, %r46, 3;
	mov.u32 	%r158, 0;
	@%p34 bra 	$L__BB0_27;

	sub.s32 	%r157, %r44, %r159;

$L__BB0_26:
	mul.wide.u32 	%rd71, %r158, 4;
	add.s64 	%rd72, %rd2, %rd71;
	ld.local.u32 	%r111, [%rd72];
	add.s32 	%r112, %r111, %r2;
	mul.wide.u32 	%rd73, %r112, 8;
	add.s64 	%rd74, %rd1, %rd73;
	ld.global.f64 	%fd27, [%rd74];
	sub.f64 	%fd28, %fd1, %fd27;
	abs.f64 	%fd29, %fd28;
	mul.wide.u32 	%rd75, %r158, 8;
	add.s64 	%rd76, %rd103, %rd75;
	st.local.f64 	[%rd76], %fd29;
	ld.local.u32 	%r113, [%rd72+4];
	add.s32 	%r114, %r113, %r2;
	mul.wide.u32 	%rd77, %r114, 8;
	add.s64 	%rd78, %rd1, %rd77;
	ld.global.f64 	%fd30, [%rd78];
	sub.f64 	%fd31, %fd1, %fd30;
	abs.f64 	%fd32, %fd31;
	st.local.f64 	[%rd76+8], %fd32;
	ld.local.u32 	%r115, [%rd72+8];
	add.s32 	%r116, %r115, %r2;
	mul.wide.u32 	%rd79, %r116, 8;
	add.s64 	%rd80, %rd1, %rd79;
	ld.global.f64 	%fd33, [%rd80];
	sub.f64 	%fd34, %fd1, %fd33;
	abs.f64 	%fd35, %fd34;
	st.local.f64 	[%rd76+16], %fd35;
	ld.local.u32 	%r117, [%rd72+12];
	add.s32 	%r118, %r117, %r2;
	mul.wide.u32 	%rd81, %r118, 8;
	add.s64 	%rd82, %rd1, %rd81;
	ld.global.f64 	%fd36, [%rd82];
	sub.f64 	%fd37, %fd1, %fd36;
	abs.f64 	%fd38, %fd37;
	st.local.f64 	[%rd76+24], %fd38;
	add.s32 	%r158, %r158, 4;
	add.s32 	%r157, %r157, -4;
	setp.ne.s32 	%p35, %r157, 0;
	@%p35 bra 	$L__BB0_26;

$L__BB0_27:
	setp.eq.s32 	%p36, %r159, 0;
	@%p36 bra 	$L__BB0_30;

	mul.wide.u32 	%rd83, %r158, 8;
	add.s64 	%rd100, %rd103, %rd83;
	mul.wide.u32 	%rd84, %r158, 4;
	add.s64 	%rd99, %rd2, %rd84;

$L__BB0_29:
	.pragma "nounroll";
	ld.local.u32 	%r119, [%rd99];
	add.s32 	%r120, %r119, %r2;
	mul.wide.u32 	%rd85, %r120, 8;
	add.s64 	%rd86, %rd1, %rd85;
	ld.global.f64 	%fd39, [%rd86];
	sub.f64 	%fd40, %fd1, %fd39;
	abs.f64 	%fd41, %fd40;
	st.local.f64 	[%rd100], %fd41;
	add.s64 	%rd100, %rd100, 8;
	add.s64 	%rd99, %rd99, 4;
	add.s32 	%r159, %r159, -1;
	setp.ne.s32 	%p37, %r159, 0;
	@%p37 bra 	$L__BB0_29;

$L__BB0_30:
	setp.lt.s32 	%p38, %r44, 2;
	@%p38 bra 	$L__BB0_54;

	add.s32 	%r56, %r44, -2;
	mov.u32 	%r160, 0;

$L__BB0_32:
	not.b32 	%r122, %r160;
	add.s32 	%r123, %r44, %r122;
	setp.lt.s32 	%p39, %r123, 1;
	@%p39 bra 	$L__BB0_53;

	sub.s32 	%r58, %r46, %r160;
	and.b32  	%r59, %r58, 3;
	sub.s32 	%r125, %r56, %r160;
	setp.lt.u32 	%p40, %r125, 3;
	mov.u32 	%r163, 0;
	@%p40 bra 	$L__BB0_44;

	sub.s32 	%r162, %r58, %r59;
	ld.local.f64 	%fd56, [%rd103];

$L__BB0_35:
	cvt.s64.s32 	%rd10, %r163;
	mul.wide.s32 	%rd87, %r163, 8;
	add.s64 	%rd11, %rd103, %rd87;
	ld.local.f64 	%fd57, [%rd11+8];
	setp.leu.f64 	%p41, %fd56, %fd57;
	mul.wide.s32 	%rd88, %r163, 4;
	add.s64 	%rd12, %rd2, %rd88;
	@%p41 bra 	$L__BB0_37;

	st.local.f64 	[%rd11], %fd57;
	st.local.f64 	[%rd11+8], %fd56;
	ld.local.u32 	%r127, [%rd12];
	ld.local.u32 	%r128, [%rd12+4];
	st.local.u32 	[%rd12], %r128;
	st.local.u32 	[%rd12+4], %r127;
	mov.f64 	%fd57, %fd56;

$L__BB0_37:
	ld.local.f64 	%fd58, [%rd11+16];
	setp.leu.f64 	%p42, %fd57, %fd58;
	@%p42 bra 	$L__BB0_39;

	st.local.f64 	[%rd11+8], %fd58;
	st.local.f64 	[%rd11+16], %fd57;
	ld.local.u32 	%r129, [%rd12+4];
	ld.local.u32 	%r130, [%rd12+8];
	st.local.u32 	[%rd12+4], %r130;
	st.local.u32 	[%rd12+8], %r129;
	mov.f64 	%fd58, %fd57;

$L__BB0_39:
	ld.local.f64 	%fd59, [%rd11+24];
	setp.leu.f64 	%p43, %fd58, %fd59;
	@%p43 bra 	$L__BB0_41;

	st.local.f64 	[%rd11+16], %fd59;
	st.local.f64 	[%rd11+24], %fd58;
	ld.local.u32 	%r131, [%rd12+8];
	ld.local.u32 	%r132, [%rd12+12];
	st.local.u32 	[%rd12+8], %r132;
	st.local.u32 	[%rd12+12], %r131;
	mov.f64 	%fd59, %fd58;

$L__BB0_41:
	cvt.u32.u64 	%r133, %rd10;
	add.s32 	%r163, %r133, 4;
	ld.local.f64 	%fd56, [%rd11+32];
	setp.leu.f64 	%p44, %fd59, %fd56;
	@%p44 bra 	$L__BB0_43;

	st.local.f64 	[%rd11+24], %fd56;
	st.local.f64 	[%rd11+32], %fd59;
	ld.local.u32 	%r134, [%rd12+12];
	ld.local.u32 	%r135, [%rd12+16];
	st.local.u32 	[%rd12+12], %r135;
	st.local.u32 	[%rd12+16], %r134;
	mov.f64 	%fd56, %fd59;

$L__BB0_43:
	add.s32 	%r162, %r162, -4;
	setp.ne.s32 	%p45, %r162, 0;
	@%p45 bra 	$L__BB0_35;

$L__BB0_44:
	setp.eq.s32 	%p46, %r59, 0;
	@%p46 bra 	$L__BB0_53;

	mul.wide.s32 	%rd89, %r163, 8;
	add.s64 	%rd13, %rd103, %rd89;
	ld.local.f64 	%fd61, [%rd13+8];
	ld.local.f64 	%fd13, [%rd13];
	setp.leu.f64 	%p47, %fd13, %fd61;
	mul.wide.s32 	%rd90, %r163, 4;
	add.s64 	%rd14, %rd2, %rd90;
	@%p47 bra 	$L__BB0_47;

	st.local.f64 	[%rd13], %fd61;
	st.local.f64 	[%rd13+8], %fd13;
	ld.local.u32 	%r136, [%rd14];
	ld.local.u32 	%r137, [%rd14+4];
	st.local.u32 	[%rd14], %r137;
	st.local.u32 	[%rd14+4], %r136;
	mov.f64 	%fd61, %fd13;

$L__BB0_47:
	setp.eq.s32 	%p48, %r59, 1;
	@%p48 bra 	$L__BB0_53;

	ld.local.f64 	%fd62, [%rd13+16];
	setp.leu.f64 	%p49, %fd61, %fd62;
	@%p49 bra 	$L__BB0_50;

	st.local.f64 	[%rd13+8], %fd62;
	st.local.f64 	[%rd13+16], %fd61;
	ld.local.u32 	%r138, [%rd14+4];
	ld.local.u32 	%r139, [%rd14+8];
	st.local.u32 	[%rd14+4], %r139;
	st.local.u32 	[%rd14+8], %r138;
	mov.f64 	%fd62, %fd61;

$L__BB0_50:
	setp.eq.s32 	%p50, %r59, 2;
	@%p50 bra 	$L__BB0_53;

	ld.local.f64 	%fd17, [%rd13+24];
	setp.leu.f64 	%p51, %fd62, %fd17;
	@%p51 bra 	$L__BB0_53;

	st.local.f64 	[%rd13+16], %fd17;
	st.local.f64 	[%rd13+24], %fd62;
	ld.local.u32 	%r140, [%rd14+8];
	ld.local.u32 	%r141, [%rd14+12];
	st.local.u32 	[%rd14+8], %r141;
	st.local.u32 	[%rd14+12], %r140;

$L__BB0_53:
	add.s32 	%r160, %r160, 1;
	setp.lt.s32 	%p52, %r160, %r46;
	@%p52 bra 	$L__BB0_32;

$L__BB0_54:
	mul.wide.u32 	%rd91, %r45, 8;
	add.s64 	%rd15, %rd103, %rd91;
	setp.eq.s64 	%p53, %rd91, 0;
	mov.f64 	%fd26, 0d0000000000000000;
	@%p53 bra 	$L__BB0_62;

	shl.b32 	%r142, %r45, 3;
	cvt.u64.u32 	%rd92, %r142;
	add.s64 	%rd17, %rd92, -8;
	shr.u64 	%rd93, %rd17, 3;
	add.s64 	%rd94, %rd93, 1;
	and.b64  	%rd102, %rd94, 3;
	setp.eq.s64 	%p54, %rd102, 0;
	mov.f64 	%fd67, 0d0000000000000000;
	@%p54 bra 	$L__BB0_58;

$L__BB0_57:
	.pragma "nounroll";
	ld.local.f64 	%fd46, [%rd103];
	add.f64 	%fd67, %fd67, %fd46;
	add.s64 	%rd103, %rd103, 8;
	add.s64 	%rd102, %rd102, -1;
	setp.ne.s64 	%p55, %rd102, 0;
	@%p55 bra 	$L__BB0_57;

$L__BB0_58:
	setp.lt.u64 	%p56, %rd17, 24;
	@%p56 bra 	$L__BB0_61;

	mov.u64 	%rd105, %rd103;

$L__BB0_60:
	ld.local.f64 	%fd47, [%rd103];
	add.f64 	%fd48, %fd67, %fd47;
	ld.local.f64 	%fd49, [%rd103+8];
	add.f64 	%fd50, %fd48, %fd49;
	ld.local.f64 	%fd51, [%rd103+16];
	add.f64 	%fd52, %fd50, %fd51;
	ld.local.f64 	%fd53, [%rd103+24];
	add.f64 	%fd67, %fd52, %fd53;
	add.s64 	%rd105, %rd105, 32;
	setp.ne.s64 	%p57, %rd105, %rd15;
	add.s64 	%rd103, %rd103, 32;
	@%p57 bra 	$L__BB0_60;

$L__BB0_61:
	cvt.rn.f32.f64 	%f1, %fd67;
	cvt.f64.f32 	%fd26, %f1;

$L__BB0_62:
	cvta.to.global.u64 	%rd95, %rd31;
	mul.wide.u32 	%rd96, %r3, 8;
	add.s64 	%rd30, %rd95, %rd96;
	setp.gtu.f64 	%p58, %fd26, 0d3FA1EB851EB851EC;
	@%p58 bra 	$L__BB0_64;
	bra.uni 	$L__BB0_63;

$L__BB0_64:
	setp.ltu.f64 	%p59, %fd26, 0d3FAEB851EB851EB8;
	@%p59 bra 	$L__BB0_66;
	bra.uni 	$L__BB0_65;

$L__BB0_66:
	add.f64 	%fd54, %fd26, 0dBFA1EB851EB851EC;
	div.rn.f64 	%fd55, %fd54, 0d3F99999999999998;
	st.global.f64 	[%rd30], %fd55;
	bra.uni 	$L__BB0_67;

$L__BB0_63:
	mov.u64 	%rd97, 0;
	st.global.u64 	[%rd30], %rd97;
	bra.uni 	$L__BB0_67;

$L__BB0_65:
	mov.u64 	%rd98, 4607182418800017408;
	st.global.u64 	[%rd30], %rd98;

$L__BB0_67:
	ret;

}
	// .globl	_Z11fuzzyFilterPdjPKdjjjj
.visible .entry _Z11fuzzyFilterPdjPKdjjjj(
	.param .u64 _Z11fuzzyFilterPdjPKdjjjj_param_0,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_1,
	.param .u64 _Z11fuzzyFilterPdjPKdjjjj_param_2,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_3,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_4,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_5,
	.param .u32 _Z11fuzzyFilterPdjPKdjjjj_param_6
)
{
	.local .align 8 .b8 	__local_depot1[112];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<66>;
	.reg .b32 	%r<166>;
	.reg .f64 	%fd<141>;
	.reg .b64 	%rd<87>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd20, [_Z11fuzzyFilterPdjPKdjjjj_param_0];
	ld.param.u32 	%r68, [_Z11fuzzyFilterPdjPKdjjjj_param_1];
	ld.param.u64 	%rd21, [_Z11fuzzyFilterPdjPKdjjjj_param_2];
	ld.param.u32 	%r69, [_Z11fuzzyFilterPdjPKdjjjj_param_3];
	ld.param.u32 	%r70, [_Z11fuzzyFilterPdjPKdjjjj_param_4];
	ld.param.u32 	%r72, [_Z11fuzzyFilterPdjPKdjjjj_param_5];
	ld.param.u32 	%r71, [_Z11fuzzyFilterPdjPKdjjjj_param_6];
	cvta.to.global.u64 	%rd1, %rd21;
	cvta.to.global.u64 	%rd2, %rd20;
	add.u64 	%rd86, %SPL, 0;
	add.u64 	%rd4, %SPL, 40;
	mov.u32 	%r73, %ntid.x;
	mov.u32 	%r74, %ctaid.x;
	mov.u32 	%r75, %tid.x;
	mad.lo.s32 	%r1, %r74, %r73, %r75;
	mov.u32 	%r76, %ntid.y;
	mov.u32 	%r77, %ctaid.y;
	mov.u32 	%r78, %tid.y;
	mad.lo.s32 	%r79, %r77, %r76, %r78;
	mul.lo.s32 	%r80, %r79, %r69;
	mul.lo.s32 	%r2, %r80, %r70;
	mul.lo.s32 	%r81, %r70, %r69;
	setp.ge.u32 	%p1, %r1, %r81;
	setp.ge.u32 	%p2, %r79, %r72;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_65;

	add.s32 	%r82, %r2, %r1;
	cvt.u64.u32 	%rd5, %r82;
	mul.wide.u32 	%rd24, %r82, 8;
	add.s64 	%rd6, %rd2, %rd24;
	ld.global.f64 	%fd1, [%rd6];
	div.u32 	%r83, %r1, %r70;
	cvt.u64.u32 	%rd25, %r83;
	cvt.u64.u32 	%rd26, %r68;
	sub.s64 	%rd27, %rd25, %rd26;
	neg.s64 	%rd28, %rd27;
	shr.s64 	%rd29, %rd27, 63;
	cvt.u32.u64 	%r84, %rd28;
	cvt.u32.u64 	%r85, %rd29;
	and.b32  	%r86, %r85, %r84;
	add.s64 	%rd30, %rd25, %rd26;
	cvt.u64.u32 	%rd31, %r69;
	setp.lt.u64 	%p4, %rd30, %rd31;
	mov.u64 	%rd32, 1;
	sub.s64 	%rd33, %rd32, %rd31;
	add.s64 	%rd34, %rd33, %rd30;
	selp.b64 	%rd35, 0, %rd34, %p4;
	cvt.u32.u64 	%r87, %rd35;
	mul.lo.s32 	%r88, %r83, %r70;
	sub.s32 	%r89, %r1, %r88;
	cvt.u64.u32 	%rd36, %r89;
	sub.s64 	%rd37, %rd36, %rd26;
	neg.s64 	%rd38, %rd37;
	shr.s64 	%rd39, %rd37, 63;
	cvt.u32.u64 	%r90, %rd38;
	cvt.u32.u64 	%r91, %rd39;
	and.b32  	%r92, %r91, %r90;
	add.s64 	%rd40, %rd36, %rd26;
	cvt.u64.u32 	%rd41, %r70;
	setp.lt.u64 	%p5, %rd40, %rd41;
	sub.s64 	%rd42, %rd32, %rd41;
	add.s64 	%rd43, %rd42, %rd40;
	selp.b64 	%rd44, 0, %rd43, %p5;
	cvt.u32.u64 	%r93, %rd44;
	shl.b32 	%r3, %r68, 1;
	or.b32  	%r94, %r3, 1;
	sub.s32 	%r95, %r94, %r86;
	sub.s32 	%r4, %r95, %r87;
	sub.s32 	%r96, %r94, %r92;
	sub.s32 	%r5, %r96, %r93;
	sub.s32 	%r97, %r83, %r68;
	add.s32 	%r144, %r97, %r86;
	sub.s32 	%r98, %r89, %r68;
	add.s32 	%r7, %r98, %r92;
	mul.lo.s32 	%r8, %r4, %r5;
	add.s32 	%r9, %r144, %r94;
	setp.ge.u32 	%p6, %r144, %r9;
	@%p6 bra 	$L__BB1_22;

	add.s32 	%r100, %r3, 1;
	add.s32 	%r10, %r7, %r100;
	add.s32 	%r11, %r4, %r144;
	add.s32 	%r12, %r5, %r7;
	and.b32  	%r13, %r100, 3;
	add.s32 	%r14, %r7, 1;
	add.s32 	%r15, %r7, 2;
	add.s32 	%r16, %r7, 3;
	mov.u32 	%r152, 0;
	setp.ge.u32 	%p7, %r7, %r10;
	setp.ge.u32 	%p8, %r7, %r12;
	setp.eq.s32 	%p11, %r13, 1;
	setp.lt.u32 	%p18, %r3, 3;

$L__BB1_3:
	@%p7 bra 	$L__BB1_21;

	mul.lo.s32 	%r19, %r144, %r70;
	setp.ge.u32 	%p9, %r144, %r11;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB1_6;

	add.s32 	%r101, %r7, %r19;
	mul.wide.s32 	%rd45, %r152, 4;
	add.s64 	%rd46, %rd86, %rd45;
	st.local.u32 	[%rd46], %r101;
	add.s32 	%r152, %r152, 1;

$L__BB1_6:
	mov.u32 	%r148, %r14;
	@%p11 bra 	$L__BB1_11;

	setp.ge.u32 	%p13, %r14, %r12;
	or.pred  	%p14, %p9, %p13;
	@%p14 bra 	$L__BB1_9;

	add.s32 	%r102, %r14, %r19;
	mul.wide.s32 	%rd47, %r152, 4;
	add.s64 	%rd48, %rd86, %rd47;
	st.local.u32 	[%rd48], %r102;
	add.s32 	%r152, %r152, 1;

$L__BB1_9:
	setp.ge.u32 	%p15, %r15, %r12;
	or.pred  	%p17, %p9, %p15;
	mov.u32 	%r148, %r16;
	@%p17 bra 	$L__BB1_11;

	add.s32 	%r103, %r15, %r19;
	mul.wide.s32 	%rd49, %r152, 4;
	add.s64 	%rd50, %rd86, %rd49;
	st.local.u32 	[%rd50], %r103;
	add.s32 	%r152, %r152, 1;
	mov.u32 	%r148, %r16;

$L__BB1_11:
	@%p18 bra 	$L__BB1_21;

$L__BB1_12:
	setp.ge.u32 	%p19, %r148, %r12;
	or.pred  	%p21, %p9, %p19;
	@%p21 bra 	$L__BB1_14;

	add.s32 	%r104, %r148, %r19;
	mul.wide.s32 	%rd51, %r152, 4;
	add.s64 	%rd52, %rd86, %rd51;
	st.local.u32 	[%rd52], %r104;
	add.s32 	%r152, %r152, 1;

$L__BB1_14:
	add.s32 	%r31, %r148, 1;
	setp.ge.u32 	%p22, %r31, %r12;
	or.pred  	%p24, %p9, %p22;
	@%p24 bra 	$L__BB1_16;

	add.s32 	%r105, %r31, %r19;
	mul.wide.s32 	%rd53, %r152, 4;
	add.s64 	%rd54, %rd86, %rd53;
	st.local.u32 	[%rd54], %r105;
	add.s32 	%r152, %r152, 1;

$L__BB1_16:
	add.s32 	%r34, %r148, 2;
	setp.ge.u32 	%p25, %r34, %r12;
	or.pred  	%p27, %p9, %p25;
	@%p27 bra 	$L__BB1_18;

	add.s32 	%r106, %r34, %r19;
	mul.wide.s32 	%rd55, %r152, 4;
	add.s64 	%rd56, %rd86, %rd55;
	st.local.u32 	[%rd56], %r106;
	add.s32 	%r152, %r152, 1;

$L__BB1_18:
	add.s32 	%r37, %r148, 3;
	setp.ge.u32 	%p28, %r37, %r12;
	or.pred  	%p30, %p9, %p28;
	@%p30 bra 	$L__BB1_20;

	add.s32 	%r107, %r37, %r19;
	mul.wide.s32 	%rd57, %r152, 4;
	add.s64 	%rd58, %rd86, %rd57;
	st.local.u32 	[%rd58], %r107;
	add.s32 	%r152, %r152, 1;

$L__BB1_20:
	add.s32 	%r148, %r148, 4;
	setp.lt.u32 	%p31, %r148, %r10;
	@%p31 bra 	$L__BB1_12;

$L__BB1_21:
	add.s32 	%r144, %r144, 1;
	setp.lt.u32 	%p32, %r144, %r9;
	@%p32 bra 	$L__BB1_3;

$L__BB1_22:
	and.b32  	%r43, %r8, 255;
	setp.eq.s32 	%p33, %r43, 0;
	add.s32 	%r44, %r43, -1;
	@%p33 bra 	$L__BB1_29;

	and.b32  	%r160, %r8, 3;
	setp.lt.u32 	%p34, %r44, 3;
	mov.u32 	%r159, 0;
	@%p34 bra 	$L__BB1_26;

	sub.s32 	%r158, %r43, %r160;

$L__BB1_25:
	mul.wide.u32 	%rd59, %r159, 4;
	add.s64 	%rd60, %rd86, %rd59;
	ld.local.u32 	%r110, [%rd60];
	add.s32 	%r111, %r110, %r2;
	mul.wide.u32 	%rd61, %r111, 8;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.f64 	%fd34, [%rd62];
	sub.f64 	%fd35, %fd1, %fd34;
	abs.f64 	%fd36, %fd35;
	mul.wide.u32 	%rd63, %r159, 8;
	add.s64 	%rd64, %rd4, %rd63;
	st.local.f64 	[%rd64], %fd36;
	ld.local.u32 	%r112, [%rd60+4];
	add.s32 	%r113, %r112, %r2;
	mul.wide.u32 	%rd65, %r113, 8;
	add.s64 	%rd66, %rd2, %rd65;
	ld.global.f64 	%fd37, [%rd66];
	sub.f64 	%fd38, %fd1, %fd37;
	abs.f64 	%fd39, %fd38;
	st.local.f64 	[%rd64+8], %fd39;
	ld.local.u32 	%r114, [%rd60+8];
	add.s32 	%r115, %r114, %r2;
	mul.wide.u32 	%rd67, %r115, 8;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.f64 	%fd40, [%rd68];
	sub.f64 	%fd41, %fd1, %fd40;
	abs.f64 	%fd42, %fd41;
	st.local.f64 	[%rd64+16], %fd42;
	ld.local.u32 	%r116, [%rd60+12];
	add.s32 	%r117, %r116, %r2;
	mul.wide.u32 	%rd69, %r117, 8;
	add.s64 	%rd70, %rd2, %rd69;
	ld.global.f64 	%fd43, [%rd70];
	sub.f64 	%fd44, %fd1, %fd43;
	abs.f64 	%fd45, %fd44;
	st.local.f64 	[%rd64+24], %fd45;
	add.s32 	%r159, %r159, 4;
	add.s32 	%r158, %r158, -4;
	setp.ne.s32 	%p35, %r158, 0;
	@%p35 bra 	$L__BB1_25;

$L__BB1_26:
	setp.eq.s32 	%p36, %r160, 0;
	@%p36 bra 	$L__BB1_29;

	mul.wide.u32 	%rd71, %r159, 8;
	add.s64 	%rd85, %rd4, %rd71;
	mul.wide.u32 	%rd72, %r159, 4;
	add.s64 	%rd84, %rd86, %rd72;

$L__BB1_28:
	.pragma "nounroll";
	ld.local.u32 	%r118, [%rd84];
	add.s32 	%r119, %r118, %r2;
	mul.wide.u32 	%rd73, %r119, 8;
	add.s64 	%rd74, %rd2, %rd73;
	ld.global.f64 	%fd46, [%rd74];
	sub.f64 	%fd47, %fd1, %fd46;
	abs.f64 	%fd48, %fd47;
	st.local.f64 	[%rd85], %fd48;
	add.s64 	%rd85, %rd85, 8;
	add.s64 	%rd84, %rd84, 4;
	add.s32 	%r160, %r160, -1;
	setp.ne.s32 	%p37, %r160, 0;
	@%p37 bra 	$L__BB1_28;

$L__BB1_29:
	setp.lt.u32 	%p38, %r43, 2;
	@%p38 bra 	$L__BB1_53;

	add.s32 	%r54, %r43, -2;
	mov.u32 	%r161, 0;

$L__BB1_31:
	not.b32 	%r121, %r161;
	add.s32 	%r122, %r43, %r121;
	setp.lt.s32 	%p39, %r122, 1;
	@%p39 bra 	$L__BB1_52;

	sub.s32 	%r56, %r44, %r161;
	and.b32  	%r57, %r56, 3;
	sub.s32 	%r124, %r54, %r161;
	setp.lt.u32 	%p40, %r124, 3;
	mov.u32 	%r164, 0;
	@%p40 bra 	$L__BB1_43;

	sub.s32 	%r163, %r56, %r57;
	ld.local.f64 	%fd128, [%rd4];

$L__BB1_34:
	cvt.s64.s32 	%rd13, %r164;
	mul.wide.s32 	%rd75, %r164, 8;
	add.s64 	%rd14, %rd4, %rd75;
	ld.local.f64 	%fd129, [%rd14+8];
	setp.leu.f64 	%p41, %fd128, %fd129;
	mul.wide.s32 	%rd76, %r164, 4;
	add.s64 	%rd15, %rd86, %rd76;
	@%p41 bra 	$L__BB1_36;

	st.local.f64 	[%rd14], %fd129;
	st.local.f64 	[%rd14+8], %fd128;
	ld.local.u32 	%r126, [%rd15];
	ld.local.u32 	%r127, [%rd15+4];
	st.local.u32 	[%rd15], %r127;
	st.local.u32 	[%rd15+4], %r126;
	mov.f64 	%fd129, %fd128;

$L__BB1_36:
	ld.local.f64 	%fd130, [%rd14+16];
	setp.leu.f64 	%p42, %fd129, %fd130;
	@%p42 bra 	$L__BB1_38;

	st.local.f64 	[%rd14+8], %fd130;
	st.local.f64 	[%rd14+16], %fd129;
	ld.local.u32 	%r128, [%rd15+4];
	ld.local.u32 	%r129, [%rd15+8];
	st.local.u32 	[%rd15+4], %r129;
	st.local.u32 	[%rd15+8], %r128;
	mov.f64 	%fd130, %fd129;

$L__BB1_38:
	ld.local.f64 	%fd131, [%rd14+24];
	setp.leu.f64 	%p43, %fd130, %fd131;
	@%p43 bra 	$L__BB1_40;

	st.local.f64 	[%rd14+16], %fd131;
	st.local.f64 	[%rd14+24], %fd130;
	ld.local.u32 	%r130, [%rd15+8];
	ld.local.u32 	%r131, [%rd15+12];
	st.local.u32 	[%rd15+8], %r131;
	st.local.u32 	[%rd15+12], %r130;
	mov.f64 	%fd131, %fd130;

$L__BB1_40:
	cvt.u32.u64 	%r132, %rd13;
	add.s32 	%r164, %r132, 4;
	ld.local.f64 	%fd128, [%rd14+32];
	setp.leu.f64 	%p44, %fd131, %fd128;
	@%p44 bra 	$L__BB1_42;

	st.local.f64 	[%rd14+24], %fd128;
	st.local.f64 	[%rd14+32], %fd131;
	ld.local.u32 	%r133, [%rd15+12];
	ld.local.u32 	%r134, [%rd15+16];
	st.local.u32 	[%rd15+12], %r134;
	st.local.u32 	[%rd15+16], %r133;
	mov.f64 	%fd128, %fd131;

$L__BB1_42:
	add.s32 	%r163, %r163, -4;
	setp.ne.s32 	%p45, %r163, 0;
	@%p45 bra 	$L__BB1_34;

$L__BB1_43:
	setp.eq.s32 	%p46, %r57, 0;
	@%p46 bra 	$L__BB1_52;

	mul.wide.s32 	%rd77, %r164, 8;
	add.s64 	%rd16, %rd4, %rd77;
	ld.local.f64 	%fd133, [%rd16+8];
	ld.local.f64 	%fd13, [%rd16];
	setp.leu.f64 	%p47, %fd13, %fd133;
	mul.wide.s32 	%rd78, %r164, 4;
	add.s64 	%rd17, %rd86, %rd78;
	@%p47 bra 	$L__BB1_46;

	st.local.f64 	[%rd16], %fd133;
	st.local.f64 	[%rd16+8], %fd13;
	ld.local.u32 	%r135, [%rd17];
	ld.local.u32 	%r136, [%rd17+4];
	st.local.u32 	[%rd17], %r136;
	st.local.u32 	[%rd17+4], %r135;
	mov.f64 	%fd133, %fd13;

$L__BB1_46:
	setp.eq.s32 	%p48, %r57, 1;
	@%p48 bra 	$L__BB1_52;

	ld.local.f64 	%fd134, [%rd16+16];
	setp.leu.f64 	%p49, %fd133, %fd134;
	@%p49 bra 	$L__BB1_49;

	st.local.f64 	[%rd16+8], %fd134;
	st.local.f64 	[%rd16+16], %fd133;
	ld.local.u32 	%r137, [%rd17+4];
	ld.local.u32 	%r138, [%rd17+8];
	st.local.u32 	[%rd17+4], %r138;
	st.local.u32 	[%rd17+8], %r137;
	mov.f64 	%fd134, %fd133;

$L__BB1_49:
	setp.eq.s32 	%p50, %r57, 2;
	@%p50 bra 	$L__BB1_52;

	ld.local.f64 	%fd17, [%rd16+24];
	setp.leu.f64 	%p51, %fd134, %fd17;
	@%p51 bra 	$L__BB1_52;

	st.local.f64 	[%rd16+16], %fd17;
	st.local.f64 	[%rd16+24], %fd134;
	ld.local.u32 	%r139, [%rd17+8];
	ld.local.u32 	%r140, [%rd17+12];
	st.local.u32 	[%rd17+8], %r140;
	st.local.u32 	[%rd17+12], %r139;

$L__BB1_52:
	add.s32 	%r161, %r161, 1;
	setp.lt.s32 	%p52, %r161, %r44;
	@%p52 bra 	$L__BB1_31;

$L__BB1_53:
	min.u32 	%r65, %r43, %r71;
	setp.eq.s32 	%p53, %r65, 0;
	mov.f64 	%fd139, 0d0000000000000000;
	mov.f64 	%fd140, %fd139;
	@%p53 bra 	$L__BB1_64;

	shl.b64 	%rd79, %rd5, 3;
	add.s64 	%rd80, %rd1, %rd79;
	ld.global.f64 	%fd18, [%rd80];
	mov.f64 	%fd53, 0d3FF0000000000000;
	sub.f64 	%fd19, %fd53, %fd18;
	mov.u32 	%r165, 0;

$L__BB1_55:
	ld.local.u32 	%r142, [%rd86];
	add.s32 	%r143, %r142, %r2;
	mul.wide.u32 	%rd81, %r143, 8;
	add.s64 	%rd82, %rd2, %rd81;
	add.s64 	%rd83, %rd1, %rd81;
	ld.global.f64 	%fd22, [%rd83];
	ld.global.f64 	%fd23, [%rd82];
	sub.f64 	%fd55, %fd1, %fd23;
	abs.f64 	%fd24, %fd55;
	setp.le.f64 	%p54, %fd24, 0d3F7F7BA49A5CEC48;
	mov.f64 	%fd137, %fd53;
	@%p54 bra 	$L__BB1_58;

	setp.ge.f64 	%p55, %fd24, 0d3F9F7BA49A5CEC48;
	mov.f64 	%fd137, 0d0000000000000000;
	@%p55 bra 	$L__BB1_58;

	div.rn.f64 	%fd57, %fd24, 0dBF979CBB73C5B136;
	add.f64 	%fd137, %fd57, 0d3FF5555555555555;

$L__BB1_58:
	setp.lt.f64 	%p56, %fd24, 0d3F8F7BA49A5CEC48;
	setp.gt.f64 	%p57, %fd24, 0d3F7F7BA49A5CEC48;
	and.pred  	%p58, %p57, %p56;
	@%p58 bra 	$L__BB1_62;
	bra.uni 	$L__BB1_59;

$L__BB1_62:
	add.f64 	%fd62, %fd24, 0dBF7F7BA49A5CEC48;
	div.rn.f64 	%fd63, %fd62, 0d3FFF5C28F5C28F5C;
	div.rn.f64 	%fd138, %fd63, 0d406FE00000000000;
	bra.uni 	$L__BB1_63;

$L__BB1_59:
	setp.ge.f64 	%p59, %fd24, 0d3F8F7BA49A5CEC48;
	setp.le.f64 	%p60, %fd24, 0d3F979CBB73C5B136;
	and.pred  	%p61, %p59, %p60;
	mov.f64 	%fd138, 0d3FF0000000000000;
	@%p61 bra 	$L__BB1_63;

	setp.leu.f64 	%p62, %fd24, 0d3F979CBB73C5B136;
	setp.geu.f64 	%p63, %fd24, 0d3F9F7BA49A5CEC48;
	mov.f64 	%fd138, 0d0000000000000000;
	or.pred  	%p64, %p62, %p63;
	@%p64 bra 	$L__BB1_63;

	div.rn.f64 	%fd60, %fd24, 0dBFFF5C28F5C28F5C;
	div.rn.f64 	%fd61, %fd60, 0d406FE00000000000;
	add.f64 	%fd138, %fd61, 0d4010000000000000;

$L__BB1_63:
	mov.f64 	%fd64, 0d3FF0000000000000;
	sub.f64 	%fd65, %fd64, %fd22;
	sub.f64 	%fd66, %fd64, %fd65;
	mul.f64 	%fd67, %fd18, %fd65;
	mul.f64 	%fd68, %fd67, %fd137;
	mul.f64 	%fd69, %fd19, %fd65;
	mul.f64 	%fd70, %fd69, %fd138;
	sub.f64 	%fd71, %fd64, %fd137;
	mul.f64 	%fd72, %fd69, %fd71;
	add.f64 	%fd73, %fd72, %fd70;
	mul.f64 	%fd74, %fd72, %fd70;
	sub.f64 	%fd75, %fd73, %fd74;
	add.f64 	%fd76, %fd68, %fd75;
	mul.f64 	%fd77, %fd68, %fd75;
	sub.f64 	%fd78, %fd76, %fd77;
	add.f64 	%fd79, %fd66, %fd78;
	mul.f64 	%fd80, %fd66, %fd78;
	sub.f64 	%fd81, %fd79, %fd80;
	mul.f64 	%fd82, %fd67, %fd138;
	mul.f64 	%fd83, %fd67, %fd71;
	mul.f64 	%fd84, %fd69, %fd137;
	add.f64 	%fd85, %fd84, %fd83;
	mul.f64 	%fd86, %fd84, %fd83;
	sub.f64 	%fd87, %fd85, %fd86;
	sub.f64 	%fd88, %fd64, %fd81;
	mul.f64 	%fd89, %fd81, %fd88;
	mul.f64 	%fd90, %fd89, 0d3FB9999999999998;
	mul.f64 	%fd91, %fd81, %fd81;
	mul.f64 	%fd92, %fd91, 0d3FB9999999999998;
	mul.f64 	%fd93, %fd92, 0d3FE0000000000000;
	mov.f64 	%fd94, 0d4000000000000000;
	sub.f64 	%fd95, %fd94, %fd82;
	mul.f64 	%fd96, %fd82, %fd95;
	mul.f64 	%fd97, %fd96, 0d3FE999999999999A;
	mul.f64 	%fd98, %fd97, 0d3FE0000000000000;
	mul.f64 	%fd99, %fd87, 0d3FB9999999999998;
	mul.f64 	%fd100, %fd87, %fd99;
	mul.f64 	%fd101, %fd100, 0d3FE0000000000000;
	sub.f64 	%fd102, %fd64, %fd87;
	mul.f64 	%fd103, %fd87, %fd102;
	mul.f64 	%fd104, %fd103, 0d3FB9999999999998;
	mul.f64 	%fd105, %fd88, 0d3FB9999999999998;
	mul.f64 	%fd106, %fd105, 0d3FE0000000000000;
	add.f64 	%fd107, %fd81, %fd81;
	mov.f64 	%fd108, 0d4008000000000000;
	sub.f64 	%fd109, %fd108, %fd107;
	mul.f64 	%fd110, %fd109, 0d3FB9999999999998;
	div.rn.f64 	%fd111, %fd110, 0d4008000000000000;
	fma.rn.f64 	%fd112, %fd87, 0d4000000000000000, 0d400599999999999A;
	fma.rn.f64 	%fd113, %fd87, 0dBFFCCCCCCCCCCCCD, %fd112;
	div.rn.f64 	%fd114, %fd113, 0d4008000000000000;
	mul.f64 	%fd115, %fd102, 0d3FB9999999999998;
	mul.f64 	%fd116, %fd115, 0d3FE0000000000000;
	mul.f64 	%fd117, %fd93, %fd111;
	fma.rn.f64 	%fd118, %fd90, %fd106, %fd117;
	fma.rn.f64 	%fd119, %fd98, 0d3FE0000000000000, %fd118;
	fma.rn.f64 	%fd120, %fd101, %fd114, %fd119;
	fma.rn.f64 	%fd121, %fd104, %fd116, %fd120;
	add.f64 	%fd122, %fd90, %fd93;
	add.f64 	%fd123, %fd98, %fd122;
	add.f64 	%fd124, %fd101, %fd123;
	add.f64 	%fd125, %fd104, %fd124;
	div.rn.f64 	%fd126, %fd121, %fd125;
	fma.rn.f64 	%fd139, %fd23, %fd126, %fd139;
	add.f64 	%fd140, %fd140, %fd126;
	add.s64 	%rd86, %rd86, 4;
	add.s32 	%r165, %r165, 1;
	setp.lt.u32 	%p65, %r165, %r65;
	@%p65 bra 	$L__BB1_55;

$L__BB1_64:
	div.rn.f64 	%fd127, %fd139, %fd140;
	st.global.f64 	[%rd6], %fd127;

$L__BB1_65:
	ret;

}
	// .globl	_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1_
.visible .entry _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1_(
	.param .align 8 .b8 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_0[24],
	.param .u64 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_1
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<6>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_1];
	ld.param.u64 	%rd6, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_0+8];
	ld.param.u64 	%rd7, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_11__transform17unary_transform_fIPdS7_NS5_14no_stencil_tagENS_8identityIdEENS5_21always_true_predicateEEElEESC_lEEvT0_T1__param_0];
	mov.u32 	%r1, %ctaid.x;
	mul.wide.u32 	%rd8, %r1, 512;
	sub.s64 	%rd1, %rd5, %rd8;
	setp.gt.s64 	%p1, %rd1, 511;
	cvta.to.global.u64 	%rd9, %rd7;
	cvta.to.global.u64 	%rd10, %rd6;
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd11, %r2;
	add.s64 	%rd12, %rd8, %rd11;
	shl.b64 	%rd13, %rd12, 3;
	add.s64 	%rd2, %rd9, %rd13;
	add.s64 	%rd3, %rd10, %rd13;
	@%p1 bra 	$L__BB2_5;
	bra.uni 	$L__BB2_1;

$L__BB2_5:
	ld.global.f64 	%fd3, [%rd2];
	st.global.f64 	[%rd3], %fd3;
	ld.global.f64 	%fd4, [%rd2+2048];
	st.global.f64 	[%rd3+2048], %fd4;
	bra.uni 	$L__BB2_6;

$L__BB2_1:
	cvt.s64.s32 	%rd4, %rd1;
	setp.le.s64 	%p2, %rd4, %rd11;
	@%p2 bra 	$L__BB2_3;

	ld.global.f64 	%fd1, [%rd2];
	st.global.f64 	[%rd3], %fd1;

$L__BB2_3:
	add.s32 	%r5, %r2, 256;
	cvt.u64.u32 	%rd15, %r5;
	setp.le.s64 	%p3, %rd4, %rd15;
	@%p3 bra 	$L__BB2_6;

	ld.global.f64 	%fd2, [%rd2+2048];
	st.global.f64 	[%rd3+2048], %fd2;

$L__BB2_6:
	ret;

}
	// .globl	_ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv()
{



	ret;

}
	// .globl	_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_
.visible .entry _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_(
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_0,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_1,
	.param .u32 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_2,
	.param .align 1 .b8 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_3[1],
	.param .f64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_4
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<51>;
	.reg .b32 	%r<265>;
	.reg .f64 	%fd<232>;
	.reg .b64 	%rd<180>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage[80];

	ld.param.u64 	%rd15, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_0];
	ld.param.u64 	%rd16, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_1];
	ld.param.u32 	%r43, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_2];
	ld.param.f64 	%fd32, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4__param_4];
	setp.eq.s32 	%p1, %r43, 0;
	@%p1 bra 	$L__BB4_42;

	setp.lt.s32 	%p2, %r43, 2560;
	@%p2 bra 	$L__BB4_21;
	bra.uni 	$L__BB4_2;

$L__BB4_21:
	mov.u32 	%r263, %tid.x;
	setp.ge.s32 	%p20, %r263, %r43;
	@%p20 bra 	$L__BB4_23;

	mov.u32 	%r122, %tid.x;
	mul.wide.s32 	%rd141, %r122, 8;
	add.s64 	%rd140, %rd15, %rd141;
	// begin inline asm
	ld.global.nc.u64 %rd139, [%rd140];
	// end inline asm
	mov.b64 	%fd230, %rd139;
	add.s32 	%r263, %r122, 256;

$L__BB4_23:
	setp.ge.s32 	%p21, %r263, %r43;
	@%p21 bra 	$L__BB4_30;

	not.b32 	%r123, %r263;
	add.s32 	%r29, %r123, %r43;
	shr.u32 	%r124, %r29, 8;
	add.s32 	%r125, %r124, 1;
	and.b32  	%r262, %r125, 3;
	setp.eq.s32 	%p22, %r262, 0;
	@%p22 bra 	$L__BB4_27;

	mul.wide.s32 	%rd142, %r263, 8;
	add.s64 	%rd178, %rd15, %rd142;

$L__BB4_26:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd143, [%rd178];
	// end inline asm
	mov.b64 	%fd148, %rd143;
	add.f64 	%fd230, %fd230, %fd148;
	add.s32 	%r263, %r263, 256;
	add.s64 	%rd178, %rd178, 2048;
	add.s32 	%r262, %r262, -1;
	setp.ne.s32 	%p23, %r262, 0;
	@%p23 bra 	$L__BB4_26;

$L__BB4_27:
	setp.lt.u32 	%p24, %r29, 768;
	@%p24 bra 	$L__BB4_30;

	mul.wide.s32 	%rd145, %r263, 8;
	add.s64 	%rd179, %rd15, %rd145;

$L__BB4_29:
	// begin inline asm
	ld.global.nc.u64 %rd146, [%rd179];
	// end inline asm
	mov.b64 	%fd149, %rd146;
	add.f64 	%fd150, %fd230, %fd149;
	add.s64 	%rd149, %rd179, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd148, [%rd149];
	// end inline asm
	mov.b64 	%fd151, %rd148;
	add.f64 	%fd152, %fd150, %fd151;
	add.s64 	%rd151, %rd179, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd150, [%rd151];
	// end inline asm
	mov.b64 	%fd153, %rd150;
	add.f64 	%fd154, %fd152, %fd153;
	add.s64 	%rd153, %rd179, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd152, [%rd153];
	// end inline asm
	mov.b64 	%fd155, %rd152;
	add.f64 	%fd230, %fd154, %fd155;
	add.s64 	%rd179, %rd179, 8192;
	add.s32 	%r263, %r263, 1024;
	setp.lt.s32 	%p25, %r263, %r43;
	@%p25 bra 	$L__BB4_29;

$L__BB4_30:
	mov.u32 	%r127, %tid.x;
	shr.s32 	%r128, %r127, 31;
	shr.u32 	%r129, %r128, 27;
	add.s32 	%r130, %r127, %r129;
	shr.s32 	%r38, %r130, 5;
	// begin inline asm
	mov.u32 %r126, %laneid;
	// end inline asm
	mov.b64 	%rd154, %fd230;
	mov.b64 	{%r40, %r41}, %rd154;
	shl.b32 	%r131, %r38, 3;
	mov.u32 	%r132, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r133, %r132, %r131;
	setp.gt.s32 	%p26, %r43, 255;
	@%p26 bra 	$L__BB4_36;
	bra.uni 	$L__BB4_31;

$L__BB4_36:
	setp.ne.s32 	%p42, %r126, 0;
	// begin inline asm
	mov.u32 %r194, %laneid;
	// end inline asm
	mov.u32 	%r202, 1;
	mov.u32 	%r243, 31;
	mov.u32 	%r244, -1;
	// begin inline asm
	shfl.sync.down.b32 %r195, %r40, %r202, %r243, %r244;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r200, %r41, %r202, %r243, %r244;
	// end inline asm
	mov.b64 	%rd164, {%r195, %r200};
	mov.b64 	%fd188, %rd164;
	setp.gt.s32 	%p43, %r194, 30;
	add.f64 	%fd189, %fd230, %fd188;
	selp.f64 	%fd190, %fd230, %fd189, %p43;
	mov.b64 	%rd165, %fd190;
	mov.u32 	%r212, 2;
	mov.b64 	{%r206, %r211}, %rd165;
	// begin inline asm
	shfl.sync.down.b32 %r205, %r206, %r212, %r243, %r244;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r210, %r211, %r212, %r243, %r244;
	// end inline asm
	mov.b64 	%rd166, {%r205, %r210};
	mov.b64 	%fd191, %rd166;
	setp.gt.s32 	%p44, %r194, 29;
	add.f64 	%fd192, %fd190, %fd191;
	selp.f64 	%fd193, %fd190, %fd192, %p44;
	mov.b64 	%rd167, %fd193;
	mov.u32 	%r222, 4;
	mov.b64 	{%r216, %r221}, %rd167;
	// begin inline asm
	shfl.sync.down.b32 %r215, %r216, %r222, %r243, %r244;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r220, %r221, %r222, %r243, %r244;
	// end inline asm
	mov.b64 	%rd168, {%r215, %r220};
	mov.b64 	%fd194, %rd168;
	setp.gt.s32 	%p45, %r194, 27;
	add.f64 	%fd195, %fd193, %fd194;
	selp.f64 	%fd196, %fd193, %fd195, %p45;
	mov.b64 	%rd169, %fd196;
	mov.u32 	%r232, 8;
	mov.b64 	{%r226, %r231}, %rd169;
	// begin inline asm
	shfl.sync.down.b32 %r225, %r226, %r232, %r243, %r244;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r230, %r231, %r232, %r243, %r244;
	// end inline asm
	mov.b64 	%rd170, {%r225, %r230};
	mov.b64 	%fd197, %rd170;
	setp.gt.s32 	%p46, %r194, 23;
	add.f64 	%fd198, %fd196, %fd197;
	selp.f64 	%fd199, %fd196, %fd198, %p46;
	mov.b64 	%rd171, %fd199;
	mov.u32 	%r242, 16;
	mov.b64 	{%r236, %r241}, %rd171;
	// begin inline asm
	shfl.sync.down.b32 %r235, %r236, %r242, %r243, %r244;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r240, %r241, %r242, %r243, %r244;
	// end inline asm
	mov.b64 	%rd172, {%r235, %r240};
	mov.b64 	%fd200, %rd172;
	setp.gt.s32 	%p47, %r194, 15;
	add.f64 	%fd201, %fd199, %fd200;
	selp.f64 	%fd231, %fd199, %fd201, %p47;
	@%p42 bra 	$L__BB4_38;

	add.s32 	%r249, %r133, 8;
	st.shared.f64 	[%r249], %fd231;

$L__BB4_38:
	bar.sync 	0;
	setp.ne.s32 	%p48, %r127, 0;
	@%p48 bra 	$L__BB4_40;

	ld.shared.f64 	%fd202, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd203, %fd231, %fd202;
	ld.shared.f64 	%fd204, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd205, %fd203, %fd204;
	ld.shared.f64 	%fd206, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd207, %fd205, %fd206;
	ld.shared.f64 	%fd208, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd209, %fd207, %fd208;
	ld.shared.f64 	%fd210, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd211, %fd209, %fd210;
	ld.shared.f64 	%fd212, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd213, %fd211, %fd212;
	ld.shared.f64 	%fd214, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd231, %fd213, %fd214;
	bra.uni 	$L__BB4_40;

$L__BB4_42:
	mov.u32 	%r247, %tid.x;
	setp.ne.s32 	%p50, %r247, 0;
	@%p50 bra 	$L__BB4_44;

	cvta.to.global.u64 	%rd174, %rd16;
	st.global.f64 	[%rd174], %fd32;
	bra.uni 	$L__BB4_44;

$L__BB4_2:
	mov.u32 	%r45, %tid.x;
	mul.wide.s32 	%rd37, %r45, 8;
	add.s64 	%rd18, %rd15, %rd37;
	// begin inline asm
	ld.global.nc.u64 %rd17, [%rd18];
	// end inline asm
	add.s64 	%rd20, %rd18, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd19, [%rd20];
	// end inline asm
	add.s64 	%rd22, %rd18, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd21, [%rd22];
	// end inline asm
	add.s64 	%rd24, %rd18, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd23, [%rd24];
	// end inline asm
	add.s64 	%rd26, %rd18, 8192;
	// begin inline asm
	ld.global.nc.u64 %rd25, [%rd26];
	// end inline asm
	add.s64 	%rd28, %rd18, 10240;
	// begin inline asm
	ld.global.nc.u64 %rd27, [%rd28];
	// end inline asm
	add.s64 	%rd30, %rd18, 12288;
	// begin inline asm
	ld.global.nc.u64 %rd29, [%rd30];
	// end inline asm
	add.s64 	%rd32, %rd18, 14336;
	// begin inline asm
	ld.global.nc.u64 %rd31, [%rd32];
	// end inline asm
	add.s64 	%rd34, %rd18, 16384;
	// begin inline asm
	ld.global.nc.u64 %rd33, [%rd34];
	// end inline asm
	add.s64 	%rd36, %rd18, 18432;
	// begin inline asm
	ld.global.nc.u64 %rd35, [%rd36];
	// end inline asm
	mov.b64 	%fd33, %rd17;
	mov.b64 	%fd34, %rd19;
	add.f64 	%fd35, %fd33, %fd34;
	mov.b64 	%fd36, %rd21;
	add.f64 	%fd37, %fd35, %fd36;
	mov.b64 	%fd38, %rd23;
	add.f64 	%fd39, %fd37, %fd38;
	mov.b64 	%fd40, %rd25;
	add.f64 	%fd41, %fd39, %fd40;
	mov.b64 	%fd42, %rd27;
	add.f64 	%fd43, %fd41, %fd42;
	mov.b64 	%fd44, %rd29;
	add.f64 	%fd45, %fd43, %fd44;
	mov.b64 	%fd46, %rd31;
	add.f64 	%fd47, %fd45, %fd46;
	mov.b64 	%fd48, %rd33;
	add.f64 	%fd49, %fd47, %fd48;
	mov.b64 	%fd50, %rd35;
	add.f64 	%fd224, %fd49, %fd50;
	setp.lt.s32 	%p3, %r43, 5120;
	mov.u32 	%r255, 2560;
	@%p3 bra 	$L__BB4_9;

	add.s32 	%r48, %r43, -5120;
	mul.wide.u32 	%rd39, %r48, -858993459;
	shr.u64 	%rd40, %rd39, 43;
	cvt.u32.u64 	%r49, %rd40;
	add.s32 	%r1, %r49, 1;
	and.b32  	%r2, %r1, 1;
	setp.lt.u32 	%p4, %r48, 2560;
	mov.u64 	%rd175, 2560;
	mov.u32 	%r254, 5120;
	@%p4 bra 	$L__BB4_7;

	sub.s32 	%r252, %r1, %r2;
	mov.u32 	%r255, 2560;
	mov.u32 	%r250, 5120;

$L__BB4_5:
	mul.wide.s32 	%rd83, %r255, 8;
	add.s64 	%rd42, %rd18, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd41, [%rd42];
	// end inline asm
	add.s64 	%rd44, %rd20, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd43, [%rd44];
	// end inline asm
	add.s64 	%rd46, %rd22, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd45, [%rd46];
	// end inline asm
	add.s64 	%rd48, %rd24, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd47, [%rd48];
	// end inline asm
	add.s64 	%rd50, %rd26, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd49, [%rd50];
	// end inline asm
	add.s64 	%rd52, %rd28, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd51, [%rd52];
	// end inline asm
	add.s64 	%rd54, %rd30, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd53, [%rd54];
	// end inline asm
	add.s64 	%rd56, %rd32, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd55, [%rd56];
	// end inline asm
	add.s64 	%rd58, %rd34, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd57, [%rd58];
	// end inline asm
	add.s64 	%rd60, %rd36, %rd83;
	// begin inline asm
	ld.global.nc.u64 %rd59, [%rd60];
	// end inline asm
	mov.b64 	%fd52, %rd41;
	add.f64 	%fd53, %fd224, %fd52;
	mov.b64 	%fd54, %rd43;
	add.f64 	%fd55, %fd53, %fd54;
	mov.b64 	%fd56, %rd45;
	add.f64 	%fd57, %fd55, %fd56;
	mov.b64 	%fd58, %rd47;
	add.f64 	%fd59, %fd57, %fd58;
	mov.b64 	%fd60, %rd49;
	add.f64 	%fd61, %fd59, %fd60;
	mov.b64 	%fd62, %rd51;
	add.f64 	%fd63, %fd61, %fd62;
	mov.b64 	%fd64, %rd53;
	add.f64 	%fd65, %fd63, %fd64;
	mov.b64 	%fd66, %rd55;
	add.f64 	%fd67, %fd65, %fd66;
	mov.b64 	%fd68, %rd57;
	add.f64 	%fd69, %fd67, %fd68;
	mov.b64 	%fd70, %rd59;
	add.f64 	%fd71, %fd69, %fd70;
	mul.wide.s32 	%rd93, %r250, 8;
	add.s64 	%rd62, %rd18, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd61, [%rd62];
	// end inline asm
	add.s64 	%rd64, %rd20, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd63, [%rd64];
	// end inline asm
	add.s64 	%rd66, %rd22, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd65, [%rd66];
	// end inline asm
	add.s64 	%rd68, %rd24, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd67, [%rd68];
	// end inline asm
	add.s64 	%rd70, %rd26, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd69, [%rd70];
	// end inline asm
	add.s64 	%rd72, %rd28, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd71, [%rd72];
	// end inline asm
	add.s64 	%rd74, %rd30, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd73, [%rd74];
	// end inline asm
	add.s64 	%rd76, %rd32, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd75, [%rd76];
	// end inline asm
	add.s64 	%rd78, %rd34, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd77, [%rd78];
	// end inline asm
	add.s64 	%rd80, %rd36, %rd93;
	// begin inline asm
	ld.global.nc.u64 %rd79, [%rd80];
	// end inline asm
	mov.b64 	%fd72, %rd61;
	add.f64 	%fd73, %fd71, %fd72;
	mov.b64 	%fd74, %rd63;
	add.f64 	%fd75, %fd73, %fd74;
	mov.b64 	%fd76, %rd65;
	add.f64 	%fd77, %fd75, %fd76;
	mov.b64 	%fd78, %rd67;
	add.f64 	%fd79, %fd77, %fd78;
	mov.b64 	%fd80, %rd69;
	add.f64 	%fd81, %fd79, %fd80;
	mov.b64 	%fd82, %rd71;
	add.f64 	%fd83, %fd81, %fd82;
	mov.b64 	%fd84, %rd73;
	add.f64 	%fd85, %fd83, %fd84;
	mov.b64 	%fd86, %rd75;
	add.f64 	%fd87, %fd85, %fd86;
	mov.b64 	%fd88, %rd77;
	add.f64 	%fd89, %fd87, %fd88;
	mov.b64 	%fd90, %rd79;
	add.f64 	%fd224, %fd89, %fd90;
	add.s32 	%r252, %r252, -2;
	setp.ne.s32 	%p5, %r252, 0;
	add.s32 	%r255, %r250, 2560;
	add.s32 	%r250, %r250, 5120;
	@%p5 bra 	$L__BB4_5;

	cvt.s64.s32 	%rd175, %r255;
	add.s32 	%r254, %r255, 2560;

$L__BB4_7:
	setp.eq.s32 	%p6, %r2, 0;
	@%p6 bra 	$L__BB4_9;

	shl.b64 	%rd116, %rd175, 3;
	add.s64 	%rd95, %rd18, %rd116;
	// begin inline asm
	ld.global.nc.u64 %rd94, [%rd95];
	// end inline asm
	add.s64 	%rd97, %rd95, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd96, [%rd97];
	// end inline asm
	add.s64 	%rd99, %rd95, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd98, [%rd99];
	// end inline asm
	add.s64 	%rd101, %rd95, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd100, [%rd101];
	// end inline asm
	add.s64 	%rd103, %rd95, 8192;
	// begin inline asm
	ld.global.nc.u64 %rd102, [%rd103];
	// end inline asm
	add.s64 	%rd105, %rd95, 10240;
	// begin inline asm
	ld.global.nc.u64 %rd104, [%rd105];
	// end inline asm
	add.s64 	%rd107, %rd95, 12288;
	// begin inline asm
	ld.global.nc.u64 %rd106, [%rd107];
	// end inline asm
	add.s64 	%rd109, %rd95, 14336;
	// begin inline asm
	ld.global.nc.u64 %rd108, [%rd109];
	// end inline asm
	add.s64 	%rd111, %rd95, 16384;
	// begin inline asm
	ld.global.nc.u64 %rd110, [%rd111];
	// end inline asm
	add.s64 	%rd113, %rd95, 18432;
	// begin inline asm
	ld.global.nc.u64 %rd112, [%rd113];
	// end inline asm
	mov.b64 	%fd91, %rd94;
	add.f64 	%fd92, %fd224, %fd91;
	mov.b64 	%fd93, %rd96;
	add.f64 	%fd94, %fd92, %fd93;
	mov.b64 	%fd95, %rd98;
	add.f64 	%fd96, %fd94, %fd95;
	mov.b64 	%fd97, %rd100;
	add.f64 	%fd98, %fd96, %fd97;
	mov.b64 	%fd99, %rd102;
	add.f64 	%fd100, %fd98, %fd99;
	mov.b64 	%fd101, %rd104;
	add.f64 	%fd102, %fd100, %fd101;
	mov.b64 	%fd103, %rd106;
	add.f64 	%fd104, %fd102, %fd103;
	mov.b64 	%fd105, %rd108;
	add.f64 	%fd106, %fd104, %fd105;
	mov.b64 	%fd107, %rd110;
	add.f64 	%fd108, %fd106, %fd107;
	mov.b64 	%fd109, %rd112;
	add.f64 	%fd224, %fd108, %fd109;
	mov.u32 	%r255, %r254;

$L__BB4_9:
	setp.ge.s32 	%p7, %r255, %r43;
	@%p7 bra 	$L__BB4_17;

	sub.s32 	%r14, %r43, %r255;
	setp.ge.s32 	%p8, %r45, %r14;
	@%p8 bra 	$L__BB4_17;

	not.b32 	%r55, %r255;
	add.s32 	%r56, %r55, %r43;
	sub.s32 	%r16, %r56, %r45;
	shr.u32 	%r57, %r16, 8;
	add.s32 	%r58, %r57, 1;
	and.b32  	%r257, %r58, 3;
	setp.eq.s32 	%p9, %r257, 0;
	mov.u32 	%r258, %r45;
	@%p9 bra 	$L__BB4_14;

	mov.u32 	%r258, %tid.x;
	add.s32 	%r59, %r258, %r255;
	mul.wide.s32 	%rd117, %r59, 8;
	add.s64 	%rd176, %rd15, %rd117;

$L__BB4_13:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd118, [%rd176];
	// end inline asm
	mov.b64 	%fd111, %rd118;
	add.f64 	%fd224, %fd224, %fd111;
	add.s32 	%r258, %r258, 256;
	add.s64 	%rd176, %rd176, 2048;
	add.s32 	%r257, %r257, -1;
	setp.ne.s32 	%p10, %r257, 0;
	@%p10 bra 	$L__BB4_13;

$L__BB4_14:
	setp.lt.u32 	%p11, %r16, 768;
	@%p11 bra 	$L__BB4_17;

	add.s32 	%r60, %r255, %r258;
	mul.wide.s32 	%rd120, %r60, 8;
	add.s64 	%rd177, %rd15, %rd120;

$L__BB4_16:
	// begin inline asm
	ld.global.nc.u64 %rd121, [%rd177];
	// end inline asm
	mov.b64 	%fd112, %rd121;
	add.f64 	%fd113, %fd224, %fd112;
	add.s64 	%rd124, %rd177, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd123, [%rd124];
	// end inline asm
	mov.b64 	%fd114, %rd123;
	add.f64 	%fd115, %fd113, %fd114;
	add.s64 	%rd126, %rd177, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd125, [%rd126];
	// end inline asm
	mov.b64 	%fd116, %rd125;
	add.f64 	%fd117, %fd115, %fd116;
	add.s64 	%rd128, %rd177, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd127, [%rd128];
	// end inline asm
	mov.b64 	%fd118, %rd127;
	add.f64 	%fd224, %fd117, %fd118;
	add.s64 	%rd177, %rd177, 8192;
	add.s32 	%r258, %r258, 1024;
	setp.lt.s32 	%p12, %r258, %r14;
	@%p12 bra 	$L__BB4_16;

$L__BB4_17:
	// begin inline asm
	mov.u32 %r61, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r62, %laneid;
	// end inline asm
	mov.b64 	%rd129, %fd224;
	mov.u32 	%r70, 1;
	mov.u32 	%r111, 31;
	mov.u32 	%r112, -1;
	mov.b64 	{%r64, %r69}, %rd129;
	// begin inline asm
	shfl.sync.down.b32 %r63, %r64, %r70, %r111, %r112;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r68, %r69, %r70, %r111, %r112;
	// end inline asm
	mov.b64 	%rd130, {%r63, %r68};
	mov.b64 	%fd119, %rd130;
	setp.gt.s32 	%p13, %r62, 30;
	add.f64 	%fd120, %fd224, %fd119;
	selp.f64 	%fd121, %fd224, %fd120, %p13;
	mov.b64 	%rd131, %fd121;
	mov.u32 	%r80, 2;
	mov.b64 	{%r74, %r79}, %rd131;
	// begin inline asm
	shfl.sync.down.b32 %r73, %r74, %r80, %r111, %r112;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r78, %r79, %r80, %r111, %r112;
	// end inline asm
	mov.b64 	%rd132, {%r73, %r78};
	mov.b64 	%fd122, %rd132;
	setp.gt.s32 	%p14, %r62, 29;
	add.f64 	%fd123, %fd121, %fd122;
	selp.f64 	%fd124, %fd121, %fd123, %p14;
	mov.b64 	%rd133, %fd124;
	mov.u32 	%r90, 4;
	mov.b64 	{%r84, %r89}, %rd133;
	// begin inline asm
	shfl.sync.down.b32 %r83, %r84, %r90, %r111, %r112;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r88, %r89, %r90, %r111, %r112;
	// end inline asm
	mov.b64 	%rd134, {%r83, %r88};
	mov.b64 	%fd125, %rd134;
	setp.gt.s32 	%p15, %r62, 27;
	add.f64 	%fd126, %fd124, %fd125;
	selp.f64 	%fd127, %fd124, %fd126, %p15;
	mov.b64 	%rd135, %fd127;
	mov.u32 	%r100, 8;
	mov.b64 	{%r94, %r99}, %rd135;
	// begin inline asm
	shfl.sync.down.b32 %r93, %r94, %r100, %r111, %r112;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r98, %r99, %r100, %r111, %r112;
	// end inline asm
	mov.b64 	%rd136, {%r93, %r98};
	mov.b64 	%fd128, %rd136;
	setp.gt.s32 	%p16, %r62, 23;
	add.f64 	%fd129, %fd127, %fd128;
	selp.f64 	%fd130, %fd127, %fd129, %p16;
	mov.b64 	%rd137, %fd130;
	mov.u32 	%r110, 16;
	mov.b64 	{%r104, %r109}, %rd137;
	// begin inline asm
	shfl.sync.down.b32 %r103, %r104, %r110, %r111, %r112;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r108, %r109, %r110, %r111, %r112;
	// end inline asm
	mov.b64 	%rd138, {%r103, %r108};
	mov.b64 	%fd131, %rd138;
	setp.gt.s32 	%p17, %r62, 15;
	add.f64 	%fd132, %fd130, %fd131;
	selp.f64 	%fd231, %fd130, %fd132, %p17;
	setp.ne.s32 	%p18, %r61, 0;
	@%p18 bra 	$L__BB4_19;

	shr.s32 	%r114, %r45, 31;
	shr.u32 	%r115, %r114, 27;
	add.s32 	%r116, %r45, %r115;
	shr.s32 	%r117, %r116, 5;
	shl.b32 	%r118, %r117, 3;
	mov.u32 	%r119, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r120, %r119, %r118;
	st.shared.f64 	[%r120+8], %fd231;

$L__BB4_19:
	bar.sync 	0;
	setp.ne.s32 	%p19, %r45, 0;
	@%p19 bra 	$L__BB4_40;

	ld.shared.f64 	%fd133, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd134, %fd231, %fd133;
	ld.shared.f64 	%fd135, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd136, %fd134, %fd135;
	ld.shared.f64 	%fd137, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd138, %fd136, %fd137;
	ld.shared.f64 	%fd139, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd140, %fd138, %fd139;
	ld.shared.f64 	%fd141, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd142, %fd140, %fd141;
	ld.shared.f64 	%fd143, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd144, %fd142, %fd143;
	ld.shared.f64 	%fd145, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd231, %fd144, %fd145;
	bra.uni 	$L__BB4_40;

$L__BB4_31:
	setp.ne.s32 	%p27, %r126, 0;
	shl.b32 	%r185, %r38, 5;
	add.s32 	%r186, %r185, 32;
	setp.gt.s32 	%p28, %r186, %r43;
	// begin inline asm
	mov.u32 %r134, %laneid;
	// end inline asm
	not.b32 	%r187, %r185;
	mov.u32 	%r184, -1;
	add.s32 	%r188, %r187, %r43;
	selp.b32 	%r183, %r188, 31, %p28;
	mov.u32 	%r142, 1;
	// begin inline asm
	shfl.sync.down.b32 %r135, %r40, %r142, %r183, %r184;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r140, %r41, %r142, %r183, %r184;
	// end inline asm
	mov.b64 	%rd155, {%r135, %r140};
	mov.b64 	%fd156, %rd155;
	setp.lt.s32 	%p29, %r134, %r183;
	add.f64 	%fd157, %fd230, %fd156;
	selp.f64 	%fd158, %fd157, %fd230, %p29;
	mov.b64 	%rd156, %fd158;
	mov.u32 	%r152, 2;
	mov.b64 	{%r146, %r151}, %rd156;
	// begin inline asm
	shfl.sync.down.b32 %r145, %r146, %r152, %r183, %r184;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r150, %r151, %r152, %r183, %r184;
	// end inline asm
	mov.b64 	%rd157, {%r145, %r150};
	mov.b64 	%fd159, %rd157;
	add.s32 	%r189, %r134, 2;
	setp.gt.s32 	%p30, %r189, %r183;
	add.f64 	%fd160, %fd158, %fd159;
	selp.f64 	%fd161, %fd158, %fd160, %p30;
	mov.b64 	%rd158, %fd161;
	mov.u32 	%r162, 4;
	mov.b64 	{%r156, %r161}, %rd158;
	// begin inline asm
	shfl.sync.down.b32 %r155, %r156, %r162, %r183, %r184;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r160, %r161, %r162, %r183, %r184;
	// end inline asm
	mov.b64 	%rd159, {%r155, %r160};
	mov.b64 	%fd162, %rd159;
	add.s32 	%r190, %r134, 4;
	setp.gt.s32 	%p31, %r190, %r183;
	add.f64 	%fd163, %fd161, %fd162;
	selp.f64 	%fd164, %fd161, %fd163, %p31;
	mov.b64 	%rd160, %fd164;
	mov.u32 	%r172, 8;
	mov.b64 	{%r166, %r171}, %rd160;
	// begin inline asm
	shfl.sync.down.b32 %r165, %r166, %r172, %r183, %r184;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r170, %r171, %r172, %r183, %r184;
	// end inline asm
	mov.b64 	%rd161, {%r165, %r170};
	mov.b64 	%fd165, %rd161;
	add.s32 	%r191, %r134, 8;
	setp.gt.s32 	%p32, %r191, %r183;
	add.f64 	%fd166, %fd164, %fd165;
	selp.f64 	%fd167, %fd164, %fd166, %p32;
	mov.b64 	%rd162, %fd167;
	mov.u32 	%r182, 16;
	mov.b64 	{%r176, %r181}, %rd162;
	// begin inline asm
	shfl.sync.down.b32 %r175, %r176, %r182, %r183, %r184;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r180, %r181, %r182, %r183, %r184;
	// end inline asm
	mov.b64 	%rd163, {%r175, %r180};
	mov.b64 	%fd168, %rd163;
	add.s32 	%r192, %r134, 16;
	setp.gt.s32 	%p33, %r192, %r183;
	add.f64 	%fd169, %fd167, %fd168;
	selp.f64 	%fd231, %fd167, %fd169, %p33;
	@%p27 bra 	$L__BB4_33;

	add.s32 	%r248, %r133, 8;
	st.shared.f64 	[%r248], %fd231;

$L__BB4_33:
	bar.sync 	0;
	setp.ne.s32 	%p34, %r127, 0;
	@%p34 bra 	$L__BB4_40;

	setp.gt.s32 	%p35, %r43, 32;
	ld.shared.f64 	%fd170, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd171, %fd231, %fd170;
	selp.f64 	%fd172, %fd171, %fd231, %p35;
	ld.shared.f64 	%fd173, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd174, %fd172, %fd173;
	setp.gt.s32 	%p36, %r43, 64;
	selp.f64 	%fd175, %fd174, %fd172, %p36;
	ld.shared.f64 	%fd176, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd177, %fd175, %fd176;
	setp.gt.s32 	%p37, %r43, 96;
	selp.f64 	%fd178, %fd177, %fd175, %p37;
	ld.shared.f64 	%fd179, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd180, %fd178, %fd179;
	setp.gt.s32 	%p38, %r43, 128;
	selp.f64 	%fd181, %fd180, %fd178, %p38;
	ld.shared.f64 	%fd182, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd183, %fd181, %fd182;
	setp.gt.s32 	%p39, %r43, 160;
	selp.f64 	%fd184, %fd183, %fd181, %p39;
	ld.shared.f64 	%fd185, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd186, %fd184, %fd185;
	setp.gt.s32 	%p40, %r43, 192;
	selp.f64 	%fd231, %fd186, %fd184, %p40;
	setp.lt.s32 	%p41, %r43, 225;
	@%p41 bra 	$L__BB4_40;

	ld.shared.f64 	%fd187, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd231, %fd231, %fd187;

$L__BB4_40:
	mov.u32 	%r246, %tid.x;
	setp.ne.s32 	%p49, %r246, 0;
	@%p49 bra 	$L__BB4_44;

	add.f64 	%fd215, %fd231, %fd32;
	cvta.to.global.u64 	%rd173, %rd16;
	st.global.f64 	[%rd173], %fd215;

$L__BB4_44:
	ret;

}
	// .globl	_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_
.visible .entry _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_(
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_0,
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_1,
	.param .u32 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_2,
	.param .align 4 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_3[40],
	.param .align 1 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_4[1]
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<47>;
	.reg .b32 	%r<274>;
	.reg .f64 	%fd<185>;
	.reg .b64 	%rd<178>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage[80];

	ld.param.u64 	%rd17, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_0];
	ld.param.u64 	%rd18, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_1];
	ld.param.u32 	%r1, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_3+20];
	ld.param.u32 	%r39, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_3+24];
	mul.lo.s32 	%r2, %r39, 2560;
	mov.u32 	%r40, %ctaid.x;
	mul.lo.s32 	%r3, %r40, 2560;
	add.s32 	%r41, %r3, 2560;
	setp.gt.s32 	%p1, %r41, %r1;
	mov.u32 	%r272, %tid.x;
	@%p1 bra 	$L__BB5_16;
	bra.uni 	$L__BB5_1;

$L__BB5_16:
	sub.s32 	%r22, %r1, %r3;
	setp.ge.s32 	%p17, %r272, %r22;
	@%p17 bra 	$L__BB5_18;

	add.s32 	%r131, %r3, %r272;
	mul.wide.s32 	%rd139, %r131, 8;
	add.s64 	%rd138, %rd17, %rd139;
	// begin inline asm
	ld.global.nc.u64 %rd137, [%rd138];
	// end inline asm
	mov.b64 	%fd183, %rd137;
	add.s32 	%r272, %r272, 256;

$L__BB5_18:
	setp.ge.s32 	%p18, %r272, %r22;
	@%p18 bra 	$L__BB5_25;

	not.b32 	%r132, %r272;
	add.s32 	%r133, %r1, %r132;
	mad.lo.s32 	%r25, %r40, -2560, %r133;
	shr.u32 	%r135, %r25, 8;
	add.s32 	%r136, %r135, 1;
	and.b32  	%r271, %r136, 3;
	setp.eq.s32 	%p19, %r271, 0;
	@%p19 bra 	$L__BB5_22;

	add.s32 	%r137, %r272, %r3;
	mul.wide.s32 	%rd140, %r137, 8;
	add.s64 	%rd176, %rd17, %rd140;

$L__BB5_21:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd141, [%rd176];
	// end inline asm
	mov.b64 	%fd104, %rd141;
	add.f64 	%fd183, %fd183, %fd104;
	add.s32 	%r272, %r272, 256;
	add.s64 	%rd176, %rd176, 2048;
	add.s32 	%r271, %r271, -1;
	setp.ne.s32 	%p20, %r271, 0;
	@%p20 bra 	$L__BB5_21;

$L__BB5_22:
	setp.lt.u32 	%p21, %r25, 768;
	@%p21 bra 	$L__BB5_25;

	add.s32 	%r138, %r272, %r3;
	mul.wide.s32 	%rd143, %r138, 8;
	add.s64 	%rd177, %rd17, %rd143;

$L__BB5_24:
	// begin inline asm
	ld.global.nc.u64 %rd144, [%rd177];
	// end inline asm
	mov.b64 	%fd105, %rd144;
	add.f64 	%fd106, %fd183, %fd105;
	add.s64 	%rd147, %rd177, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd146, [%rd147];
	// end inline asm
	mov.b64 	%fd107, %rd146;
	add.f64 	%fd108, %fd106, %fd107;
	add.s64 	%rd149, %rd177, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd148, [%rd149];
	// end inline asm
	mov.b64 	%fd109, %rd148;
	add.f64 	%fd110, %fd108, %fd109;
	add.s64 	%rd151, %rd177, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd150, [%rd151];
	// end inline asm
	mov.b64 	%fd111, %rd150;
	add.f64 	%fd183, %fd110, %fd111;
	add.s64 	%rd177, %rd177, 8192;
	add.s32 	%r272, %r272, 1024;
	setp.lt.s32 	%p22, %r272, %r22;
	@%p22 bra 	$L__BB5_24;

$L__BB5_25:
	mov.u32 	%r140, %tid.x;
	shr.s32 	%r141, %r140, 31;
	shr.u32 	%r142, %r141, 27;
	add.s32 	%r143, %r140, %r142;
	shr.s32 	%r34, %r143, 5;
	// begin inline asm
	mov.u32 %r139, %laneid;
	// end inline asm
	mov.b64 	%rd152, %fd183;
	mov.b64 	{%r36, %r37}, %rd152;
	shl.b32 	%r144, %r34, 3;
	mov.u32 	%r145, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage;
	add.s32 	%r146, %r145, %r144;
	setp.gt.s32 	%p23, %r22, 255;
	@%p23 bra 	$L__BB5_31;
	bra.uni 	$L__BB5_26;

$L__BB5_31:
	setp.ne.s32 	%p39, %r139, 0;
	// begin inline asm
	mov.u32 %r207, %laneid;
	// end inline asm
	mov.u32 	%r215, 1;
	mov.u32 	%r256, 31;
	mov.u32 	%r257, -1;
	// begin inline asm
	shfl.sync.down.b32 %r208, %r36, %r215, %r256, %r257;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r213, %r37, %r215, %r256, %r257;
	// end inline asm
	mov.b64 	%rd162, {%r208, %r213};
	mov.b64 	%fd144, %rd162;
	setp.gt.s32 	%p40, %r207, 30;
	add.f64 	%fd145, %fd183, %fd144;
	selp.f64 	%fd146, %fd183, %fd145, %p40;
	mov.b64 	%rd163, %fd146;
	mov.u32 	%r225, 2;
	mov.b64 	{%r219, %r224}, %rd163;
	// begin inline asm
	shfl.sync.down.b32 %r218, %r219, %r225, %r256, %r257;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r223, %r224, %r225, %r256, %r257;
	// end inline asm
	mov.b64 	%rd164, {%r218, %r223};
	mov.b64 	%fd147, %rd164;
	setp.gt.s32 	%p41, %r207, 29;
	add.f64 	%fd148, %fd146, %fd147;
	selp.f64 	%fd149, %fd146, %fd148, %p41;
	mov.b64 	%rd165, %fd149;
	mov.u32 	%r235, 4;
	mov.b64 	{%r229, %r234}, %rd165;
	// begin inline asm
	shfl.sync.down.b32 %r228, %r229, %r235, %r256, %r257;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r233, %r234, %r235, %r256, %r257;
	// end inline asm
	mov.b64 	%rd166, {%r228, %r233};
	mov.b64 	%fd150, %rd166;
	setp.gt.s32 	%p42, %r207, 27;
	add.f64 	%fd151, %fd149, %fd150;
	selp.f64 	%fd152, %fd149, %fd151, %p42;
	mov.b64 	%rd167, %fd152;
	mov.u32 	%r245, 8;
	mov.b64 	{%r239, %r244}, %rd167;
	// begin inline asm
	shfl.sync.down.b32 %r238, %r239, %r245, %r256, %r257;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r243, %r244, %r245, %r256, %r257;
	// end inline asm
	mov.b64 	%rd168, {%r238, %r243};
	mov.b64 	%fd153, %rd168;
	setp.gt.s32 	%p43, %r207, 23;
	add.f64 	%fd154, %fd152, %fd153;
	selp.f64 	%fd155, %fd152, %fd154, %p43;
	mov.b64 	%rd169, %fd155;
	mov.u32 	%r255, 16;
	mov.b64 	{%r249, %r254}, %rd169;
	// begin inline asm
	shfl.sync.down.b32 %r248, %r249, %r255, %r256, %r257;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r253, %r254, %r255, %r256, %r257;
	// end inline asm
	mov.b64 	%rd170, {%r248, %r253};
	mov.b64 	%fd156, %rd170;
	setp.gt.s32 	%p44, %r207, 15;
	add.f64 	%fd157, %fd155, %fd156;
	selp.f64 	%fd184, %fd155, %fd157, %p44;
	@%p39 bra 	$L__BB5_33;

	add.s32 	%r262, %r146, 8;
	st.shared.f64 	[%r262], %fd184;

$L__BB5_33:
	bar.sync 	0;
	setp.ne.s32 	%p45, %r140, 0;
	@%p45 bra 	$L__BB5_35;

	ld.shared.f64 	%fd158, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+16];
	add.f64 	%fd159, %fd184, %fd158;
	ld.shared.f64 	%fd160, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+24];
	add.f64 	%fd161, %fd159, %fd160;
	ld.shared.f64 	%fd162, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+32];
	add.f64 	%fd163, %fd161, %fd162;
	ld.shared.f64 	%fd164, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+40];
	add.f64 	%fd165, %fd163, %fd164;
	ld.shared.f64 	%fd166, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+48];
	add.f64 	%fd167, %fd165, %fd166;
	ld.shared.f64 	%fd168, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+56];
	add.f64 	%fd169, %fd167, %fd168;
	ld.shared.f64 	%fd170, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+64];
	add.f64 	%fd184, %fd169, %fd170;
	bra.uni 	$L__BB5_35;

$L__BB5_1:
	cvt.s64.s32 	%rd39, %r3;
	cvt.s64.s32 	%rd40, %r272;
	add.s64 	%rd41, %rd40, %rd39;
	shl.b64 	%rd42, %rd41, 3;
	add.s64 	%rd20, %rd17, %rd42;
	// begin inline asm
	ld.global.nc.u64 %rd19, [%rd20];
	// end inline asm
	add.s32 	%r42, %r272, 256;
	cvt.s64.s32 	%rd43, %r42;
	add.s64 	%rd44, %rd43, %rd39;
	shl.b64 	%rd45, %rd44, 3;
	add.s64 	%rd22, %rd17, %rd45;
	// begin inline asm
	ld.global.nc.u64 %rd21, [%rd22];
	// end inline asm
	add.s32 	%r43, %r272, 512;
	cvt.s64.s32 	%rd46, %r43;
	add.s64 	%rd47, %rd46, %rd39;
	shl.b64 	%rd48, %rd47, 3;
	add.s64 	%rd24, %rd17, %rd48;
	// begin inline asm
	ld.global.nc.u64 %rd23, [%rd24];
	// end inline asm
	add.s32 	%r44, %r272, 768;
	cvt.s64.s32 	%rd49, %r44;
	add.s64 	%rd50, %rd49, %rd39;
	shl.b64 	%rd51, %rd50, 3;
	add.s64 	%rd26, %rd17, %rd51;
	// begin inline asm
	ld.global.nc.u64 %rd25, [%rd26];
	// end inline asm
	add.s32 	%r45, %r272, 1024;
	cvt.s64.s32 	%rd52, %r45;
	add.s64 	%rd53, %rd52, %rd39;
	shl.b64 	%rd54, %rd53, 3;
	add.s64 	%rd28, %rd17, %rd54;
	// begin inline asm
	ld.global.nc.u64 %rd27, [%rd28];
	// end inline asm
	add.s32 	%r46, %r272, 1280;
	cvt.s64.s32 	%rd55, %r46;
	add.s64 	%rd56, %rd55, %rd39;
	shl.b64 	%rd57, %rd56, 3;
	add.s64 	%rd30, %rd17, %rd57;
	// begin inline asm
	ld.global.nc.u64 %rd29, [%rd30];
	// end inline asm
	add.s32 	%r47, %r272, 1536;
	cvt.s64.s32 	%rd58, %r47;
	add.s64 	%rd59, %rd58, %rd39;
	shl.b64 	%rd60, %rd59, 3;
	add.s64 	%rd32, %rd17, %rd60;
	// begin inline asm
	ld.global.nc.u64 %rd31, [%rd32];
	// end inline asm
	add.s32 	%r48, %r272, 1792;
	cvt.s64.s32 	%rd61, %r48;
	add.s64 	%rd62, %rd61, %rd39;
	shl.b64 	%rd63, %rd62, 3;
	add.s64 	%rd34, %rd17, %rd63;
	// begin inline asm
	ld.global.nc.u64 %rd33, [%rd34];
	// end inline asm
	add.s32 	%r49, %r272, 2048;
	cvt.s64.s32 	%rd1, %r49;
	add.s64 	%rd64, %rd1, %rd39;
	shl.b64 	%rd65, %rd64, 3;
	add.s64 	%rd36, %rd17, %rd65;
	// begin inline asm
	ld.global.nc.u64 %rd35, [%rd36];
	// end inline asm
	add.s32 	%r50, %r272, 2304;
	cvt.s64.s32 	%rd2, %r50;
	add.s64 	%rd66, %rd2, %rd39;
	shl.b64 	%rd67, %rd66, 3;
	add.s64 	%rd38, %rd17, %rd67;
	// begin inline asm
	ld.global.nc.u64 %rd37, [%rd38];
	// end inline asm
	mov.b64 	%fd29, %rd19;
	mov.b64 	%fd30, %rd21;
	add.f64 	%fd31, %fd29, %fd30;
	mov.b64 	%fd32, %rd23;
	add.f64 	%fd33, %fd31, %fd32;
	mov.b64 	%fd34, %rd25;
	add.f64 	%fd35, %fd33, %fd34;
	mov.b64 	%fd36, %rd27;
	add.f64 	%fd37, %fd35, %fd36;
	mov.b64 	%fd38, %rd29;
	add.f64 	%fd39, %fd37, %fd38;
	mov.b64 	%fd40, %rd31;
	add.f64 	%fd41, %fd39, %fd40;
	mov.b64 	%fd42, %rd33;
	add.f64 	%fd43, %fd41, %fd42;
	mov.b64 	%fd44, %rd35;
	add.f64 	%fd45, %fd43, %fd44;
	mov.b64 	%fd46, %rd37;
	add.f64 	%fd177, %fd45, %fd46;
	add.s32 	%r264, %r3, %r2;
	add.s32 	%r51, %r264, 2560;
	setp.gt.s32 	%p2, %r51, %r1;
	@%p2 bra 	$L__BB5_4;

	mad.lo.s32 	%r264, %r40, 2560, %r2;
	mov.u32 	%r53, %tid.x;
	add.s32 	%r54, %r53, 1536;
	cvt.s64.s32 	%rd3, %r54;
	add.s32 	%r55, %r53, 1792;
	cvt.s64.s32 	%rd4, %r55;

$L__BB5_3:
	cvt.s64.s32 	%rd88, %r53;
	cvt.s64.s32 	%rd89, %r264;
	add.s64 	%rd90, %rd88, %rd89;
	shl.b64 	%rd91, %rd90, 3;
	add.s64 	%rd69, %rd17, %rd91;
	// begin inline asm
	ld.global.nc.u64 %rd68, [%rd69];
	// end inline asm
	add.s32 	%r57, %r53, 256;
	cvt.s64.s32 	%rd92, %r57;
	add.s64 	%rd93, %rd92, %rd89;
	shl.b64 	%rd94, %rd93, 3;
	add.s64 	%rd71, %rd17, %rd94;
	// begin inline asm
	ld.global.nc.u64 %rd70, [%rd71];
	// end inline asm
	add.s32 	%r58, %r53, 512;
	cvt.s64.s32 	%rd95, %r58;
	add.s64 	%rd96, %rd95, %rd89;
	shl.b64 	%rd97, %rd96, 3;
	add.s64 	%rd73, %rd17, %rd97;
	// begin inline asm
	ld.global.nc.u64 %rd72, [%rd73];
	// end inline asm
	add.s32 	%r59, %r53, 768;
	cvt.s64.s32 	%rd98, %r59;
	add.s64 	%rd99, %rd98, %rd89;
	shl.b64 	%rd100, %rd99, 3;
	add.s64 	%rd75, %rd17, %rd100;
	// begin inline asm
	ld.global.nc.u64 %rd74, [%rd75];
	// end inline asm
	add.s32 	%r60, %r53, 1024;
	cvt.s64.s32 	%rd101, %r60;
	add.s64 	%rd102, %rd101, %rd89;
	shl.b64 	%rd103, %rd102, 3;
	add.s64 	%rd77, %rd17, %rd103;
	// begin inline asm
	ld.global.nc.u64 %rd76, [%rd77];
	// end inline asm
	add.s32 	%r61, %r53, 1280;
	cvt.s64.s32 	%rd104, %r61;
	add.s64 	%rd105, %rd104, %rd89;
	shl.b64 	%rd106, %rd105, 3;
	add.s64 	%rd79, %rd17, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd78, [%rd79];
	// end inline asm
	add.s64 	%rd107, %rd3, %rd89;
	shl.b64 	%rd108, %rd107, 3;
	add.s64 	%rd81, %rd17, %rd108;
	// begin inline asm
	ld.global.nc.u64 %rd80, [%rd81];
	// end inline asm
	add.s64 	%rd109, %rd4, %rd89;
	shl.b64 	%rd110, %rd109, 3;
	add.s64 	%rd83, %rd17, %rd110;
	// begin inline asm
	ld.global.nc.u64 %rd82, [%rd83];
	// end inline asm
	add.s64 	%rd111, %rd1, %rd89;
	shl.b64 	%rd112, %rd111, 3;
	add.s64 	%rd85, %rd17, %rd112;
	// begin inline asm
	ld.global.nc.u64 %rd84, [%rd85];
	// end inline asm
	add.s64 	%rd113, %rd2, %rd89;
	shl.b64 	%rd114, %rd113, 3;
	add.s64 	%rd87, %rd17, %rd114;
	// begin inline asm
	ld.global.nc.u64 %rd86, [%rd87];
	// end inline asm
	mov.b64 	%fd47, %rd68;
	add.f64 	%fd48, %fd177, %fd47;
	mov.b64 	%fd49, %rd70;
	add.f64 	%fd50, %fd48, %fd49;
	mov.b64 	%fd51, %rd72;
	add.f64 	%fd52, %fd50, %fd51;
	mov.b64 	%fd53, %rd74;
	add.f64 	%fd54, %fd52, %fd53;
	mov.b64 	%fd55, %rd76;
	add.f64 	%fd56, %fd54, %fd55;
	mov.b64 	%fd57, %rd78;
	add.f64 	%fd58, %fd56, %fd57;
	mov.b64 	%fd59, %rd80;
	add.f64 	%fd60, %fd58, %fd59;
	mov.b64 	%fd61, %rd82;
	add.f64 	%fd62, %fd60, %fd61;
	mov.b64 	%fd63, %rd84;
	add.f64 	%fd64, %fd62, %fd63;
	mov.b64 	%fd65, %rd86;
	add.f64 	%fd177, %fd64, %fd65;
	add.s32 	%r264, %r264, %r2;
	add.s32 	%r62, %r264, 2560;
	setp.le.s32 	%p3, %r62, %r1;
	@%p3 bra 	$L__BB5_3;

$L__BB5_4:
	setp.le.s32 	%p4, %r1, %r264;
	@%p4 bra 	$L__BB5_12;

	sub.s32 	%r10, %r1, %r264;
	mov.u32 	%r267, %tid.x;
	setp.ge.s32 	%p5, %r267, %r10;
	@%p5 bra 	$L__BB5_12;

	not.b32 	%r64, %r267;
	add.s32 	%r65, %r1, %r64;
	sub.s32 	%r12, %r65, %r264;
	shr.u32 	%r66, %r12, 8;
	add.s32 	%r67, %r66, 1;
	and.b32  	%r266, %r67, 3;
	setp.eq.s32 	%p6, %r266, 0;
	@%p6 bra 	$L__BB5_9;

	mov.u32 	%r267, %tid.x;
	add.s32 	%r68, %r267, %r264;
	mul.wide.s32 	%rd115, %r68, 8;
	add.s64 	%rd174, %rd17, %rd115;

$L__BB5_8:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd116, [%rd174];
	// end inline asm
	mov.b64 	%fd67, %rd116;
	add.f64 	%fd177, %fd177, %fd67;
	add.s32 	%r267, %r267, 256;
	add.s64 	%rd174, %rd174, 2048;
	add.s32 	%r266, %r266, -1;
	setp.ne.s32 	%p7, %r266, 0;
	@%p7 bra 	$L__BB5_8;

$L__BB5_9:
	setp.lt.u32 	%p8, %r12, 768;
	@%p8 bra 	$L__BB5_12;

	add.s32 	%r69, %r267, %r264;
	mul.wide.s32 	%rd118, %r69, 8;
	add.s64 	%rd175, %rd17, %rd118;

$L__BB5_11:
	// begin inline asm
	ld.global.nc.u64 %rd119, [%rd175];
	// end inline asm
	mov.b64 	%fd68, %rd119;
	add.f64 	%fd69, %fd177, %fd68;
	add.s64 	%rd122, %rd175, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd121, [%rd122];
	// end inline asm
	mov.b64 	%fd70, %rd121;
	add.f64 	%fd71, %fd69, %fd70;
	add.s64 	%rd124, %rd175, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd123, [%rd124];
	// end inline asm
	mov.b64 	%fd72, %rd123;
	add.f64 	%fd73, %fd71, %fd72;
	add.s64 	%rd126, %rd175, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd125, [%rd126];
	// end inline asm
	mov.b64 	%fd74, %rd125;
	add.f64 	%fd177, %fd73, %fd74;
	add.s64 	%rd175, %rd175, 8192;
	add.s32 	%r267, %r267, 1024;
	setp.lt.s32 	%p9, %r267, %r10;
	@%p9 bra 	$L__BB5_11;

$L__BB5_12:
	// begin inline asm
	mov.u32 %r70, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r71, %laneid;
	// end inline asm
	mov.b64 	%rd127, %fd177;
	mov.u32 	%r79, 1;
	mov.u32 	%r120, 31;
	mov.u32 	%r121, -1;
	mov.b64 	{%r73, %r78}, %rd127;
	// begin inline asm
	shfl.sync.down.b32 %r72, %r73, %r79, %r120, %r121;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r77, %r78, %r79, %r120, %r121;
	// end inline asm
	mov.b64 	%rd128, {%r72, %r77};
	mov.b64 	%fd75, %rd128;
	setp.gt.s32 	%p10, %r71, 30;
	add.f64 	%fd76, %fd177, %fd75;
	selp.f64 	%fd77, %fd177, %fd76, %p10;
	mov.b64 	%rd129, %fd77;
	mov.u32 	%r89, 2;
	mov.b64 	{%r83, %r88}, %rd129;
	// begin inline asm
	shfl.sync.down.b32 %r82, %r83, %r89, %r120, %r121;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r87, %r88, %r89, %r120, %r121;
	// end inline asm
	mov.b64 	%rd130, {%r82, %r87};
	mov.b64 	%fd78, %rd130;
	setp.gt.s32 	%p11, %r71, 29;
	add.f64 	%fd79, %fd77, %fd78;
	selp.f64 	%fd80, %fd77, %fd79, %p11;
	mov.b64 	%rd131, %fd80;
	mov.u32 	%r99, 4;
	mov.b64 	{%r93, %r98}, %rd131;
	// begin inline asm
	shfl.sync.down.b32 %r92, %r93, %r99, %r120, %r121;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r97, %r98, %r99, %r120, %r121;
	// end inline asm
	mov.b64 	%rd132, {%r92, %r97};
	mov.b64 	%fd81, %rd132;
	setp.gt.s32 	%p12, %r71, 27;
	add.f64 	%fd82, %fd80, %fd81;
	selp.f64 	%fd83, %fd80, %fd82, %p12;
	mov.b64 	%rd133, %fd83;
	mov.u32 	%r109, 8;
	mov.b64 	{%r103, %r108}, %rd133;
	// begin inline asm
	shfl.sync.down.b32 %r102, %r103, %r109, %r120, %r121;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r107, %r108, %r109, %r120, %r121;
	// end inline asm
	mov.b64 	%rd134, {%r102, %r107};
	mov.b64 	%fd84, %rd134;
	setp.gt.s32 	%p13, %r71, 23;
	add.f64 	%fd85, %fd83, %fd84;
	selp.f64 	%fd86, %fd83, %fd85, %p13;
	mov.b64 	%rd135, %fd86;
	mov.u32 	%r119, 16;
	mov.b64 	{%r113, %r118}, %rd135;
	// begin inline asm
	shfl.sync.down.b32 %r112, %r113, %r119, %r120, %r121;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r117, %r118, %r119, %r120, %r121;
	// end inline asm
	mov.b64 	%rd136, {%r112, %r117};
	mov.b64 	%fd87, %rd136;
	setp.gt.s32 	%p14, %r71, 15;
	add.f64 	%fd88, %fd86, %fd87;
	selp.f64 	%fd184, %fd86, %fd88, %p14;
	setp.ne.s32 	%p15, %r70, 0;
	@%p15 bra 	$L__BB5_14;

	mov.u32 	%r122, %tid.x;
	shr.s32 	%r123, %r122, 31;
	shr.u32 	%r124, %r123, 27;
	add.s32 	%r125, %r122, %r124;
	shr.s32 	%r126, %r125, 5;
	shl.b32 	%r127, %r126, 3;
	mov.u32 	%r128, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage;
	add.s32 	%r129, %r128, %r127;
	st.shared.f64 	[%r129+8], %fd184;

$L__BB5_14:
	bar.sync 	0;
	mov.u32 	%r130, %tid.x;
	setp.ne.s32 	%p16, %r130, 0;
	@%p16 bra 	$L__BB5_35;

	ld.shared.f64 	%fd89, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+16];
	add.f64 	%fd90, %fd184, %fd89;
	ld.shared.f64 	%fd91, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+24];
	add.f64 	%fd92, %fd90, %fd91;
	ld.shared.f64 	%fd93, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+32];
	add.f64 	%fd94, %fd92, %fd93;
	ld.shared.f64 	%fd95, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+40];
	add.f64 	%fd96, %fd94, %fd95;
	ld.shared.f64 	%fd97, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+48];
	add.f64 	%fd98, %fd96, %fd97;
	ld.shared.f64 	%fd99, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+56];
	add.f64 	%fd100, %fd98, %fd99;
	ld.shared.f64 	%fd101, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+64];
	add.f64 	%fd184, %fd100, %fd101;
	bra.uni 	$L__BB5_35;

$L__BB5_26:
	setp.ne.s32 	%p24, %r139, 0;
	shl.b32 	%r198, %r34, 5;
	add.s32 	%r199, %r198, 32;
	setp.gt.s32 	%p25, %r199, %r22;
	// begin inline asm
	mov.u32 %r147, %laneid;
	// end inline asm
	not.b32 	%r200, %r198;
	mov.u32 	%r197, -1;
	add.s32 	%r201, %r22, %r200;
	selp.b32 	%r196, %r201, 31, %p25;
	mov.u32 	%r155, 1;
	// begin inline asm
	shfl.sync.down.b32 %r148, %r36, %r155, %r196, %r197;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r153, %r37, %r155, %r196, %r197;
	// end inline asm
	mov.b64 	%rd153, {%r148, %r153};
	mov.b64 	%fd112, %rd153;
	setp.lt.s32 	%p26, %r147, %r196;
	add.f64 	%fd113, %fd183, %fd112;
	selp.f64 	%fd114, %fd113, %fd183, %p26;
	mov.b64 	%rd154, %fd114;
	mov.u32 	%r165, 2;
	mov.b64 	{%r159, %r164}, %rd154;
	// begin inline asm
	shfl.sync.down.b32 %r158, %r159, %r165, %r196, %r197;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r163, %r164, %r165, %r196, %r197;
	// end inline asm
	mov.b64 	%rd155, {%r158, %r163};
	mov.b64 	%fd115, %rd155;
	add.s32 	%r202, %r147, 2;
	setp.gt.s32 	%p27, %r202, %r196;
	add.f64 	%fd116, %fd114, %fd115;
	selp.f64 	%fd117, %fd114, %fd116, %p27;
	mov.b64 	%rd156, %fd117;
	mov.u32 	%r175, 4;
	mov.b64 	{%r169, %r174}, %rd156;
	// begin inline asm
	shfl.sync.down.b32 %r168, %r169, %r175, %r196, %r197;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r173, %r174, %r175, %r196, %r197;
	// end inline asm
	mov.b64 	%rd157, {%r168, %r173};
	mov.b64 	%fd118, %rd157;
	add.s32 	%r203, %r147, 4;
	setp.gt.s32 	%p28, %r203, %r196;
	add.f64 	%fd119, %fd117, %fd118;
	selp.f64 	%fd120, %fd117, %fd119, %p28;
	mov.b64 	%rd158, %fd120;
	mov.u32 	%r185, 8;
	mov.b64 	{%r179, %r184}, %rd158;
	// begin inline asm
	shfl.sync.down.b32 %r178, %r179, %r185, %r196, %r197;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r183, %r184, %r185, %r196, %r197;
	// end inline asm
	mov.b64 	%rd159, {%r178, %r183};
	mov.b64 	%fd121, %rd159;
	add.s32 	%r204, %r147, 8;
	setp.gt.s32 	%p29, %r204, %r196;
	add.f64 	%fd122, %fd120, %fd121;
	selp.f64 	%fd123, %fd120, %fd122, %p29;
	mov.b64 	%rd160, %fd123;
	mov.u32 	%r195, 16;
	mov.b64 	{%r189, %r194}, %rd160;
	// begin inline asm
	shfl.sync.down.b32 %r188, %r189, %r195, %r196, %r197;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r193, %r194, %r195, %r196, %r197;
	// end inline asm
	mov.b64 	%rd161, {%r188, %r193};
	mov.b64 	%fd124, %rd161;
	add.s32 	%r205, %r147, 16;
	setp.gt.s32 	%p30, %r205, %r196;
	add.f64 	%fd125, %fd123, %fd124;
	selp.f64 	%fd184, %fd123, %fd125, %p30;
	@%p24 bra 	$L__BB5_28;

	add.s32 	%r261, %r146, 8;
	st.shared.f64 	[%r261], %fd184;

$L__BB5_28:
	bar.sync 	0;
	setp.ne.s32 	%p31, %r140, 0;
	@%p31 bra 	$L__BB5_35;

	setp.gt.s32 	%p32, %r22, 32;
	ld.shared.f64 	%fd126, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+16];
	add.f64 	%fd127, %fd184, %fd126;
	selp.f64 	%fd128, %fd127, %fd184, %p32;
	ld.shared.f64 	%fd129, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+24];
	add.f64 	%fd130, %fd128, %fd129;
	setp.gt.s32 	%p33, %r22, 64;
	selp.f64 	%fd131, %fd130, %fd128, %p33;
	ld.shared.f64 	%fd132, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+32];
	add.f64 	%fd133, %fd131, %fd132;
	setp.gt.s32 	%p34, %r22, 96;
	selp.f64 	%fd134, %fd133, %fd131, %p34;
	ld.shared.f64 	%fd135, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+40];
	add.f64 	%fd136, %fd134, %fd135;
	setp.gt.s32 	%p35, %r22, 128;
	selp.f64 	%fd137, %fd136, %fd134, %p35;
	ld.shared.f64 	%fd138, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+48];
	add.f64 	%fd139, %fd137, %fd138;
	setp.gt.s32 	%p36, %r22, 160;
	selp.f64 	%fd140, %fd139, %fd137, %p36;
	ld.shared.f64 	%fd141, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+56];
	add.f64 	%fd142, %fd140, %fd141;
	setp.gt.s32 	%p37, %r22, 192;
	selp.f64 	%fd184, %fd142, %fd140, %p37;
	setp.lt.s32 	%p38, %r22, 225;
	@%p38 bra 	$L__BB5_35;

	ld.shared.f64 	%fd143, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddiN6thrust4plusIdEEE9Policy600EPdS7_iS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+64];
	add.f64 	%fd184, %fd184, %fd143;

$L__BB5_35:
	mov.u32 	%r259, %tid.x;
	setp.ne.s32 	%p46, %r259, 0;
	@%p46 bra 	$L__BB5_37;

	cvta.to.global.u64 	%rd171, %rd18;
	mul.wide.u32 	%rd172, %r40, 8;
	add.s64 	%rd173, %rd171, %rd172;
	st.global.f64 	[%rd173], %fd184;

$L__BB5_37:
	ret;

}
	// .globl	_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_
.visible .entry _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_(
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_0,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_1,
	.param .u64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_2,
	.param .align 1 .b8 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_3[1],
	.param .f64 _ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_4
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<51>;
	.reg .b32 	%r<234>;
	.reg .f64 	%fd<232>;
	.reg .b64 	%rd<211>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage[80];

	ld.param.u64 	%rd29, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_0];
	ld.param.u64 	%rd30, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_1];
	ld.param.u64 	%rd31, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_2];
	ld.param.f64 	%fd32, [_ZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4__param_4];
	mov.u32 	%r1, %tid.x;
	setp.eq.s64 	%p1, %rd31, 0;
	@%p1 bra 	$L__BB6_41;

	setp.lt.s64 	%p2, %rd31, 2560;
	@%p2 bra 	$L__BB6_20;
	bra.uni 	$L__BB6_2;

$L__BB6_20:
	cvt.u32.u64 	%r14, %rd31;
	setp.ge.s32 	%p20, %r1, %r14;
	mov.u32 	%r232, %r1;
	@%p20 bra 	$L__BB6_22;

	mul.wide.s32 	%rd166, %r1, 8;
	add.s64 	%rd165, %rd29, %rd166;
	// begin inline asm
	ld.global.nc.u64 %rd164, [%rd165];
	// end inline asm
	mov.b64 	%fd230, %rd164;
	add.s32 	%r232, %r1, 256;

$L__BB6_22:
	setp.ge.s32 	%p21, %r232, %r14;
	@%p21 bra 	$L__BB6_29;

	not.b32 	%r102, %r232;
	add.s32 	%r17, %r102, %r14;
	shr.u32 	%r103, %r17, 8;
	add.s32 	%r104, %r103, 1;
	and.b32  	%r231, %r104, 3;
	setp.eq.s32 	%p22, %r231, 0;
	@%p22 bra 	$L__BB6_26;

	mul.wide.s32 	%rd167, %r232, 8;
	add.s64 	%rd209, %rd29, %rd167;

$L__BB6_25:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd168, [%rd209];
	// end inline asm
	mov.b64 	%fd148, %rd168;
	add.f64 	%fd230, %fd230, %fd148;
	add.s32 	%r232, %r232, 256;
	add.s64 	%rd209, %rd209, 2048;
	add.s32 	%r231, %r231, -1;
	setp.ne.s32 	%p23, %r231, 0;
	@%p23 bra 	$L__BB6_25;

$L__BB6_26:
	setp.lt.u32 	%p24, %r17, 768;
	@%p24 bra 	$L__BB6_29;

	mul.wide.s32 	%rd170, %r232, 8;
	add.s64 	%rd210, %rd29, %rd170;

$L__BB6_28:
	// begin inline asm
	ld.global.nc.u64 %rd171, [%rd210];
	// end inline asm
	mov.b64 	%fd149, %rd171;
	add.f64 	%fd150, %fd230, %fd149;
	add.s64 	%rd174, %rd210, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd173, [%rd174];
	// end inline asm
	mov.b64 	%fd151, %rd173;
	add.f64 	%fd152, %fd150, %fd151;
	add.s64 	%rd176, %rd210, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd175, [%rd176];
	// end inline asm
	mov.b64 	%fd153, %rd175;
	add.f64 	%fd154, %fd152, %fd153;
	add.s64 	%rd178, %rd210, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd177, [%rd178];
	// end inline asm
	mov.b64 	%fd155, %rd177;
	add.f64 	%fd230, %fd154, %fd155;
	add.s64 	%rd210, %rd210, 8192;
	add.s32 	%r232, %r232, 1024;
	setp.lt.s32 	%p25, %r232, %r14;
	@%p25 bra 	$L__BB6_28;

$L__BB6_29:
	// begin inline asm
	mov.u32 %r105, %laneid;
	// end inline asm
	mov.b64 	%rd179, %fd230;
	mov.b64 	{%r27, %r28}, %rd179;
	shr.s32 	%r106, %r1, 31;
	shr.u32 	%r107, %r106, 27;
	add.s32 	%r108, %r1, %r107;
	shr.s32 	%r29, %r108, 5;
	shl.b32 	%r109, %r29, 3;
	mov.u32 	%r110, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r111, %r110, %r109;
	setp.gt.s32 	%p26, %r14, 255;
	@%p26 bra 	$L__BB6_35;
	bra.uni 	$L__BB6_30;

$L__BB6_35:
	setp.ne.s32 	%p42, %r105, 0;
	// begin inline asm
	mov.u32 %r171, %laneid;
	// end inline asm
	mov.u32 	%r179, 1;
	mov.u32 	%r220, 31;
	mov.u32 	%r221, -1;
	// begin inline asm
	shfl.sync.down.b32 %r172, %r27, %r179, %r220, %r221;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r177, %r28, %r179, %r220, %r221;
	// end inline asm
	mov.b64 	%rd189, {%r172, %r177};
	mov.b64 	%fd188, %rd189;
	setp.gt.s32 	%p43, %r171, 30;
	add.f64 	%fd189, %fd230, %fd188;
	selp.f64 	%fd190, %fd230, %fd189, %p43;
	mov.b64 	%rd190, %fd190;
	mov.u32 	%r189, 2;
	mov.b64 	{%r183, %r188}, %rd190;
	// begin inline asm
	shfl.sync.down.b32 %r182, %r183, %r189, %r220, %r221;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r187, %r188, %r189, %r220, %r221;
	// end inline asm
	mov.b64 	%rd191, {%r182, %r187};
	mov.b64 	%fd191, %rd191;
	setp.gt.s32 	%p44, %r171, 29;
	add.f64 	%fd192, %fd190, %fd191;
	selp.f64 	%fd193, %fd190, %fd192, %p44;
	mov.b64 	%rd192, %fd193;
	mov.u32 	%r199, 4;
	mov.b64 	{%r193, %r198}, %rd192;
	// begin inline asm
	shfl.sync.down.b32 %r192, %r193, %r199, %r220, %r221;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r197, %r198, %r199, %r220, %r221;
	// end inline asm
	mov.b64 	%rd193, {%r192, %r197};
	mov.b64 	%fd194, %rd193;
	setp.gt.s32 	%p45, %r171, 27;
	add.f64 	%fd195, %fd193, %fd194;
	selp.f64 	%fd196, %fd193, %fd195, %p45;
	mov.b64 	%rd194, %fd196;
	mov.u32 	%r209, 8;
	mov.b64 	{%r203, %r208}, %rd194;
	// begin inline asm
	shfl.sync.down.b32 %r202, %r203, %r209, %r220, %r221;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r207, %r208, %r209, %r220, %r221;
	// end inline asm
	mov.b64 	%rd195, {%r202, %r207};
	mov.b64 	%fd197, %rd195;
	setp.gt.s32 	%p46, %r171, 23;
	add.f64 	%fd198, %fd196, %fd197;
	selp.f64 	%fd199, %fd196, %fd198, %p46;
	mov.b64 	%rd196, %fd199;
	mov.u32 	%r219, 16;
	mov.b64 	{%r213, %r218}, %rd196;
	// begin inline asm
	shfl.sync.down.b32 %r212, %r213, %r219, %r220, %r221;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r217, %r218, %r219, %r220, %r221;
	// end inline asm
	mov.b64 	%rd197, {%r212, %r217};
	mov.b64 	%fd200, %rd197;
	setp.gt.s32 	%p47, %r171, 15;
	add.f64 	%fd201, %fd199, %fd200;
	selp.f64 	%fd231, %fd199, %fd201, %p47;
	@%p42 bra 	$L__BB6_37;

	add.s32 	%r224, %r111, 8;
	st.shared.f64 	[%r224], %fd231;

$L__BB6_37:
	bar.sync 	0;
	setp.ne.s32 	%p48, %r1, 0;
	@%p48 bra 	$L__BB6_39;

	ld.shared.f64 	%fd202, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd203, %fd231, %fd202;
	ld.shared.f64 	%fd204, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd205, %fd203, %fd204;
	ld.shared.f64 	%fd206, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd207, %fd205, %fd206;
	ld.shared.f64 	%fd208, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd209, %fd207, %fd208;
	ld.shared.f64 	%fd210, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd211, %fd209, %fd210;
	ld.shared.f64 	%fd212, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd213, %fd211, %fd212;
	ld.shared.f64 	%fd214, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd231, %fd213, %fd214;
	bra.uni 	$L__BB6_39;

$L__BB6_41:
	setp.ne.s32 	%p50, %r1, 0;
	@%p50 bra 	$L__BB6_43;

	cvta.to.global.u64 	%rd199, %rd30;
	st.global.f64 	[%rd199], %fd32;
	bra.uni 	$L__BB6_43;

$L__BB6_2:
	mul.wide.s32 	%rd53, %r1, 8;
	add.s64 	%rd33, %rd29, %rd53;
	// begin inline asm
	ld.global.nc.u64 %rd32, [%rd33];
	// end inline asm
	add.s64 	%rd35, %rd33, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd34, [%rd35];
	// end inline asm
	add.s64 	%rd37, %rd33, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd36, [%rd37];
	// end inline asm
	add.s64 	%rd39, %rd33, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd38, [%rd39];
	// end inline asm
	add.s64 	%rd41, %rd33, 8192;
	// begin inline asm
	ld.global.nc.u64 %rd40, [%rd41];
	// end inline asm
	add.s64 	%rd43, %rd33, 10240;
	// begin inline asm
	ld.global.nc.u64 %rd42, [%rd43];
	// end inline asm
	add.s64 	%rd45, %rd33, 12288;
	// begin inline asm
	ld.global.nc.u64 %rd44, [%rd45];
	// end inline asm
	add.s64 	%rd47, %rd33, 14336;
	// begin inline asm
	ld.global.nc.u64 %rd46, [%rd47];
	// end inline asm
	add.s64 	%rd49, %rd33, 16384;
	// begin inline asm
	ld.global.nc.u64 %rd48, [%rd49];
	// end inline asm
	add.s64 	%rd51, %rd33, 18432;
	// begin inline asm
	ld.global.nc.u64 %rd50, [%rd51];
	// end inline asm
	mov.b64 	%fd33, %rd32;
	mov.b64 	%fd34, %rd34;
	add.f64 	%fd35, %fd33, %fd34;
	mov.b64 	%fd36, %rd36;
	add.f64 	%fd37, %fd35, %fd36;
	mov.b64 	%fd38, %rd38;
	add.f64 	%fd39, %fd37, %fd38;
	mov.b64 	%fd40, %rd40;
	add.f64 	%fd41, %fd39, %fd40;
	mov.b64 	%fd42, %rd42;
	add.f64 	%fd43, %fd41, %fd42;
	mov.b64 	%fd44, %rd44;
	add.f64 	%fd45, %fd43, %fd44;
	mov.b64 	%fd46, %rd46;
	add.f64 	%fd47, %fd45, %fd46;
	mov.b64 	%fd48, %rd48;
	add.f64 	%fd49, %fd47, %fd48;
	mov.b64 	%fd50, %rd50;
	add.f64 	%fd224, %fd49, %fd50;
	setp.lt.s64 	%p3, %rd31, 5120;
	mov.u64 	%rd203, 2560;
	@%p3 bra 	$L__BB6_8;

	add.s64 	%rd57, %rd31, -5120;
	mul.hi.u64 	%rd58, %rd57, -3689348814741910323;
	shr.u64 	%rd59, %rd58, 11;
	mov.u64 	%rd203, 2560;
	add.s64 	%rd2, %rd59, 1;
	and.b64  	%rd3, %rd2, 1;
	setp.lt.u64 	%p4, %rd57, 2560;
	mov.u64 	%rd204, 5120;
	@%p4 bra 	$L__BB6_6;

	sub.s64 	%rd202, %rd2, %rd3;
	mov.u32 	%r31, %tid.x;
	mul.wide.s32 	%rd62, %r31, 8;
	add.s64 	%rd63, %rd29, %rd62;
	add.s64 	%rd5, %rd63, 14336;
	add.s64 	%rd6, %rd63, 16384;

$L__BB6_5:
	shl.b64 	%rd106, %rd203, 3;
	add.s64 	%rd65, %rd63, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd64, [%rd65];
	// end inline asm
	add.s64 	%rd107, %rd63, 2048;
	add.s64 	%rd67, %rd107, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd66, [%rd67];
	// end inline asm
	add.s64 	%rd108, %rd63, 4096;
	add.s64 	%rd69, %rd108, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd68, [%rd69];
	// end inline asm
	add.s64 	%rd109, %rd63, 6144;
	add.s64 	%rd71, %rd109, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd70, [%rd71];
	// end inline asm
	add.s64 	%rd110, %rd63, 8192;
	add.s64 	%rd73, %rd110, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd72, [%rd73];
	// end inline asm
	add.s64 	%rd111, %rd63, 10240;
	add.s64 	%rd75, %rd111, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd74, [%rd75];
	// end inline asm
	add.s64 	%rd112, %rd63, 12288;
	add.s64 	%rd77, %rd112, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd76, [%rd77];
	// end inline asm
	add.s64 	%rd79, %rd5, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd78, [%rd79];
	// end inline asm
	add.s64 	%rd81, %rd6, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd80, [%rd81];
	// end inline asm
	add.s64 	%rd83, %rd51, %rd106;
	// begin inline asm
	ld.global.nc.u64 %rd82, [%rd83];
	// end inline asm
	mov.b64 	%fd52, %rd64;
	add.f64 	%fd53, %fd224, %fd52;
	mov.b64 	%fd54, %rd66;
	add.f64 	%fd55, %fd53, %fd54;
	mov.b64 	%fd56, %rd68;
	add.f64 	%fd57, %fd55, %fd56;
	mov.b64 	%fd58, %rd70;
	add.f64 	%fd59, %fd57, %fd58;
	mov.b64 	%fd60, %rd72;
	add.f64 	%fd61, %fd59, %fd60;
	mov.b64 	%fd62, %rd74;
	add.f64 	%fd63, %fd61, %fd62;
	mov.b64 	%fd64, %rd76;
	add.f64 	%fd65, %fd63, %fd64;
	mov.b64 	%fd66, %rd78;
	add.f64 	%fd67, %fd65, %fd66;
	mov.b64 	%fd68, %rd80;
	add.f64 	%fd69, %fd67, %fd68;
	mov.b64 	%fd70, %rd82;
	add.f64 	%fd71, %fd69, %fd70;
	add.s64 	%rd203, %rd204, 2560;
	shl.b64 	%rd113, %rd204, 3;
	add.s64 	%rd85, %rd63, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd84, [%rd85];
	// end inline asm
	add.s64 	%rd87, %rd107, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd86, [%rd87];
	// end inline asm
	add.s64 	%rd89, %rd108, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd88, [%rd89];
	// end inline asm
	add.s64 	%rd91, %rd109, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd90, [%rd91];
	// end inline asm
	add.s64 	%rd93, %rd110, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd92, [%rd93];
	// end inline asm
	add.s64 	%rd95, %rd111, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd94, [%rd95];
	// end inline asm
	add.s64 	%rd97, %rd112, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd96, [%rd97];
	// end inline asm
	add.s64 	%rd99, %rd5, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd98, [%rd99];
	// end inline asm
	add.s64 	%rd101, %rd6, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd100, [%rd101];
	// end inline asm
	add.s64 	%rd103, %rd51, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd102, [%rd103];
	// end inline asm
	mov.b64 	%fd72, %rd84;
	add.f64 	%fd73, %fd71, %fd72;
	mov.b64 	%fd74, %rd86;
	add.f64 	%fd75, %fd73, %fd74;
	mov.b64 	%fd76, %rd88;
	add.f64 	%fd77, %fd75, %fd76;
	mov.b64 	%fd78, %rd90;
	add.f64 	%fd79, %fd77, %fd78;
	mov.b64 	%fd80, %rd92;
	add.f64 	%fd81, %fd79, %fd80;
	mov.b64 	%fd82, %rd94;
	add.f64 	%fd83, %fd81, %fd82;
	mov.b64 	%fd84, %rd96;
	add.f64 	%fd85, %fd83, %fd84;
	mov.b64 	%fd86, %rd98;
	add.f64 	%fd87, %fd85, %fd86;
	mov.b64 	%fd88, %rd100;
	add.f64 	%fd89, %fd87, %fd88;
	mov.b64 	%fd90, %rd102;
	add.f64 	%fd224, %fd89, %fd90;
	add.s64 	%rd204, %rd204, 5120;
	add.s64 	%rd202, %rd202, -2;
	setp.ne.s64 	%p5, %rd202, 0;
	@%p5 bra 	$L__BB6_5;

$L__BB6_6:
	setp.eq.s64 	%p6, %rd3, 0;
	@%p6 bra 	$L__BB6_8;

	mov.u32 	%r33, %tid.x;
	mul.wide.s32 	%rd134, %r33, 8;
	add.s64 	%rd135, %rd29, %rd134;
	shl.b64 	%rd136, %rd203, 3;
	add.s64 	%rd115, %rd135, %rd136;
	// begin inline asm
	ld.global.nc.u64 %rd114, [%rd115];
	// end inline asm
	add.s64 	%rd117, %rd115, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd116, [%rd117];
	// end inline asm
	add.s64 	%rd119, %rd115, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd118, [%rd119];
	// end inline asm
	add.s64 	%rd121, %rd115, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd120, [%rd121];
	// end inline asm
	add.s64 	%rd123, %rd115, 8192;
	// begin inline asm
	ld.global.nc.u64 %rd122, [%rd123];
	// end inline asm
	add.s64 	%rd125, %rd115, 10240;
	// begin inline asm
	ld.global.nc.u64 %rd124, [%rd125];
	// end inline asm
	add.s64 	%rd127, %rd115, 12288;
	// begin inline asm
	ld.global.nc.u64 %rd126, [%rd127];
	// end inline asm
	add.s64 	%rd129, %rd115, 14336;
	// begin inline asm
	ld.global.nc.u64 %rd128, [%rd129];
	// end inline asm
	add.s64 	%rd131, %rd115, 16384;
	// begin inline asm
	ld.global.nc.u64 %rd130, [%rd131];
	// end inline asm
	add.s64 	%rd133, %rd51, %rd136;
	// begin inline asm
	ld.global.nc.u64 %rd132, [%rd133];
	// end inline asm
	mov.b64 	%fd91, %rd114;
	add.f64 	%fd92, %fd224, %fd91;
	mov.b64 	%fd93, %rd116;
	add.f64 	%fd94, %fd92, %fd93;
	mov.b64 	%fd95, %rd118;
	add.f64 	%fd96, %fd94, %fd95;
	mov.b64 	%fd97, %rd120;
	add.f64 	%fd98, %fd96, %fd97;
	mov.b64 	%fd99, %rd122;
	add.f64 	%fd100, %fd98, %fd99;
	mov.b64 	%fd101, %rd124;
	add.f64 	%fd102, %fd100, %fd101;
	mov.b64 	%fd103, %rd126;
	add.f64 	%fd104, %fd102, %fd103;
	mov.b64 	%fd105, %rd128;
	add.f64 	%fd106, %fd104, %fd105;
	mov.b64 	%fd107, %rd130;
	add.f64 	%fd108, %fd106, %fd107;
	mov.b64 	%fd109, %rd132;
	add.f64 	%fd224, %fd108, %fd109;
	mov.u64 	%rd203, %rd204;

$L__BB6_8:
	setp.ge.s64 	%p7, %rd203, %rd31;
	@%p7 bra 	$L__BB6_16;

	sub.s64 	%rd137, %rd31, %rd203;
	cvt.u32.u64 	%r2, %rd137;
	mov.u32 	%r227, %tid.x;
	setp.ge.s32 	%p8, %r227, %r2;
	@%p8 bra 	$L__BB6_16;

	cvt.u32.u64 	%r35, %rd31;
	not.b32 	%r36, %r227;
	add.s32 	%r37, %r36, %r35;
	cvt.u32.u64 	%r38, %rd203;
	sub.s32 	%r4, %r37, %r38;
	shr.u32 	%r39, %r4, 8;
	add.s32 	%r40, %r39, 1;
	and.b32  	%r226, %r40, 3;
	setp.eq.s32 	%p9, %r226, 0;
	@%p9 bra 	$L__BB6_13;

	mov.u32 	%r227, %tid.x;
	cvt.s64.s32 	%rd138, %r227;
	add.s64 	%rd139, %rd203, %rd138;
	shl.b64 	%rd140, %rd139, 3;
	add.s64 	%rd207, %rd29, %rd140;

$L__BB6_12:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd141, [%rd207];
	// end inline asm
	mov.b64 	%fd111, %rd141;
	add.f64 	%fd224, %fd224, %fd111;
	add.s32 	%r227, %r227, 256;
	add.s64 	%rd207, %rd207, 2048;
	add.s32 	%r226, %r226, -1;
	setp.ne.s32 	%p10, %r226, 0;
	@%p10 bra 	$L__BB6_12;

$L__BB6_13:
	setp.lt.u32 	%p11, %r4, 768;
	@%p11 bra 	$L__BB6_16;

	cvt.s64.s32 	%rd143, %r227;
	add.s64 	%rd144, %rd203, %rd143;
	shl.b64 	%rd145, %rd144, 3;
	add.s64 	%rd208, %rd29, %rd145;

$L__BB6_15:
	// begin inline asm
	ld.global.nc.u64 %rd146, [%rd208];
	// end inline asm
	mov.b64 	%fd112, %rd146;
	add.f64 	%fd113, %fd224, %fd112;
	add.s64 	%rd149, %rd208, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd148, [%rd149];
	// end inline asm
	mov.b64 	%fd114, %rd148;
	add.f64 	%fd115, %fd113, %fd114;
	add.s64 	%rd151, %rd208, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd150, [%rd151];
	// end inline asm
	mov.b64 	%fd116, %rd150;
	add.f64 	%fd117, %fd115, %fd116;
	add.s64 	%rd153, %rd208, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd152, [%rd153];
	// end inline asm
	mov.b64 	%fd118, %rd152;
	add.f64 	%fd224, %fd117, %fd118;
	add.s64 	%rd208, %rd208, 8192;
	add.s32 	%r227, %r227, 1024;
	setp.lt.s32 	%p12, %r227, %r2;
	@%p12 bra 	$L__BB6_15;

$L__BB6_16:
	// begin inline asm
	mov.u32 %r41, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r42, %laneid;
	// end inline asm
	mov.b64 	%rd154, %fd224;
	mov.u32 	%r50, 1;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, -1;
	mov.b64 	{%r44, %r49}, %rd154;
	// begin inline asm
	shfl.sync.down.b32 %r43, %r44, %r50, %r91, %r92;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r48, %r49, %r50, %r91, %r92;
	// end inline asm
	mov.b64 	%rd155, {%r43, %r48};
	mov.b64 	%fd119, %rd155;
	setp.gt.s32 	%p13, %r42, 30;
	add.f64 	%fd120, %fd224, %fd119;
	selp.f64 	%fd121, %fd224, %fd120, %p13;
	mov.b64 	%rd156, %fd121;
	mov.u32 	%r60, 2;
	mov.b64 	{%r54, %r59}, %rd156;
	// begin inline asm
	shfl.sync.down.b32 %r53, %r54, %r60, %r91, %r92;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r58, %r59, %r60, %r91, %r92;
	// end inline asm
	mov.b64 	%rd157, {%r53, %r58};
	mov.b64 	%fd122, %rd157;
	setp.gt.s32 	%p14, %r42, 29;
	add.f64 	%fd123, %fd121, %fd122;
	selp.f64 	%fd124, %fd121, %fd123, %p14;
	mov.b64 	%rd158, %fd124;
	mov.u32 	%r70, 4;
	mov.b64 	{%r64, %r69}, %rd158;
	// begin inline asm
	shfl.sync.down.b32 %r63, %r64, %r70, %r91, %r92;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r68, %r69, %r70, %r91, %r92;
	// end inline asm
	mov.b64 	%rd159, {%r63, %r68};
	mov.b64 	%fd125, %rd159;
	setp.gt.s32 	%p15, %r42, 27;
	add.f64 	%fd126, %fd124, %fd125;
	selp.f64 	%fd127, %fd124, %fd126, %p15;
	mov.b64 	%rd160, %fd127;
	mov.u32 	%r80, 8;
	mov.b64 	{%r74, %r79}, %rd160;
	// begin inline asm
	shfl.sync.down.b32 %r73, %r74, %r80, %r91, %r92;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r78, %r79, %r80, %r91, %r92;
	// end inline asm
	mov.b64 	%rd161, {%r73, %r78};
	mov.b64 	%fd128, %rd161;
	setp.gt.s32 	%p16, %r42, 23;
	add.f64 	%fd129, %fd127, %fd128;
	selp.f64 	%fd130, %fd127, %fd129, %p16;
	mov.b64 	%rd162, %fd130;
	mov.u32 	%r90, 16;
	mov.b64 	{%r84, %r89}, %rd162;
	// begin inline asm
	shfl.sync.down.b32 %r83, %r84, %r90, %r91, %r92;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r88, %r89, %r90, %r91, %r92;
	// end inline asm
	mov.b64 	%rd163, {%r83, %r88};
	mov.b64 	%fd131, %rd163;
	setp.gt.s32 	%p17, %r42, 15;
	add.f64 	%fd132, %fd130, %fd131;
	selp.f64 	%fd231, %fd130, %fd132, %p17;
	setp.ne.s32 	%p18, %r41, 0;
	@%p18 bra 	$L__BB6_18;

	mov.u32 	%r93, %tid.x;
	shr.s32 	%r94, %r93, 31;
	shr.u32 	%r95, %r94, 27;
	add.s32 	%r96, %r93, %r95;
	shr.s32 	%r97, %r96, 5;
	shl.b32 	%r98, %r97, 3;
	mov.u32 	%r99, _ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage;
	add.s32 	%r100, %r99, %r98;
	st.shared.f64 	[%r100+8], %fd231;

$L__BB6_18:
	bar.sync 	0;
	mov.u32 	%r101, %tid.x;
	setp.ne.s32 	%p19, %r101, 0;
	@%p19 bra 	$L__BB6_39;

	ld.shared.f64 	%fd133, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd134, %fd231, %fd133;
	ld.shared.f64 	%fd135, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd136, %fd134, %fd135;
	ld.shared.f64 	%fd137, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd138, %fd136, %fd137;
	ld.shared.f64 	%fd139, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd140, %fd138, %fd139;
	ld.shared.f64 	%fd141, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd142, %fd140, %fd141;
	ld.shared.f64 	%fd143, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd144, %fd142, %fd143;
	ld.shared.f64 	%fd145, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd231, %fd144, %fd145;
	bra.uni 	$L__BB6_39;

$L__BB6_30:
	setp.ne.s32 	%p27, %r105, 0;
	shl.b32 	%r163, %r29, 5;
	add.s32 	%r164, %r163, 32;
	setp.gt.s32 	%p28, %r164, %r14;
	// begin inline asm
	mov.u32 %r112, %laneid;
	// end inline asm
	not.b32 	%r165, %r163;
	mov.u32 	%r162, -1;
	add.s32 	%r166, %r14, %r165;
	selp.b32 	%r161, %r166, 31, %p28;
	mov.u32 	%r120, 1;
	// begin inline asm
	shfl.sync.down.b32 %r113, %r27, %r120, %r161, %r162;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r118, %r28, %r120, %r161, %r162;
	// end inline asm
	mov.b64 	%rd180, {%r113, %r118};
	mov.b64 	%fd156, %rd180;
	setp.lt.s32 	%p29, %r112, %r161;
	add.f64 	%fd157, %fd230, %fd156;
	selp.f64 	%fd158, %fd157, %fd230, %p29;
	mov.b64 	%rd181, %fd158;
	mov.u32 	%r130, 2;
	mov.b64 	{%r124, %r129}, %rd181;
	// begin inline asm
	shfl.sync.down.b32 %r123, %r124, %r130, %r161, %r162;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r128, %r129, %r130, %r161, %r162;
	// end inline asm
	mov.b64 	%rd182, {%r123, %r128};
	mov.b64 	%fd159, %rd182;
	add.s32 	%r167, %r112, 2;
	setp.gt.s32 	%p30, %r167, %r161;
	add.f64 	%fd160, %fd158, %fd159;
	selp.f64 	%fd161, %fd158, %fd160, %p30;
	mov.b64 	%rd183, %fd161;
	mov.u32 	%r140, 4;
	mov.b64 	{%r134, %r139}, %rd183;
	// begin inline asm
	shfl.sync.down.b32 %r133, %r134, %r140, %r161, %r162;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r138, %r139, %r140, %r161, %r162;
	// end inline asm
	mov.b64 	%rd184, {%r133, %r138};
	mov.b64 	%fd162, %rd184;
	add.s32 	%r168, %r112, 4;
	setp.gt.s32 	%p31, %r168, %r161;
	add.f64 	%fd163, %fd161, %fd162;
	selp.f64 	%fd164, %fd161, %fd163, %p31;
	mov.b64 	%rd185, %fd164;
	mov.u32 	%r150, 8;
	mov.b64 	{%r144, %r149}, %rd185;
	// begin inline asm
	shfl.sync.down.b32 %r143, %r144, %r150, %r161, %r162;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r148, %r149, %r150, %r161, %r162;
	// end inline asm
	mov.b64 	%rd186, {%r143, %r148};
	mov.b64 	%fd165, %rd186;
	add.s32 	%r169, %r112, 8;
	setp.gt.s32 	%p32, %r169, %r161;
	add.f64 	%fd166, %fd164, %fd165;
	selp.f64 	%fd167, %fd164, %fd166, %p32;
	mov.b64 	%rd187, %fd167;
	mov.u32 	%r160, 16;
	mov.b64 	{%r154, %r159}, %rd187;
	// begin inline asm
	shfl.sync.down.b32 %r153, %r154, %r160, %r161, %r162;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r158, %r159, %r160, %r161, %r162;
	// end inline asm
	mov.b64 	%rd188, {%r153, %r158};
	mov.b64 	%fd168, %rd188;
	add.s32 	%r170, %r112, 16;
	setp.gt.s32 	%p33, %r170, %r161;
	add.f64 	%fd169, %fd167, %fd168;
	selp.f64 	%fd231, %fd167, %fd169, %p33;
	@%p27 bra 	$L__BB6_32;

	add.s32 	%r223, %r111, 8;
	st.shared.f64 	[%r223], %fd231;

$L__BB6_32:
	bar.sync 	0;
	setp.ne.s32 	%p34, %r1, 0;
	@%p34 bra 	$L__BB6_39;

	setp.gt.s32 	%p35, %r14, 32;
	ld.shared.f64 	%fd170, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+16];
	add.f64 	%fd171, %fd231, %fd170;
	selp.f64 	%fd172, %fd171, %fd231, %p35;
	ld.shared.f64 	%fd173, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+24];
	add.f64 	%fd174, %fd172, %fd173;
	setp.gt.s32 	%p36, %r14, 64;
	selp.f64 	%fd175, %fd174, %fd172, %p36;
	ld.shared.f64 	%fd176, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+32];
	add.f64 	%fd177, %fd175, %fd176;
	setp.gt.s32 	%p37, %r14, 96;
	selp.f64 	%fd178, %fd177, %fd175, %p37;
	ld.shared.f64 	%fd179, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+40];
	add.f64 	%fd180, %fd178, %fd179;
	setp.gt.s32 	%p38, %r14, 128;
	selp.f64 	%fd181, %fd180, %fd178, %p38;
	ld.shared.f64 	%fd182, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+48];
	add.f64 	%fd183, %fd181, %fd182;
	setp.gt.s32 	%p39, %r14, 160;
	selp.f64 	%fd184, %fd183, %fd181, %p39;
	ld.shared.f64 	%fd185, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+56];
	add.f64 	%fd186, %fd184, %fd185;
	setp.gt.s32 	%p40, %r14, 192;
	selp.f64 	%fd231, %fd186, %fd184, %p40;
	setp.lt.s32 	%p41, %r14, 225;
	@%p41 bra 	$L__BB6_39;

	ld.shared.f64 	%fd187, [_ZZN3cub28DeviceReduceSingleTileKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_dEEvT0_T1_T2_T3_T4_E12temp_storage+64];
	add.f64 	%fd231, %fd231, %fd187;

$L__BB6_39:
	mov.u32 	%r222, %tid.x;
	setp.ne.s32 	%p49, %r222, 0;
	@%p49 bra 	$L__BB6_43;

	add.f64 	%fd215, %fd231, %fd32;
	cvta.to.global.u64 	%rd198, %rd30;
	st.global.f64 	[%rd198], %fd215;

$L__BB6_43:
	ret;

}
	// .globl	_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_
.visible .entry _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_(
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_0,
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_1,
	.param .u64 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_2,
	.param .align 8 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_3[72],
	.param .align 1 .b8 _ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_4[1]
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<47>;
	.reg .b32 	%r<263>;
	.reg .f64 	%fd<185>;
	.reg .b64 	%rd<202>;
	// demoted variable
	.shared .align 8 .b8 _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage[80];

	ld.param.u64 	%rd25, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_0];
	ld.param.u64 	%rd26, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_1];
	ld.param.u64 	%rd1, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_3+32];
	ld.param.u32 	%r33, [_ZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3__param_3+40];
	mul.lo.s32 	%r1, %r33, 2560;
	mov.u32 	%r34, %ctaid.x;
	mul.lo.s32 	%r35, %r34, 2560;
	cvt.s64.s32 	%rd2, %r35;
	add.s64 	%rd27, %rd2, 2560;
	setp.gt.s64 	%p1, %rd27, %rd1;
	mov.u32 	%r261, %tid.x;
	@%p1 bra 	$L__BB7_16;
	bra.uni 	$L__BB7_1;

$L__BB7_16:
	cvt.u32.u64 	%r124, %rd2;
	cvt.u32.u64 	%r15, %rd1;
	sub.s32 	%r16, %r15, %r124;
	setp.ge.s32 	%p17, %r261, %r16;
	@%p17 bra 	$L__BB7_18;

	cvt.s64.s32 	%rd155, %r261;
	add.s64 	%rd156, %rd2, %rd155;
	shl.b64 	%rd157, %rd156, 3;
	add.s64 	%rd154, %rd25, %rd157;
	// begin inline asm
	ld.global.nc.u64 %rd153, [%rd154];
	// end inline asm
	mov.b64 	%fd183, %rd153;
	add.s32 	%r261, %r261, 256;

$L__BB7_18:
	setp.ge.s32 	%p18, %r261, %r16;
	@%p18 bra 	$L__BB7_25;

	not.b32 	%r125, %r261;
	add.s32 	%r126, %r125, %r15;
	mad.lo.s32 	%r19, %r34, -2560, %r126;
	shr.u32 	%r128, %r19, 8;
	add.s32 	%r129, %r128, 1;
	and.b32  	%r260, %r129, 3;
	setp.eq.s32 	%p19, %r260, 0;
	@%p19 bra 	$L__BB7_22;

	cvt.s64.s32 	%rd158, %r261;
	add.s64 	%rd159, %rd158, %rd2;
	shl.b64 	%rd160, %rd159, 3;
	add.s64 	%rd200, %rd25, %rd160;

$L__BB7_21:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd161, [%rd200];
	// end inline asm
	mov.b64 	%fd104, %rd161;
	add.f64 	%fd183, %fd183, %fd104;
	add.s32 	%r261, %r261, 256;
	add.s64 	%rd200, %rd200, 2048;
	add.s32 	%r260, %r260, -1;
	setp.ne.s32 	%p20, %r260, 0;
	@%p20 bra 	$L__BB7_21;

$L__BB7_22:
	setp.lt.u32 	%p21, %r19, 768;
	@%p21 bra 	$L__BB7_25;

	cvt.s64.s32 	%rd163, %r261;
	add.s64 	%rd164, %rd163, %rd2;
	shl.b64 	%rd165, %rd164, 3;
	add.s64 	%rd201, %rd25, %rd165;

$L__BB7_24:
	// begin inline asm
	ld.global.nc.u64 %rd166, [%rd201];
	// end inline asm
	mov.b64 	%fd105, %rd166;
	add.f64 	%fd106, %fd183, %fd105;
	add.s64 	%rd169, %rd201, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd168, [%rd169];
	// end inline asm
	mov.b64 	%fd107, %rd168;
	add.f64 	%fd108, %fd106, %fd107;
	add.s64 	%rd171, %rd201, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd170, [%rd171];
	// end inline asm
	mov.b64 	%fd109, %rd170;
	add.f64 	%fd110, %fd108, %fd109;
	add.s64 	%rd173, %rd201, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd172, [%rd173];
	// end inline asm
	mov.b64 	%fd111, %rd172;
	add.f64 	%fd183, %fd110, %fd111;
	add.s64 	%rd201, %rd201, 8192;
	add.s32 	%r261, %r261, 1024;
	setp.lt.s32 	%p22, %r261, %r16;
	@%p22 bra 	$L__BB7_24;

$L__BB7_25:
	mov.u32 	%r131, %tid.x;
	shr.s32 	%r132, %r131, 31;
	shr.u32 	%r133, %r132, 27;
	add.s32 	%r134, %r131, %r133;
	shr.s32 	%r28, %r134, 5;
	// begin inline asm
	mov.u32 %r130, %laneid;
	// end inline asm
	mov.b64 	%rd174, %fd183;
	mov.b64 	{%r30, %r31}, %rd174;
	shl.b32 	%r135, %r28, 3;
	mov.u32 	%r136, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage;
	add.s32 	%r137, %r136, %r135;
	setp.gt.s32 	%p23, %r16, 255;
	@%p23 bra 	$L__BB7_31;
	bra.uni 	$L__BB7_26;

$L__BB7_31:
	setp.ne.s32 	%p39, %r130, 0;
	// begin inline asm
	mov.u32 %r198, %laneid;
	// end inline asm
	mov.u32 	%r206, 1;
	mov.u32 	%r247, 31;
	mov.u32 	%r248, -1;
	// begin inline asm
	shfl.sync.down.b32 %r199, %r30, %r206, %r247, %r248;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r204, %r31, %r206, %r247, %r248;
	// end inline asm
	mov.b64 	%rd184, {%r199, %r204};
	mov.b64 	%fd144, %rd184;
	setp.gt.s32 	%p40, %r198, 30;
	add.f64 	%fd145, %fd183, %fd144;
	selp.f64 	%fd146, %fd183, %fd145, %p40;
	mov.b64 	%rd185, %fd146;
	mov.u32 	%r216, 2;
	mov.b64 	{%r210, %r215}, %rd185;
	// begin inline asm
	shfl.sync.down.b32 %r209, %r210, %r216, %r247, %r248;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r214, %r215, %r216, %r247, %r248;
	// end inline asm
	mov.b64 	%rd186, {%r209, %r214};
	mov.b64 	%fd147, %rd186;
	setp.gt.s32 	%p41, %r198, 29;
	add.f64 	%fd148, %fd146, %fd147;
	selp.f64 	%fd149, %fd146, %fd148, %p41;
	mov.b64 	%rd187, %fd149;
	mov.u32 	%r226, 4;
	mov.b64 	{%r220, %r225}, %rd187;
	// begin inline asm
	shfl.sync.down.b32 %r219, %r220, %r226, %r247, %r248;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r224, %r225, %r226, %r247, %r248;
	// end inline asm
	mov.b64 	%rd188, {%r219, %r224};
	mov.b64 	%fd150, %rd188;
	setp.gt.s32 	%p42, %r198, 27;
	add.f64 	%fd151, %fd149, %fd150;
	selp.f64 	%fd152, %fd149, %fd151, %p42;
	mov.b64 	%rd189, %fd152;
	mov.u32 	%r236, 8;
	mov.b64 	{%r230, %r235}, %rd189;
	// begin inline asm
	shfl.sync.down.b32 %r229, %r230, %r236, %r247, %r248;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r234, %r235, %r236, %r247, %r248;
	// end inline asm
	mov.b64 	%rd190, {%r229, %r234};
	mov.b64 	%fd153, %rd190;
	setp.gt.s32 	%p43, %r198, 23;
	add.f64 	%fd154, %fd152, %fd153;
	selp.f64 	%fd155, %fd152, %fd154, %p43;
	mov.b64 	%rd191, %fd155;
	mov.u32 	%r246, 16;
	mov.b64 	{%r240, %r245}, %rd191;
	// begin inline asm
	shfl.sync.down.b32 %r239, %r240, %r246, %r247, %r248;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r244, %r245, %r246, %r247, %r248;
	// end inline asm
	mov.b64 	%rd192, {%r239, %r244};
	mov.b64 	%fd156, %rd192;
	setp.gt.s32 	%p44, %r198, 15;
	add.f64 	%fd157, %fd155, %fd156;
	selp.f64 	%fd184, %fd155, %fd157, %p44;
	@%p39 bra 	$L__BB7_33;

	add.s32 	%r253, %r137, 8;
	st.shared.f64 	[%r253], %fd184;

$L__BB7_33:
	bar.sync 	0;
	setp.ne.s32 	%p45, %r131, 0;
	@%p45 bra 	$L__BB7_35;

	ld.shared.f64 	%fd158, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+16];
	add.f64 	%fd159, %fd184, %fd158;
	ld.shared.f64 	%fd160, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+24];
	add.f64 	%fd161, %fd159, %fd160;
	ld.shared.f64 	%fd162, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+32];
	add.f64 	%fd163, %fd161, %fd162;
	ld.shared.f64 	%fd164, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+40];
	add.f64 	%fd165, %fd163, %fd164;
	ld.shared.f64 	%fd166, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+48];
	add.f64 	%fd167, %fd165, %fd166;
	ld.shared.f64 	%fd168, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+56];
	add.f64 	%fd169, %fd167, %fd168;
	ld.shared.f64 	%fd170, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+64];
	add.f64 	%fd184, %fd169, %fd170;
	bra.uni 	$L__BB7_35;

$L__BB7_1:
	cvt.s64.s32 	%rd48, %r261;
	add.s64 	%rd49, %rd48, %rd2;
	shl.b64 	%rd50, %rd49, 3;
	add.s64 	%rd29, %rd25, %rd50;
	// begin inline asm
	ld.global.nc.u64 %rd28, [%rd29];
	// end inline asm
	add.s32 	%r36, %r261, 256;
	cvt.s64.s32 	%rd51, %r36;
	add.s64 	%rd52, %rd51, %rd2;
	shl.b64 	%rd53, %rd52, 3;
	add.s64 	%rd31, %rd25, %rd53;
	// begin inline asm
	ld.global.nc.u64 %rd30, [%rd31];
	// end inline asm
	add.s32 	%r37, %r261, 512;
	cvt.s64.s32 	%rd54, %r37;
	add.s64 	%rd55, %rd54, %rd2;
	shl.b64 	%rd56, %rd55, 3;
	add.s64 	%rd33, %rd25, %rd56;
	// begin inline asm
	ld.global.nc.u64 %rd32, [%rd33];
	// end inline asm
	add.s32 	%r38, %r261, 768;
	cvt.s64.s32 	%rd57, %r38;
	add.s64 	%rd58, %rd57, %rd2;
	shl.b64 	%rd59, %rd58, 3;
	add.s64 	%rd35, %rd25, %rd59;
	// begin inline asm
	ld.global.nc.u64 %rd34, [%rd35];
	// end inline asm
	add.s32 	%r39, %r261, 1024;
	cvt.s64.s32 	%rd60, %r39;
	add.s64 	%rd61, %rd60, %rd2;
	shl.b64 	%rd62, %rd61, 3;
	add.s64 	%rd37, %rd25, %rd62;
	// begin inline asm
	ld.global.nc.u64 %rd36, [%rd37];
	// end inline asm
	add.s32 	%r40, %r261, 1280;
	cvt.s64.s32 	%rd63, %r40;
	add.s64 	%rd64, %rd63, %rd2;
	shl.b64 	%rd65, %rd64, 3;
	add.s64 	%rd39, %rd25, %rd65;
	// begin inline asm
	ld.global.nc.u64 %rd38, [%rd39];
	// end inline asm
	add.s32 	%r41, %r261, 1536;
	cvt.s64.s32 	%rd66, %r41;
	add.s64 	%rd67, %rd66, %rd2;
	shl.b64 	%rd68, %rd67, 3;
	add.s64 	%rd41, %rd25, %rd68;
	// begin inline asm
	ld.global.nc.u64 %rd40, [%rd41];
	// end inline asm
	add.s32 	%r42, %r261, 1792;
	cvt.s64.s32 	%rd69, %r42;
	add.s64 	%rd70, %rd69, %rd2;
	shl.b64 	%rd71, %rd70, 3;
	add.s64 	%rd43, %rd25, %rd71;
	// begin inline asm
	ld.global.nc.u64 %rd42, [%rd43];
	// end inline asm
	add.s32 	%r43, %r261, 2048;
	cvt.s64.s32 	%rd3, %r43;
	add.s64 	%rd72, %rd3, %rd2;
	shl.b64 	%rd73, %rd72, 3;
	add.s64 	%rd45, %rd25, %rd73;
	// begin inline asm
	ld.global.nc.u64 %rd44, [%rd45];
	// end inline asm
	add.s32 	%r44, %r261, 2304;
	cvt.s64.s32 	%rd4, %r44;
	add.s64 	%rd74, %rd4, %rd2;
	shl.b64 	%rd75, %rd74, 3;
	add.s64 	%rd47, %rd25, %rd75;
	// begin inline asm
	ld.global.nc.u64 %rd46, [%rd47];
	// end inline asm
	mov.b64 	%fd29, %rd28;
	mov.b64 	%fd30, %rd30;
	add.f64 	%fd31, %fd29, %fd30;
	mov.b64 	%fd32, %rd32;
	add.f64 	%fd33, %fd31, %fd32;
	mov.b64 	%fd34, %rd34;
	add.f64 	%fd35, %fd33, %fd34;
	mov.b64 	%fd36, %rd36;
	add.f64 	%fd37, %fd35, %fd36;
	mov.b64 	%fd38, %rd38;
	add.f64 	%fd39, %fd37, %fd38;
	mov.b64 	%fd40, %rd40;
	add.f64 	%fd41, %fd39, %fd40;
	mov.b64 	%fd42, %rd42;
	add.f64 	%fd43, %fd41, %fd42;
	mov.b64 	%fd44, %rd44;
	add.f64 	%fd45, %fd43, %fd44;
	mov.b64 	%fd46, %rd46;
	add.f64 	%fd177, %fd45, %fd46;
	cvt.s64.s32 	%rd76, %r1;
	add.s64 	%rd197, %rd2, %rd76;
	add.s64 	%rd77, %rd197, 2560;
	setp.gt.s64 	%p2, %rd77, %rd1;
	@%p2 bra 	$L__BB7_4;

	add.s64 	%rd197, %rd2, %rd76;
	mov.u32 	%r47, %tid.x;
	add.s32 	%r48, %r47, 1536;
	cvt.s64.s32 	%rd8, %r48;
	add.s32 	%r49, %r47, 1792;
	cvt.s64.s32 	%rd9, %r49;
	add.s32 	%r51, %r47, 256;

$L__BB7_3:
	cvt.s64.s32 	%rd99, %r47;
	add.s64 	%rd100, %rd197, %rd99;
	shl.b64 	%rd101, %rd100, 3;
	add.s64 	%rd80, %rd25, %rd101;
	// begin inline asm
	ld.global.nc.u64 %rd79, [%rd80];
	// end inline asm
	cvt.s64.s32 	%rd102, %r51;
	add.s64 	%rd103, %rd197, %rd102;
	shl.b64 	%rd104, %rd103, 3;
	add.s64 	%rd82, %rd25, %rd104;
	// begin inline asm
	ld.global.nc.u64 %rd81, [%rd82];
	// end inline asm
	add.s32 	%r52, %r47, 512;
	cvt.s64.s32 	%rd105, %r52;
	add.s64 	%rd106, %rd197, %rd105;
	shl.b64 	%rd107, %rd106, 3;
	add.s64 	%rd84, %rd25, %rd107;
	// begin inline asm
	ld.global.nc.u64 %rd83, [%rd84];
	// end inline asm
	add.s32 	%r53, %r47, 768;
	cvt.s64.s32 	%rd108, %r53;
	add.s64 	%rd109, %rd197, %rd108;
	shl.b64 	%rd110, %rd109, 3;
	add.s64 	%rd86, %rd25, %rd110;
	// begin inline asm
	ld.global.nc.u64 %rd85, [%rd86];
	// end inline asm
	add.s32 	%r54, %r47, 1024;
	cvt.s64.s32 	%rd111, %r54;
	add.s64 	%rd112, %rd197, %rd111;
	shl.b64 	%rd113, %rd112, 3;
	add.s64 	%rd88, %rd25, %rd113;
	// begin inline asm
	ld.global.nc.u64 %rd87, [%rd88];
	// end inline asm
	add.s32 	%r55, %r47, 1280;
	cvt.s64.s32 	%rd114, %r55;
	add.s64 	%rd115, %rd197, %rd114;
	shl.b64 	%rd116, %rd115, 3;
	add.s64 	%rd90, %rd25, %rd116;
	// begin inline asm
	ld.global.nc.u64 %rd89, [%rd90];
	// end inline asm
	add.s64 	%rd117, %rd197, %rd8;
	shl.b64 	%rd118, %rd117, 3;
	add.s64 	%rd92, %rd25, %rd118;
	// begin inline asm
	ld.global.nc.u64 %rd91, [%rd92];
	// end inline asm
	add.s64 	%rd119, %rd197, %rd9;
	shl.b64 	%rd120, %rd119, 3;
	add.s64 	%rd94, %rd25, %rd120;
	// begin inline asm
	ld.global.nc.u64 %rd93, [%rd94];
	// end inline asm
	add.s64 	%rd121, %rd197, %rd3;
	shl.b64 	%rd122, %rd121, 3;
	add.s64 	%rd96, %rd25, %rd122;
	// begin inline asm
	ld.global.nc.u64 %rd95, [%rd96];
	// end inline asm
	add.s64 	%rd123, %rd197, %rd4;
	shl.b64 	%rd124, %rd123, 3;
	add.s64 	%rd98, %rd25, %rd124;
	// begin inline asm
	ld.global.nc.u64 %rd97, [%rd98];
	// end inline asm
	mov.b64 	%fd47, %rd79;
	add.f64 	%fd48, %fd177, %fd47;
	mov.b64 	%fd49, %rd81;
	add.f64 	%fd50, %fd48, %fd49;
	mov.b64 	%fd51, %rd83;
	add.f64 	%fd52, %fd50, %fd51;
	mov.b64 	%fd53, %rd85;
	add.f64 	%fd54, %fd52, %fd53;
	mov.b64 	%fd55, %rd87;
	add.f64 	%fd56, %fd54, %fd55;
	mov.b64 	%fd57, %rd89;
	add.f64 	%fd58, %fd56, %fd57;
	mov.b64 	%fd59, %rd91;
	add.f64 	%fd60, %fd58, %fd59;
	mov.b64 	%fd61, %rd93;
	add.f64 	%fd62, %fd60, %fd61;
	mov.b64 	%fd63, %rd95;
	add.f64 	%fd64, %fd62, %fd63;
	mov.b64 	%fd65, %rd97;
	add.f64 	%fd177, %fd64, %fd65;
	add.s64 	%rd197, %rd197, %rd76;
	add.s64 	%rd125, %rd197, 2560;
	setp.le.s64 	%p3, %rd125, %rd1;
	@%p3 bra 	$L__BB7_3;

$L__BB7_4:
	setp.le.s64 	%p4, %rd1, %rd197;
	@%p4 bra 	$L__BB7_12;

	sub.s64 	%rd126, %rd1, %rd197;
	cvt.u32.u64 	%r3, %rd126;
	mov.u32 	%r256, %tid.x;
	setp.ge.s32 	%p5, %r256, %r3;
	@%p5 bra 	$L__BB7_12;

	cvt.u32.u64 	%r57, %rd1;
	not.b32 	%r58, %r256;
	add.s32 	%r59, %r58, %r57;
	cvt.u32.u64 	%r60, %rd197;
	sub.s32 	%r5, %r59, %r60;
	shr.u32 	%r61, %r5, 8;
	add.s32 	%r62, %r61, 1;
	and.b32  	%r255, %r62, 3;
	setp.eq.s32 	%p6, %r255, 0;
	@%p6 bra 	$L__BB7_9;

	mov.u32 	%r256, %tid.x;
	cvt.s64.s32 	%rd127, %r256;
	add.s64 	%rd128, %rd197, %rd127;
	shl.b64 	%rd129, %rd128, 3;
	add.s64 	%rd198, %rd25, %rd129;

$L__BB7_8:
	.pragma "nounroll";
	// begin inline asm
	ld.global.nc.u64 %rd130, [%rd198];
	// end inline asm
	mov.b64 	%fd67, %rd130;
	add.f64 	%fd177, %fd177, %fd67;
	add.s32 	%r256, %r256, 256;
	add.s64 	%rd198, %rd198, 2048;
	add.s32 	%r255, %r255, -1;
	setp.ne.s32 	%p7, %r255, 0;
	@%p7 bra 	$L__BB7_8;

$L__BB7_9:
	setp.lt.u32 	%p8, %r5, 768;
	@%p8 bra 	$L__BB7_12;

	cvt.s64.s32 	%rd132, %r256;
	add.s64 	%rd133, %rd197, %rd132;
	shl.b64 	%rd134, %rd133, 3;
	add.s64 	%rd199, %rd25, %rd134;

$L__BB7_11:
	// begin inline asm
	ld.global.nc.u64 %rd135, [%rd199];
	// end inline asm
	mov.b64 	%fd68, %rd135;
	add.f64 	%fd69, %fd177, %fd68;
	add.s64 	%rd138, %rd199, 2048;
	// begin inline asm
	ld.global.nc.u64 %rd137, [%rd138];
	// end inline asm
	mov.b64 	%fd70, %rd137;
	add.f64 	%fd71, %fd69, %fd70;
	add.s64 	%rd140, %rd199, 4096;
	// begin inline asm
	ld.global.nc.u64 %rd139, [%rd140];
	// end inline asm
	mov.b64 	%fd72, %rd139;
	add.f64 	%fd73, %fd71, %fd72;
	add.s64 	%rd142, %rd199, 6144;
	// begin inline asm
	ld.global.nc.u64 %rd141, [%rd142];
	// end inline asm
	mov.b64 	%fd74, %rd141;
	add.f64 	%fd177, %fd73, %fd74;
	add.s64 	%rd199, %rd199, 8192;
	add.s32 	%r256, %r256, 1024;
	setp.lt.s32 	%p9, %r256, %r3;
	@%p9 bra 	$L__BB7_11;

$L__BB7_12:
	// begin inline asm
	mov.u32 %r63, %laneid;
	// end inline asm
	// begin inline asm
	mov.u32 %r64, %laneid;
	// end inline asm
	mov.b64 	%rd143, %fd177;
	mov.u32 	%r72, 1;
	mov.u32 	%r113, 31;
	mov.u32 	%r114, -1;
	mov.b64 	{%r66, %r71}, %rd143;
	// begin inline asm
	shfl.sync.down.b32 %r65, %r66, %r72, %r113, %r114;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r70, %r71, %r72, %r113, %r114;
	// end inline asm
	mov.b64 	%rd144, {%r65, %r70};
	mov.b64 	%fd75, %rd144;
	setp.gt.s32 	%p10, %r64, 30;
	add.f64 	%fd76, %fd177, %fd75;
	selp.f64 	%fd77, %fd177, %fd76, %p10;
	mov.b64 	%rd145, %fd77;
	mov.u32 	%r82, 2;
	mov.b64 	{%r76, %r81}, %rd145;
	// begin inline asm
	shfl.sync.down.b32 %r75, %r76, %r82, %r113, %r114;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r80, %r81, %r82, %r113, %r114;
	// end inline asm
	mov.b64 	%rd146, {%r75, %r80};
	mov.b64 	%fd78, %rd146;
	setp.gt.s32 	%p11, %r64, 29;
	add.f64 	%fd79, %fd77, %fd78;
	selp.f64 	%fd80, %fd77, %fd79, %p11;
	mov.b64 	%rd147, %fd80;
	mov.u32 	%r92, 4;
	mov.b64 	{%r86, %r91}, %rd147;
	// begin inline asm
	shfl.sync.down.b32 %r85, %r86, %r92, %r113, %r114;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r90, %r91, %r92, %r113, %r114;
	// end inline asm
	mov.b64 	%rd148, {%r85, %r90};
	mov.b64 	%fd81, %rd148;
	setp.gt.s32 	%p12, %r64, 27;
	add.f64 	%fd82, %fd80, %fd81;
	selp.f64 	%fd83, %fd80, %fd82, %p12;
	mov.b64 	%rd149, %fd83;
	mov.u32 	%r102, 8;
	mov.b64 	{%r96, %r101}, %rd149;
	// begin inline asm
	shfl.sync.down.b32 %r95, %r96, %r102, %r113, %r114;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r100, %r101, %r102, %r113, %r114;
	// end inline asm
	mov.b64 	%rd150, {%r95, %r100};
	mov.b64 	%fd84, %rd150;
	setp.gt.s32 	%p13, %r64, 23;
	add.f64 	%fd85, %fd83, %fd84;
	selp.f64 	%fd86, %fd83, %fd85, %p13;
	mov.b64 	%rd151, %fd86;
	mov.u32 	%r112, 16;
	mov.b64 	{%r106, %r111}, %rd151;
	// begin inline asm
	shfl.sync.down.b32 %r105, %r106, %r112, %r113, %r114;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r110, %r111, %r112, %r113, %r114;
	// end inline asm
	mov.b64 	%rd152, {%r105, %r110};
	mov.b64 	%fd87, %rd152;
	setp.gt.s32 	%p14, %r64, 15;
	add.f64 	%fd88, %fd86, %fd87;
	selp.f64 	%fd184, %fd86, %fd88, %p14;
	setp.ne.s32 	%p15, %r63, 0;
	@%p15 bra 	$L__BB7_14;

	mov.u32 	%r115, %tid.x;
	shr.s32 	%r116, %r115, 31;
	shr.u32 	%r117, %r116, 27;
	add.s32 	%r118, %r115, %r117;
	shr.s32 	%r119, %r118, 5;
	shl.b32 	%r120, %r119, 3;
	mov.u32 	%r121, _ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage;
	add.s32 	%r122, %r121, %r120;
	st.shared.f64 	[%r122+8], %fd184;

$L__BB7_14:
	bar.sync 	0;
	mov.u32 	%r123, %tid.x;
	setp.ne.s32 	%p16, %r123, 0;
	@%p16 bra 	$L__BB7_35;

	ld.shared.f64 	%fd89, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+16];
	add.f64 	%fd90, %fd184, %fd89;
	ld.shared.f64 	%fd91, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+24];
	add.f64 	%fd92, %fd90, %fd91;
	ld.shared.f64 	%fd93, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+32];
	add.f64 	%fd94, %fd92, %fd93;
	ld.shared.f64 	%fd95, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+40];
	add.f64 	%fd96, %fd94, %fd95;
	ld.shared.f64 	%fd97, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+48];
	add.f64 	%fd98, %fd96, %fd97;
	ld.shared.f64 	%fd99, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+56];
	add.f64 	%fd100, %fd98, %fd99;
	ld.shared.f64 	%fd101, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+64];
	add.f64 	%fd184, %fd100, %fd101;
	bra.uni 	$L__BB7_35;

$L__BB7_26:
	setp.ne.s32 	%p24, %r130, 0;
	shl.b32 	%r189, %r28, 5;
	add.s32 	%r190, %r189, 32;
	setp.gt.s32 	%p25, %r190, %r16;
	// begin inline asm
	mov.u32 %r138, %laneid;
	// end inline asm
	not.b32 	%r191, %r189;
	mov.u32 	%r188, -1;
	add.s32 	%r192, %r16, %r191;
	selp.b32 	%r187, %r192, 31, %p25;
	mov.u32 	%r146, 1;
	// begin inline asm
	shfl.sync.down.b32 %r139, %r30, %r146, %r187, %r188;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r144, %r31, %r146, %r187, %r188;
	// end inline asm
	mov.b64 	%rd175, {%r139, %r144};
	mov.b64 	%fd112, %rd175;
	setp.lt.s32 	%p26, %r138, %r187;
	add.f64 	%fd113, %fd183, %fd112;
	selp.f64 	%fd114, %fd113, %fd183, %p26;
	mov.b64 	%rd176, %fd114;
	mov.u32 	%r156, 2;
	mov.b64 	{%r150, %r155}, %rd176;
	// begin inline asm
	shfl.sync.down.b32 %r149, %r150, %r156, %r187, %r188;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r154, %r155, %r156, %r187, %r188;
	// end inline asm
	mov.b64 	%rd177, {%r149, %r154};
	mov.b64 	%fd115, %rd177;
	add.s32 	%r193, %r138, 2;
	setp.gt.s32 	%p27, %r193, %r187;
	add.f64 	%fd116, %fd114, %fd115;
	selp.f64 	%fd117, %fd114, %fd116, %p27;
	mov.b64 	%rd178, %fd117;
	mov.u32 	%r166, 4;
	mov.b64 	{%r160, %r165}, %rd178;
	// begin inline asm
	shfl.sync.down.b32 %r159, %r160, %r166, %r187, %r188;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r164, %r165, %r166, %r187, %r188;
	// end inline asm
	mov.b64 	%rd179, {%r159, %r164};
	mov.b64 	%fd118, %rd179;
	add.s32 	%r194, %r138, 4;
	setp.gt.s32 	%p28, %r194, %r187;
	add.f64 	%fd119, %fd117, %fd118;
	selp.f64 	%fd120, %fd117, %fd119, %p28;
	mov.b64 	%rd180, %fd120;
	mov.u32 	%r176, 8;
	mov.b64 	{%r170, %r175}, %rd180;
	// begin inline asm
	shfl.sync.down.b32 %r169, %r170, %r176, %r187, %r188;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r174, %r175, %r176, %r187, %r188;
	// end inline asm
	mov.b64 	%rd181, {%r169, %r174};
	mov.b64 	%fd121, %rd181;
	add.s32 	%r195, %r138, 8;
	setp.gt.s32 	%p29, %r195, %r187;
	add.f64 	%fd122, %fd120, %fd121;
	selp.f64 	%fd123, %fd120, %fd122, %p29;
	mov.b64 	%rd182, %fd123;
	mov.u32 	%r186, 16;
	mov.b64 	{%r180, %r185}, %rd182;
	// begin inline asm
	shfl.sync.down.b32 %r179, %r180, %r186, %r187, %r188;
	// end inline asm
	// begin inline asm
	shfl.sync.down.b32 %r184, %r185, %r186, %r187, %r188;
	// end inline asm
	mov.b64 	%rd183, {%r179, %r184};
	mov.b64 	%fd124, %rd183;
	add.s32 	%r196, %r138, 16;
	setp.gt.s32 	%p30, %r196, %r187;
	add.f64 	%fd125, %fd123, %fd124;
	selp.f64 	%fd184, %fd123, %fd125, %p30;
	@%p24 bra 	$L__BB7_28;

	add.s32 	%r252, %r137, 8;
	st.shared.f64 	[%r252], %fd184;

$L__BB7_28:
	bar.sync 	0;
	setp.ne.s32 	%p31, %r131, 0;
	@%p31 bra 	$L__BB7_35;

	setp.gt.s32 	%p32, %r16, 32;
	ld.shared.f64 	%fd126, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+16];
	add.f64 	%fd127, %fd184, %fd126;
	selp.f64 	%fd128, %fd127, %fd184, %p32;
	ld.shared.f64 	%fd129, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+24];
	add.f64 	%fd130, %fd128, %fd129;
	setp.gt.s32 	%p33, %r16, 64;
	selp.f64 	%fd131, %fd130, %fd128, %p33;
	ld.shared.f64 	%fd132, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+32];
	add.f64 	%fd133, %fd131, %fd132;
	setp.gt.s32 	%p34, %r16, 96;
	selp.f64 	%fd134, %fd133, %fd131, %p34;
	ld.shared.f64 	%fd135, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+40];
	add.f64 	%fd136, %fd134, %fd135;
	setp.gt.s32 	%p35, %r16, 128;
	selp.f64 	%fd137, %fd136, %fd134, %p35;
	ld.shared.f64 	%fd138, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+48];
	add.f64 	%fd139, %fd137, %fd138;
	setp.gt.s32 	%p36, %r16, 160;
	selp.f64 	%fd140, %fd139, %fd137, %p36;
	ld.shared.f64 	%fd141, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+56];
	add.f64 	%fd142, %fd140, %fd141;
	setp.gt.s32 	%p37, %r16, 192;
	selp.f64 	%fd184, %fd142, %fd140, %p37;
	setp.lt.s32 	%p38, %r16, 225;
	@%p38 bra 	$L__BB7_35;

	ld.shared.f64 	%fd143, [_ZZN3cub18DeviceReduceKernelINS_18DeviceReducePolicyIddlN6thrust4plusIdEEE9Policy600EPdS7_lS4_EEvT0_T1_T2_NS_13GridEvenShareISA_EET3_E12temp_storage+64];
	add.f64 	%fd184, %fd184, %fd143;

$L__BB7_35:
	mov.u32 	%r250, %tid.x;
	setp.ne.s32 	%p46, %r250, 0;
	@%p46 bra 	$L__BB7_37;

	cvta.to.global.u64 	%rd193, %rd26;
	mul.wide.u32 	%rd194, %r34, 8;
	add.s64 	%rd195, %rd193, %rd194;
	st.global.f64 	[%rd195], %fd184;

$L__BB7_37:
	ret;

}

